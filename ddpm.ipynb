{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Models\n",
    "\n",
    "In this notebook, we'll implement the Denoising Diffusion Probabilistic Model (DDPM) proposed in the paper [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) from scratch using PyTorch. We'll implement the U-Net model, the forward and reverse diffusion processes, the training loop and the sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet\n",
    "\n",
    "The U-net is implemented as in the original paper. It's been divided into blocks to make it more modular and easier to work with. The \"downblock\" consists of a series of convolutional layers, each followed by a maxpooling layer. The \"upblock\" consists of a convolutional layer followed by a transpose convolutional layer. The bottom-most conv block is implemented separately and is different from the rest of the blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.resnet import ResBlock\n",
    "\n",
    "## TODO: all resblocks should be able to accept timestep embedding as an input \n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels: int, \n",
    "            out_channels: int, \n",
    "            num_layers: int, \n",
    "            num_groups: int = 1, \n",
    "            dropout: float = 0.2, \n",
    "            activation: nn.Module = nn.ReLU\n",
    "            ):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        convs = []\n",
    "        convs.append(\n",
    "            ResBlock(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            num_groups=num_groups, \n",
    "            dropout=dropout, \n",
    "            activation=activation\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for _ in range(num_layers-1):\n",
    "            convs.append(\n",
    "                ResBlock(\n",
    "                out_channels,\n",
    "                out_channels, \n",
    "                num_groups=num_groups, \n",
    "                dropout=dropout, \n",
    "                activation=activation\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Downampling (left) side of the UNet.\n",
    "    Excludes the bottom-most conv block.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, filters: list[int], num_layers: int):\n",
    "        super(DownBlock, self).__init__()\n",
    "        conv_blocks = [ConvBlock(in_channels, filters[0], num_layers)]\n",
    "        for i in range(1, len(filters)):\n",
    "            conv_blocks.append(ConvBlock(filters[i-1], filters[i], num_layers))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(*conv_blocks)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual_outputs = []\n",
    "        for conv_block in self.conv_blocks:\n",
    "            x = conv_block(x)\n",
    "            residual_outputs.append(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        return residual_outputs, x\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling (right) side of the UNet.\n",
    "    \"\"\"\n",
    "    def __init__(self, filters: list[int], num_layers: int):\n",
    "        super(UpBlock, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(filters) - 2):\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    ConvBlock(filters[i], filters[i+1], num_layers), \n",
    "                    nn.ConvTranspose2d(filters[i+1], filters[i+1]//2, 2, stride=2)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        layers.append(ConvBlock(filters[-2], filters[-1], num_layers))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, residual_outputs):\n",
    "        for i in range(len(self.layers)):\n",
    "            residual = residual_outputs[-(i+1)]\n",
    "            _, _, h, w = x.shape\n",
    "            residual = residual[:, :, :h, :w]\n",
    "            x = torch.cat([x, residual], dim=1)\n",
    "            x = self.layers[i](x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DownBlock(in_channels=3, filters=[32, 64, 128], num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DownBlock(\n",
       "  (conv_blocks): Sequential(\n",
       "    (0): ConvBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 3, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Identity()\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ConvBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Identity()\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ConvBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Identity()\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 16, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = db(torch.randn(1, 3, 128, 128))\n",
    "a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 128, 128])\n",
      "----------\n",
      "torch.Size([1, 64, 64, 64])\n",
      "----------\n",
      "torch.Size([1, 128, 32, 32])\n",
      "----------\n",
      "torch.Size([1, 128, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "for residual_output in a[0]:\n",
    "    print(residual_output.shape)\n",
    "    print(\"-\"*10)\n",
    "\n",
    "print(a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 32, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_conv = nn.Sequential(ResBlock(128, 256), nn.ConvTranspose2d(256, 128, 2, stride=2))\n",
    "bottom_conv(a[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "up = UpBlock([256, 128, 64, 32], 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ConvBlock(\n",
      "    (convs): ModuleList(\n",
      "      (0): ResBlock(\n",
      "        (norm1): Sequential(\n",
      "          (0): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (activation): ReLU()\n",
      "        (idconv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (avgpool): Identity()\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): ResBlock(\n",
      "        (norm1): Sequential(\n",
      "          (0): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (activation): ReLU()\n",
      "        (idconv): Identity()\n",
      "        (avgpool): Identity()\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      ")\n",
      "----------\n",
      "Sequential(\n",
      "  (0): ConvBlock(\n",
      "    (convs): ModuleList(\n",
      "      (0): ResBlock(\n",
      "        (norm1): Sequential(\n",
      "          (0): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (activation): ReLU()\n",
      "        (idconv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (avgpool): Identity()\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): ResBlock(\n",
      "        (norm1): Sequential(\n",
      "          (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (activation): ReLU()\n",
      "        (idconv): Identity()\n",
      "        (avgpool): Identity()\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      ")\n",
      "----------\n",
      "ConvBlock(\n",
      "  (convs): ModuleList(\n",
      "    (0): ResBlock(\n",
      "      (norm1): Sequential(\n",
      "        (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (activation): ReLU()\n",
      "      (idconv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (avgpool): Identity()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (norm1): Sequential(\n",
      "        (0): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (activation): ReLU()\n",
      "      (idconv): Identity()\n",
      "      (avgpool): Identity()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for layer in up.layers:\n",
    "    print(layer)\n",
    "    print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, down_filters, in_channels, num_layers):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down_filters = down_filters\n",
    "        self.down_block = DownBlock(filters=down_filters, num_layers=num_layers, in_channels=in_channels)\n",
    "        \n",
    "        # the bottom-most conv block is different in that it doesn't have a maxpool or a residual connection\n",
    "        self.bottom_conv = nn.Sequential(\n",
    "            ConvBlock(down_filters[-1], down_filters[-1]*2, num_layers=num_layers), \n",
    "            nn.ConvTranspose2d(down_filters[-1]*2, down_filters[-1], 2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.up_filters = [down_filters[-1]*2]\n",
    "        self.up_filters.extend(reversed(down_filters))\n",
    "        self.up_block = UpBlock(self.up_filters, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual_outputs, down_output = self.down_block(x)\n",
    "        bottom_output = self.bottom_conv(down_output)\n",
    "        return self.up_block(bottom_output, residual_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 128, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_net = UNet([32, 64, 128], 3, 2)\n",
    "a = u_net(torch.randn(1, 3, 128, 128))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_bar(beta):\n",
    "    alpha = 1. - beta\n",
    "    return alpha.cumprod(dim=0)\n",
    "\n",
    "def prepare_batch(x: torch.Tensor, T: int, alpha_bar: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Prepare a batch for training by generating the noise and the noisy image.\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    t = torch.randint(0, T, (batch_size,))\n",
    "    e = torch.randn_like(x)\n",
    "    alpha_bar_t = alpha_bar(t)\n",
    "    alpha_bar_t = alpha_bar_t.view(-1, 1, 1, 1)\n",
    "    noisy_images = alpha_bar_t.sqrt() * x + (1 - alpha_bar_t).sqrt() * e\n",
    "    \n",
    "    return (noisy_images, t), e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes that need to be made to the U-Net for DDPM:\n",
    "1. Swap batch norm with group norm \n",
    "2. Introduce an attention mechanism at each conv block in the down and up blocks\n",
    "3. Create an embedding for the timestep\n",
    "\n",
    "Next, we'll implement these missing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scaled_dot_product_attention(q, k, d_k, mask):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is True:\n",
    "        mask = torch.tril(torch.ones(scores.shape)).to(q.device)\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    return nn.Softmax(-1)(scores)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead attention.\n",
    "\n",
    "    Differences from multihead attention for text:\n",
    "    \n",
    "    - we no longer need a d_model, the internal hidden size \n",
    "    is determined by the number of channels which is determined \n",
    "    by the convolutional layers leading up to the attention layer.\n",
    "\n",
    "    - swap batch norm with group norm\n",
    "\n",
    "    - we resize the image to one of shape: (batch_size, num_channels, height * width)\n",
    "    so that we can perform multihead attention across the image. This closely \n",
    "    mirrors (batch_size, embed_dim, seq_len\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_k: int, \n",
    "                 dropout: float, \n",
    "                 num_heads: int, \n",
    "                 num_channels: int,\n",
    "                 num_groups: int = 8,\n",
    "                 mask: bool = False\n",
    "                 ):\n",
    "        super(Attention, self).__init__()\n",
    "        self.d_k, self.num_heads = d_k, num_heads\n",
    "        self.query_projection, self.key_projection, self.value_projection = (\n",
    "            nn.Linear(num_channels, num_heads* d_k),\n",
    "            nn.Linear(num_channels, num_heads* d_k), \n",
    "            nn.Linear(num_channels, num_heads*d_k)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(num_channels)\n",
    "        self.output_layer = nn.Linear(num_heads*d_k, num_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.mask = mask\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        x = x.view(batch_size, n_channels, height * width).permute(0, 2, 1)\n",
    "        residual = x\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        if y is not None:\n",
    "            k, q, v = y, x, y\n",
    "        else:\n",
    "            k, q, v = x, x, x\n",
    "        \n",
    "        k_len, q_len, v_len, batch_size = k.size(1), q.size(1), v.size(1),  q.size(0)\n",
    "        \n",
    "        k = self.key_projection(k).view(batch_size, k_len,  self.num_heads, self.d_k)\n",
    "        q = self.query_projection(q).view(batch_size, q_len,  self.num_heads, self.d_k)\n",
    "        v = self.value_projection(v).view(batch_size, v_len,  self.num_heads, self.d_k)\n",
    "        \n",
    "        attention = scaled_dot_product_attention(\n",
    "            q.transpose(1, 2), \n",
    "            k.transpose(1, 2), \n",
    "            self.d_k, \n",
    "            self.mask\n",
    "        )\n",
    "        output = torch.matmul(attention, v.transpose(1, 2))\n",
    "        output = self.output_layer(output.transpose(1, 2).contiguous().view(batch_size, q_len, -1))\n",
    "        \n",
    "        return self.dropout(output) + residual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net left and right blocks with attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "import pdb\n",
    "\n",
    "class ConvBlockAttn(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels: int, \n",
    "            out_channels: int, \n",
    "            num_layers: int, \n",
    "            num_groups: int = 1, \n",
    "            dropout: float = 0.2, \n",
    "            activation: nn.Module = nn.ReLU,\n",
    "            has_attention: bool = False,\n",
    "            num_heads: int = 8\n",
    "            ):\n",
    "        super(ConvBlockAttn, self).__init__()\n",
    "        convs = []\n",
    "        convs.append(\n",
    "            ResBlock(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            num_groups=num_groups, \n",
    "            dropout=dropout, \n",
    "            activation=activation,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for _ in range(num_layers-1):\n",
    "            convs.append(\n",
    "                ResBlock(\n",
    "                out_channels,\n",
    "                out_channels, \n",
    "                num_groups=num_groups, \n",
    "                dropout=dropout, \n",
    "                activation=activation,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pdb.set_trace()\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class DownBlockAttn(nn.Module):\n",
    "    \"\"\"\n",
    "    Downampling (left) side of the UNet.\n",
    "    Excludes the bottom-most conv block.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels: int, \n",
    "            filters: list[int], \n",
    "            num_layers: int, \n",
    "            has_attention: bool = False, \n",
    "            num_heads: int = 8, \n",
    "            dropout: float = 0.2, \n",
    "            ):\n",
    "        super(DownBlockAttn, self).__init__()\n",
    "        \n",
    "        self.has_attention = has_attention\n",
    "        conv_blocks = [ConvBlockAttn(in_channels, filters[0], num_layers, has_attention, num_heads)]\n",
    "        attention_blocks = [\n",
    "            Attention(\n",
    "                d_k=64, \n",
    "                dropout=0.1, \n",
    "                num_heads=num_heads, \n",
    "                num_channels=filters[0], \n",
    "            )\n",
    "        ] if has_attention else []\n",
    "        \n",
    "        for i in range(1, len(filters)):\n",
    "            conv_blocks.append(ConvBlockAttn(filters[i-1], filters[i], num_layers, has_attention, num_heads))\n",
    "            if has_attention:\n",
    "                attention_blocks.append(\n",
    "                    Attention(\n",
    "                        d_k=64, \n",
    "                        dropout=0.1, \n",
    "                        num_heads=num_heads, \n",
    "                        num_channels=filters[i], \n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        self.conv_blocks = nn.ModuleList(conv_blocks)\n",
    "        self.attention_blocks = nn.ModuleList(attention_blocks)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual_outputs = []\n",
    "        for conv_block, attention_block in zip(self.conv_blocks, self.attention_blocks):\n",
    "            x = conv_block(x)\n",
    "            if self.has_attention:\n",
    "                x = attention_block(x)\n",
    "            \n",
    "            residual_outputs.append(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        return residual_outputs, x\n",
    "\n",
    "class UpBlockAttn(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling (right) side of the UNet.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            filters: list[int], \n",
    "            num_layers: int, \n",
    "            has_attention: bool = False, \n",
    "            num_heads: int = 8, \n",
    "            dropout: float = 0.2\n",
    "            ):\n",
    "        super(UpBlockAttn, self).__init__()\n",
    "        \n",
    "        conv_layers = []\n",
    "        attention_layers = []\n",
    "        \n",
    "        for i in range(len(filters) - 2):\n",
    "            conv_layers.append(\n",
    "                nn.Sequential(\n",
    "                    ConvBlock(filters[i], filters[i+1], num_layers), \n",
    "                    nn.ConvTranspose2d(filters[i+1], filters[i+1]//2, 2, stride=2)\n",
    "                )\n",
    "            )\n",
    "            if has_attention:\n",
    "                attention_layers.append(\n",
    "                    Attention(\n",
    "                        d_k=64, \n",
    "                        dropout=0.1, \n",
    "                        num_heads=num_heads, \n",
    "                        num_channels=filters[i+1]//2, \n",
    "                    )\n",
    "                )\n",
    "        conv_layers.append(ConvBlock(filters[-2], filters[-1], num_layers))\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "        self.attention_layers = nn.Sequential(*attention_layers)\n",
    "    \n",
    "    def forward(self, x, residual_outputs):\n",
    "        for i in range(len(self.conv_layers)):\n",
    "            residual = residual_outputs[-(i+1)]\n",
    "            _, _, h, w = x.shape\n",
    "            residual = residual[:, :, :h, :w]\n",
    "            x = torch.cat([x, residual], dim=1)\n",
    "            \n",
    "            x = self.conv_layers[i](x)\n",
    "            if self.has_attention:\n",
    "                x = self.attention_layers[i](x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_encoding(curr_t: torch.Tensor, T: torch.Tensor, embedding_dim: int, n=10000):\n",
    "    \"\"\"\n",
    "    Naive sin/cosin positional embedding adapted for timestep embedding in DDPM\n",
    "    \"\"\"\n",
    "    curr_t = curr_t / T # normalize the timestep to be between 0 and 1\n",
    "    p = torch.zeros((curr_t.shape[-1], embedding_dim)) # initialize the positional embedding tensor\n",
    "\n",
    "    m = torch.arange(int(embedding_dim/2)) # this is divided by two because we alternate between sin and cos\n",
    "    denominators = torch.float_power(n, 2*m/embedding_dim) # compute the denominators for the sin and cos functions\n",
    "    \n",
    "    p[:, 0::2] = torch.sin(curr_t.unsqueeze(1) / denominators.unsqueeze(0))\n",
    "    p[:, 1::2] = torch.cos(curr_t.unsqueeze(1) / denominators.unsqueeze(0))\n",
    "    return p\n",
    "\n",
    "\n",
    "\n",
    "class TimestepEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeds the timestep into a higher dimensional space using a 2 layer MLP.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                in_channels: int, \n",
    "                embedding_dim: int, \n",
    "                activation: nn.Module = nn.ReLU, \n",
    "                post_activation: bool = True\n",
    "                ):\n",
    "        super(TimestepEmbedding, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_channels, embedding_dim)\n",
    "        self.linear2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.activation = activation()\n",
    "        self.post_activation = post_activation\n",
    "\n",
    "    def forward(self, curr_t: torch.Tensor, T: torch.Tensor):\n",
    "        x = self.linear1(curr_t)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        if self.post_activation:\n",
    "            x = self.activation(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetAttn(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet model for DDPM.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 down_filters, \n",
    "                 in_channels, \n",
    "                 num_layers, \n",
    "                 has_attention: bool = False, \n",
    "                 num_heads: int = 8,\n",
    "                 diffusion_steps: int = None,\n",
    "                 num_groups: int = 8,\n",
    "                 activation: nn.Module = nn.ReLU\n",
    "                ):\n",
    "        super(UNetAttn, self).__init__()\n",
    "\n",
    "        self.T = diffusion_steps\n",
    "        self.down_filters = down_filters\n",
    "\n",
    "        self.time_embed_dim = down_filters[0] * 4 \n",
    "        \n",
    "        if self.T is not None:\n",
    "            self.timestep_embedding = TimestepEmbedding(\n",
    "                in_channels=in_channels, \n",
    "                embedding_dim=in_channels, \n",
    "                activation=activation, \n",
    "                post_activation=True\n",
    "            )\n",
    "        \n",
    "        \n",
    "        self.down_block = DownBlock(\n",
    "            filters=down_filters, \n",
    "            num_layers=num_layers, \n",
    "            in_channels=in_channels, \n",
    "            has_attention=has_attention, \n",
    "            num_heads=num_heads\n",
    "        )\n",
    "        \n",
    "        # the bottom-most (middle) conv block \n",
    "        \n",
    "        # The timestep embedding is fed to the bottom-most conv block because it \n",
    "        # provides temporal context at the deepest, most abstract level of features,\n",
    "        # allowing the model to adjust its predictions based on the diffusion step.\n",
    "        if has_attention:\n",
    "            self.bottom_conv = nn.Sequential(\n",
    "                ConvBlock(down_filters[-1], down_filters[-1]*2, num_layers=num_layers), \n",
    "                Attention(\n",
    "                    d_k=64, \n",
    "                    dropout=0.1, \n",
    "                    num_heads=num_heads, \n",
    "                    num_channels=down_filters[-1]*2, \n",
    "                ),\n",
    "                nn.ConvTranspose2d(down_filters[-1]*2, down_filters[-1], 2, stride=2)\n",
    "            )\n",
    "        else:\n",
    "            self.bottom_conv = nn.Sequential(\n",
    "                ConvBlock(down_filters[-1], down_filters[-1]*2, num_layers=num_layers), \n",
    "                nn.ConvTranspose2d(down_filters[-1]*2, down_filters[-1], 2, stride=2)\n",
    "            )\n",
    "        \n",
    "        self.up_filters = [down_filters[-1]*2]\n",
    "        self.up_filters.extend(reversed(down_filters))\n",
    "        self.up_block = UpBlock(self.up_filters, num_layers)\n",
    "\n",
    "        self.group_norm = nn.GroupNorm(num_groups=num_groups, num_channels=in_channels)\n",
    "        self.conv_out = nn.Conv2d(down_filters[-1], in_channels, 3, padding=1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        if self.T is not None:\n",
    "            t_emb = self.timestep_embedding(curr_t=t, T=self.T)\n",
    "            t_emb = t_emb.view(-1, self.time_embed_dim, 1, 1)\n",
    "\n",
    "        x = self.group_norm(x)\n",
    "        residual_outputs, down_output = self.down_block(x)\n",
    "        bottom_output = self.bottom_conv(down_output)\n",
    "        return self.up_block(bottom_output, residual_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 32, 32])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_out = bottom_conv(a[1])\n",
    "bottom_out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_out = up(bottom_out, a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dropout probability has to be between 0 and 1, but got 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m db \u001b[38;5;241m=\u001b[39m \u001b[43mDownBlockAttn\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m db\n",
      "Cell \u001b[0;32mIn[60], line 64\u001b[0m, in \u001b[0;36mDownBlockAttn.__init__\u001b[0;34m(self, in_channels, filters, num_layers, has_attention, num_heads, dropout)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28msuper\u001b[39m(DownBlockAttn, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_attention \u001b[38;5;241m=\u001b[39m has_attention\n\u001b[0;32m---> 64\u001b[0m conv_blocks \u001b[38;5;241m=\u001b[39m [\u001b[43mConvBlockAttn\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_attention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     65\u001b[0m attention_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     66\u001b[0m     Attention(\n\u001b[1;32m     67\u001b[0m         d_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m     )\n\u001b[1;32m     72\u001b[0m ] \u001b[38;5;28;01mif\u001b[39;00m has_attention \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(filters)):\n",
      "Cell \u001b[0;32mIn[60], line 18\u001b[0m, in \u001b[0;36mConvBlockAttn.__init__\u001b[0;34m(self, in_channels, out_channels, num_layers, num_groups, dropout, activation, has_attention, num_heads)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28msuper\u001b[39m(ConvBlockAttn, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     16\u001b[0m convs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m convs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m---> 18\u001b[0m     \u001b[43mResBlock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     28\u001b[0m     convs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     29\u001b[0m         ResBlock(\n\u001b[1;32m     30\u001b[0m         out_channels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     35\u001b[0m         )\n\u001b[1;32m     36\u001b[0m     )\n",
      "File \u001b[0;32m~/computer_vision/src/resnet.py:72\u001b[0m, in \u001b[0;36mResBlock.__init__\u001b[0;34m(self, in_channels, out_channels, activation, kernel_size, stride, padding, dropout, num_groups, timestep_emb_dim)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     69\u001b[0m     nn\u001b[38;5;241m.\u001b[39mIdentity() \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mAvgPool2d(kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stride\u001b[38;5;241m=\u001b[39mstride)\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m=\u001b[39m activation()\n\u001b[0;32m---> 72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep_emb_dim \u001b[38;5;241m=\u001b[39m timestep_emb_dim\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep_emb_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/computer_vision/.venv/lib/python3.10/site-packages/torch/nn/modules/dropout.py:16\u001b[0m, in \u001b[0;36m_DropoutNd.__init__\u001b[0;34m(self, p, inplace)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace \u001b[38;5;241m=\u001b[39m inplace\n",
      "\u001b[0;31mValueError\u001b[0m: dropout probability has to be between 0 and 1, but got 3"
     ]
    }
   ],
   "source": [
    "db = DownBlockAttn(in_channels=3, filters=[32, 64, 128], num_layers=2, has_attention=True, num_heads=3)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected weight to be a vector of size equal to the number of channels in input, but got weight of shape [32] and input of shape [1, 8192, 16]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mdb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m a[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/computer_vision/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/computer_vision/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[46], line 98\u001b[0m, in \u001b[0;36mDownBlockAttn.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m residual_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv_block, attention_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_blocks):\n\u001b[0;32m---> 98\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mconv_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_attention:\n\u001b[1;32m    100\u001b[0m         x \u001b[38;5;241m=\u001b[39m attention_block(x)\n",
      "File \u001b[0;32m~/computer_vision/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/computer_vision/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 42\u001b[0m, in \u001b[0;36mConvBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs:\n\u001b[0;32m---> 42\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/computer_vision/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/computer_vision/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/computer_vision/src/resnet.py:85\u001b[0m, in \u001b[0;36mResBlock.forward\u001b[0;34m(self, x, timestep_emb)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, timestep_emb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     79\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m    Forward pass of the residual block.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03m        x: input tensor\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m        timestep_emb: optional timestep embedding\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(h)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep_emb_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/computer_vision/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/computer_vision/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/computer_vision/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/computer_vision/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/computer_vision/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/computer_vision/.venv/lib/python3.10/site-packages/torch/nn/modules/normalization.py:288\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/computer_vision/.venv/lib/python3.10/site-packages/torch/nn/functional.py:2606\u001b[0m, in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2604\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2605\u001b[0m _verify_batch_size([\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_groups, num_groups] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:]))\n\u001b[0;32m-> 2606\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected weight to be a vector of size equal to the number of channels in input, but got weight of shape [32] and input of shape [1, 8192, 16]"
     ]
    }
   ],
   "source": [
    "a = db(torch.randn(1, 3, 128, 128))\n",
    "a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 128, 128])\n",
      "----------\n",
      "torch.Size([1, 64, 64, 64])\n",
      "----------\n",
      "torch.Size([1, 128, 32, 32])\n",
      "----------\n",
      "torch.Size([1, 128, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "for residual_output in a[0]:\n",
    "    print(residual_output.shape)\n",
    "    print(\"-\"*10)\n",
    "\n",
    "print(a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 32, 32])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bottom_conv = nn.Sequential(ResBlock(128, 256), nn.ConvTranspose2d(256, 128, 2, stride=2))\n",
    "bottom_conv(a[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up = UpBlock([256, 128, 64, 32], 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ConvBlock(\n",
      "    (convs): ModuleList(\n",
      "      (0): ResBlock(\n",
      "        (norm1): Sequential(\n",
      "          (0): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (activation): ReLU()\n",
      "        (idconv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (avgpool): Identity()\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): ResBlock(\n",
      "        (norm1): Sequential(\n",
      "          (0): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Sequential(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (activation): ReLU()\n",
      "        (idconv): Identity()\n",
      "        (avgpool): Identity()\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      ")\n",
      "----------\n",
      "Sequential(\n",
      "  (0): ConvBlock(\n",
      "    (convs): ModuleList(\n",
      "      (0): ResBlock(\n",
      "        (norm1): Sequential(\n",
      "          (0): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (activation): ReLU()\n",
      "        (idconv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (avgpool): Identity()\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): ResBlock(\n",
      "        (norm1): Sequential(\n",
      "          (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "          (1): ReLU()\n",
      "        )\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (conv2): Sequential(\n",
      "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (activation): ReLU()\n",
      "        (idconv): Identity()\n",
      "        (avgpool): Identity()\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      ")\n",
      "----------\n",
      "ConvBlock(\n",
      "  (convs): ModuleList(\n",
      "    (0): ResBlock(\n",
      "      (norm1): Sequential(\n",
      "        (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (activation): ReLU()\n",
      "      (idconv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (avgpool): Identity()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (norm1): Sequential(\n",
      "        (0): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
      "        (1): ReLU()\n",
      "      )\n",
      "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (activation): ReLU()\n",
      "      (idconv): Identity()\n",
      "      (avgpool): Identity()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for layer in up.layers:\n",
    "    print(layer)\n",
    "    print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 32, 32])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bottom_out = bottom_conv(a[1])\n",
    "bottom_out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_out = up(bottom_out, a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = torch.randn(2, 3, 128, 128)\n",
    "conv_block = ConvBlock(3, 32, 2)\n",
    "h_example_image = conv_block(example_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16384, 32])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_block = Attention(d_k=64, dropout=0.1, num_heads=3, num_channels=32, num_groups=8)\n",
    "attn_output = attention_block(h_example_image)\n",
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(15, 3, 128, 128)\n",
    "T = 1000\n",
    "batch_size = x.shape[0]\n",
    "t = torch.randint(0, T, (batch_size,))\n",
    "e = torch.randn_like(x)\n",
    "alpha_bar_t = alpha_bar(t)\n",
    "print(alpha_bar_t.shape)\n",
    "alpha_bar_t = alpha_bar_t.view(-1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_bar_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, valid_loader, loss_fn, all_valid_loss):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            x, y = batch\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            valid_loss.append(loss.item())\n",
    "    \n",
    "    all_valid_loss.append(sum(valid_loss) / len(valid_loss))\n",
    "\n",
    "def plot_loss(all_train_loss, all_valid_loss):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(all_train_loss, label='Training Loss')\n",
    "    plt.plot(all_valid_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module,\n",
    "                optim: torch.optim.Optimizer,\n",
    "                loss_fn,\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                scheduler,\n",
    "                epochs=10,\n",
    "                valid_every=1\n",
    "                ):\n",
    "\n",
    "    all_train_loss = []\n",
    "    all_valid_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x, _ = batch\n",
    "            (noisy_images, t), e = prepare_batch(x, T, alpha_bar_t)\n",
    "            optim.zero_grad()\n",
    "            y_pred = model(noisy_images)\n",
    "            loss = loss_fn(y_pred, e)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        all_train_loss.append(sum(train_loss) / len(train_loss))\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % valid_every == 0:\n",
    "            validate_model(model, valid_loader, loss_fn, all_valid_loss)\n",
    "            print(\n",
    "                f\"Epoch {epoch}, Train Loss: {sum(train_loss) / len(train_loss)}, \"\n",
    "                f\"Valid Loss: {all_valid_loss[-1]}\"\n",
    "            )\n",
    "    \n",
    "    plot_loss(all_train_loss, all_valid_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
