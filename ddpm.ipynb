{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Models\n",
    "\n",
    "In this notebook, we'll implement the Denoising Diffusion Probabilistic Model (DDPM) proposed in the paper [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) from scratch using PyTorch. We'll implement the U-Net model, the forward and reverse diffusion processes, the training loop and the sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet\n",
    "\n",
    "The U-net is implemented as in the original paper. It's been divided into blocks to make it more modular and easier to work with. The \"downblock\" consists of a series of convolutional layers, each followed by a maxpooling layer. The \"upblock\" consists of a convolutional layer followed by a transpose convolutional layer. The bottom-most conv block is implemented separately and is different from the rest of the blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.resnet import ResBlock\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels: int, \n",
    "            out_channels: int, \n",
    "            num_layers: int, \n",
    "            num_groups: int = 1, \n",
    "            dropout: float = 0.2, \n",
    "            activation: nn.Module = nn.ReLU\n",
    "            ):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        convs = []\n",
    "        convs.append(\n",
    "            ResBlock(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            num_groups=num_groups, \n",
    "            dropout=dropout, \n",
    "            activation=activation\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for _ in range(num_layers-1):\n",
    "            convs.append(\n",
    "                ResBlock(\n",
    "                out_channels,\n",
    "                out_channels, \n",
    "                num_groups=num_groups, \n",
    "                dropout=dropout, \n",
    "                activation=activation\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Downampling (left) side of the UNet.\n",
    "    Excludes the bottom-most conv block.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, filters: list[int], num_layers: int):\n",
    "        super(DownBlock, self).__init__()\n",
    "        conv_blocks = [ConvBlock(in_channels, filters[0], num_layers)]\n",
    "        for i in range(1, len(filters)):\n",
    "            conv_blocks.append(ConvBlock(filters[i-1], filters[i], num_layers))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(*conv_blocks)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual_outputs = []\n",
    "        for conv_block in self.conv_blocks:\n",
    "            x = conv_block(x)\n",
    "            residual_outputs.append(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        return residual_outputs, x\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling (right) side of the UNet.\n",
    "    \"\"\"\n",
    "    def __init__(self, filters: list[int], num_layers: int):\n",
    "        super(UpBlock, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(filters) - 2):\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    ConvBlock(filters[i], filters[i+1], num_layers), \n",
    "                    nn.ConvTranspose2d(filters[i+1], filters[i+1]//2, 2, stride=2)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        layers.append(ConvBlock(filters[-2], filters[-1], num_layers))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, residual_outputs):\n",
    "        for i in range(len(self.layers)):\n",
    "            residual = residual_outputs[-(i+1)]\n",
    "            _, _, h, w = x.shape\n",
    "            residual = residual[:, :, :h, :w]\n",
    "            x = torch.cat([x, residual], dim=1)\n",
    "            x = self.layers[i](x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DownBlock(in_channels=3, filters=[32, 64, 128], num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DownBlock(\n",
       "  (conv_blocks): Sequential(\n",
       "    (0): ConvBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): GroupNorm(1, 3, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (3): ReLU()\n",
       "            (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (5): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (idconv): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (avgpool): Identity()\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (3): ReLU()\n",
       "            (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (5): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (idconv): Identity()\n",
       "          (avgpool): Identity()\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ConvBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (3): ReLU()\n",
       "            (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (5): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (idconv): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (avgpool): Identity()\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (3): ReLU()\n",
       "            (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (5): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (idconv): Identity()\n",
       "          (avgpool): Identity()\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ConvBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (3): ReLU()\n",
       "            (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (5): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (idconv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (avgpool): Identity()\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (3): ReLU()\n",
       "            (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (5): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (idconv): Identity()\n",
       "          (avgpool): Identity()\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 16, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = db(torch.randn(1, 3, 128, 128))\n",
    "a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 128, 128])\n",
      "----------\n",
      "torch.Size([1, 64, 64, 64])\n",
      "----------\n",
      "torch.Size([1, 128, 32, 32])\n",
      "----------\n",
      "torch.Size([1, 128, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "for residual_output in a[0]:\n",
    "    print(residual_output.shape)\n",
    "    print(\"-\"*10)\n",
    "\n",
    "print(a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 32, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_conv = nn.Sequential(ResBlock(128, 256), nn.ConvTranspose2d(256, 128, 2, stride=2))\n",
    "bottom_conv(a[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "up = UpBlock([256, 128, 64, 32], 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ConvBlock(\n",
      "    (convs): ModuleList(\n",
      "      (0): ResBlock(\n",
      "        (conv): Sequential(\n",
      "          (0): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): ReLU()\n",
      "          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (5): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (idconv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (avgpool): Identity()\n",
      "        (activation): ReLU()\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): ResBlock(\n",
      "        (conv): Sequential(\n",
      "          (0): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): ReLU()\n",
      "          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (5): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (idconv): Identity()\n",
      "        (avgpool): Identity()\n",
      "        (activation): ReLU()\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      ")\n",
      "----------\n",
      "Sequential(\n",
      "  (0): ConvBlock(\n",
      "    (convs): ModuleList(\n",
      "      (0): ResBlock(\n",
      "        (conv): Sequential(\n",
      "          (0): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): ReLU()\n",
      "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (5): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (idconv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (avgpool): Identity()\n",
      "        (activation): ReLU()\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): ResBlock(\n",
      "        (conv): Sequential(\n",
      "          (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): ReLU()\n",
      "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (5): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (idconv): Identity()\n",
      "        (avgpool): Identity()\n",
      "        (activation): ReLU()\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      ")\n",
      "----------\n",
      "ConvBlock(\n",
      "  (convs): ModuleList(\n",
      "    (0): ResBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "        (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (5): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (idconv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (avgpool): Identity()\n",
      "      (activation): ReLU()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "        (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (5): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (idconv): Identity()\n",
      "      (avgpool): Identity()\n",
      "      (activation): ReLU()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for layer in up.layers:\n",
    "    print(layer)\n",
    "    print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_out = bottom_conv(a[1])\n",
    "bottom_out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_out = up(bottom_out, a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, down_filters, in_channels, num_layers):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down_filters = down_filters\n",
    "        self.down_block = DownBlock(filters=down_filters, num_layers=num_layers, in_channels=in_channels)\n",
    "        \n",
    "        # the bottom-most conv block is different in that it doesn't have a maxpool or a residual connection\n",
    "        self.bottom_conv = nn.Sequential(\n",
    "            ConvBlock(down_filters[-1], down_filters[-1]*2, num_layers=num_layers), \n",
    "            nn.ConvTranspose2d(down_filters[-1]*2, down_filters[-1], 2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.up_filters = [down_filters[-1]*2]\n",
    "        self.up_filters.extend(reversed(down_filters))\n",
    "        self.up_block = UpBlock(self.up_filters, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual_outputs, down_output = self.down_block(x)\n",
    "        bottom_output = self.bottom_conv(down_output)\n",
    "        return self.up_block(bottom_output, residual_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 128, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_net = UNet([32, 64, 128], 3, 2)\n",
    "a = u_net(torch.randn(1, 3, 128, 128))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_bar(beta):\n",
    "    alpha = 1. - beta\n",
    "    return alpha.cumprod(dim=0)\n",
    "\n",
    "def prepare_batch(x: torch.Tensor, T: int, alpha_bar: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Prepare a batch for training by generating the noise and the noisy image.\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    t = torch.randint(0, T, (batch_size,))\n",
    "    e = torch.randn_like(x)\n",
    "    alpha_bar_t = alpha_bar(t)\n",
    "    alpha_bar_t = alpha_bar_t.view(-1, 1, 1, 1)\n",
    "    noisy_images = alpha_bar_t.sqrt() * x + (1 - alpha_bar_t).sqrt() * e\n",
    "    \n",
    "    return (noisy_images, t), e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes that need to be made to the U-Net for DDPM:\n",
    "1. Swap batch norm with group norm \n",
    "2. Introduce an attention mechanism at each conv block in the down and up blocks\n",
    "3. Create an embedding for the timestep\n",
    "\n",
    "Next, we'll implement these missing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scaled_dot_product_attention(q, k, d_k, mask):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is True:\n",
    "        mask = torch.tril(torch.ones(scores.shape)).to(q.device)\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    return nn.Softmax(-1)(scores)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead attention.\n",
    "\n",
    "    Differences from multihead attention for text:\n",
    "    \n",
    "    - we no longer need a d_model, the internal hidden size \n",
    "    is determined by the number of channels which is determined \n",
    "    by the convolutional layers leading up to the attention layer.\n",
    "\n",
    "    - swap batch norm with group norm\n",
    "\n",
    "    - we resize the image to one of shape: (batch_size, num_channels, height * width)\n",
    "    so that we can perform multihead attention across the image. This closely \n",
    "    mirrors (batch_size, embed_dim, seq_len\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_k: int, \n",
    "                 dropout: float, \n",
    "                 num_heads: int, \n",
    "                 num_channels: int,\n",
    "                 num_groups: int = 8,\n",
    "                 mask: bool = False\n",
    "                 ):\n",
    "        super(Attention, self).__init__()\n",
    "        self.d_k, self.num_heads = d_k, num_heads\n",
    "        self.query_projection, self.key_projection, self.value_projection = (\n",
    "            nn.Linear(num_channels, num_heads* d_k),\n",
    "            nn.Linear(num_channels, num_heads* d_k), \n",
    "            nn.Linear(num_channels, num_heads*d_k)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(num_channels)\n",
    "        self.output_layer = nn.Linear(num_heads*d_k, num_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.mask = mask\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        x = x.view(batch_size, n_channels, height * width).permute(0, 2, 1)\n",
    "        residual = x\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        if y is not None:\n",
    "            k, q, v = y, x, y\n",
    "        else:\n",
    "            k, q, v = x, x, x\n",
    "        \n",
    "        k_len, q_len, v_len, batch_size = k.size(1), q.size(1), v.size(1),  q.size(0)\n",
    "        \n",
    "        k = self.key_projection(k).view(batch_size, k_len,  self.num_heads, self.d_k)\n",
    "        q = self.query_projection(q).view(batch_size, q_len,  self.num_heads, self.d_k)\n",
    "        v = self.value_projection(v).view(batch_size, v_len,  self.num_heads, self.d_k)\n",
    "        \n",
    "        attention = scaled_dot_product_attention(\n",
    "            q.transpose(1, 2), \n",
    "            k.transpose(1, 2), \n",
    "            self.d_k, \n",
    "            self.mask\n",
    "        )\n",
    "        output = torch.matmul(attention, v.transpose(1, 2))\n",
    "        output = self.output_layer(output.transpose(1, 2).contiguous().view(batch_size, q_len, -1))\n",
    "        \n",
    "        return self.dropout(output) + residual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U-Net left and right blocks with attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Downampling (left) side of the UNet.\n",
    "    Excludes the bottom-most conv block.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels: int, \n",
    "            filters: list[int], \n",
    "            num_layers: int, \n",
    "            has_attention: bool = False, \n",
    "            num_heads: int = 8, \n",
    "            dropout: float = 0.2, \n",
    "            ):\n",
    "        super(DownBlock, self).__init__()\n",
    "        \n",
    "        self.has_attention = has_attention\n",
    "        conv_blocks = [ConvBlock(in_channels, filters[0], num_layers)]\n",
    "        attention_blocks = [\n",
    "            Attention(\n",
    "                d_k=64, \n",
    "                dropout=0.1, \n",
    "                num_heads=num_heads, \n",
    "                num_channels=filters[0], \n",
    "            )\n",
    "        ] if has_attention else []\n",
    "        \n",
    "        for i in range(1, len(filters)):\n",
    "            conv_blocks.append(ConvBlock(filters[i-1], filters[i], num_layers))\n",
    "            if has_attention:\n",
    "                attention_blocks.append(\n",
    "                    Attention(\n",
    "                        d_k=64, \n",
    "                        dropout=0.1, \n",
    "                        num_heads=num_heads, \n",
    "                        num_channels=filters[i], \n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(*conv_blocks)\n",
    "        self.attention_blocks = nn.Sequential(*attention_blocks)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual_outputs = []\n",
    "        for conv_block in self.conv_blocks:\n",
    "            x = conv_block(x)\n",
    "            if self.has_attention:\n",
    "                x = self.attention_blocks(x)\n",
    "            \n",
    "            residual_outputs.append(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        return residual_outputs, x\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling (right) side of the UNet.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            filters: list[int], \n",
    "            num_layers: int, \n",
    "            has_attention: bool = False, \n",
    "            num_heads: int = 8, \n",
    "            dropout: float = 0.2\n",
    "            ):\n",
    "        super(UpBlock, self).__init__()\n",
    "        \n",
    "        conv_layers = []\n",
    "        attention_layers = []\n",
    "        \n",
    "        for i in range(len(filters) - 2):\n",
    "            conv_layers.append(\n",
    "                nn.Sequential(\n",
    "                    ConvBlock(filters[i], filters[i+1], num_layers), \n",
    "                    nn.ConvTranspose2d(filters[i+1], filters[i+1]//2, 2, stride=2)\n",
    "                )\n",
    "            )\n",
    "            if has_attention:\n",
    "                attention_layers.append(\n",
    "                    Attention(\n",
    "                        d_k=64, \n",
    "                        dropout=0.1, \n",
    "                        num_heads=num_heads, \n",
    "                        num_channels=filters[i+1]//2, \n",
    "                    )\n",
    "                )\n",
    "        conv_layers.append(ConvBlock(filters[-2], filters[-1], num_layers))\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "        self.attention_layers = nn.Sequential(*attention_layers)\n",
    "    \n",
    "    def forward(self, x, residual_outputs):\n",
    "        for i in range(len(self.conv_layers)):\n",
    "            residual = residual_outputs[-(i+1)]\n",
    "            _, _, h, w = x.shape\n",
    "            residual = residual[:, :, :h, :w]\n",
    "            x = torch.cat([x, residual], dim=1)\n",
    "            \n",
    "            x = self.conv_layers[i](x)\n",
    "            if self.has_attention:\n",
    "                x = self.attention_layers[i](x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_encoding(curr_t: torch.Tensor, T: torch.Tensor, embedding_dim: int, n=10000):\n",
    "    \"\"\"\n",
    "    Naive sin/cosin positional embedding adapted for timestep embedding in DDPM\n",
    "    \"\"\"\n",
    "    curr_t = curr_t / T # normalize the timestep to be between 0 and 1\n",
    "    p = torch.zeros((curr_t.shape[-1], embedding_dim)) # initialize the positional embedding tensor\n",
    "\n",
    "    m = torch.arange(int(embedding_dim/2)) # this is divided by two because we alternate between sin and cos\n",
    "    denominators = torch.float_power(n, 2*m/embedding_dim) # compute the denominators for the sin and cos functions\n",
    "    \n",
    "    p[:, 0::2] = torch.sin(curr_t.unsqueeze(1) / denominators.unsqueeze(0))\n",
    "    p[:, 1::2] = torch.cos(curr_t.unsqueeze(1) / denominators.unsqueeze(0))\n",
    "    return p\n",
    "\n",
    "\n",
    "\n",
    "class TimestepEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeds the timestep into a higher dimensional space using a 2 layer MLP.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                in_channels: int, \n",
    "                embedding_dim: int, \n",
    "                activation: nn.Module = nn.ReLU, \n",
    "                post_activation: bool = True\n",
    "                ):\n",
    "        super(TimestepEmbedding, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_channels, embedding_dim)\n",
    "        self.linear2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.activation = activation()\n",
    "        self.post_activation = post_activation\n",
    "\n",
    "    def forward(self, curr_t: torch.Tensor, T: torch.Tensor):\n",
    "        x = self.linear1(curr_t)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        if self.post_activation:\n",
    "            x = self.activation(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet model for DDPM.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 down_filters, \n",
    "                 in_channels, \n",
    "                 num_layers, \n",
    "                 has_attention: bool = False, \n",
    "                 num_heads: int = 8,\n",
    "                 diffusion_steps: int = None,\n",
    "                 num_groups: int = 8,\n",
    "                 activation: nn.Module = nn.ReLU\n",
    "                ):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.T = diffusion_steps\n",
    "        self.down_filters = down_filters\n",
    "\n",
    "        self.time_embed_dim = down_filters[0] * 4 \n",
    "        \n",
    "        if self.T is not None:\n",
    "            self.timestep_embedding = TimestepEmbedding(\n",
    "                in_channels=in_channels, \n",
    "                embedding_dim=in_channels, \n",
    "                activation=activation, \n",
    "                post_activation=True\n",
    "            )\n",
    "        \n",
    "        \n",
    "        self.down_block = DownBlock(\n",
    "            filters=down_filters, \n",
    "            num_layers=num_layers, \n",
    "            in_channels=in_channels, \n",
    "            has_attention=has_attention, \n",
    "            num_heads=num_heads\n",
    "        )\n",
    "        \n",
    "        # the bottom-most (middle) conv block \n",
    "        if has_attention:\n",
    "            self.bottom_conv = nn.Sequential(\n",
    "                ConvBlock(down_filters[-1], down_filters[-1]*2, num_layers=num_layers), \n",
    "                Attention(\n",
    "                    d_k=64, \n",
    "                    dropout=0.1, \n",
    "                    num_heads=num_heads, \n",
    "                    num_channels=down_filters[-1]*2, \n",
    "                ),\n",
    "                nn.ConvTranspose2d(down_filters[-1]*2, down_filters[-1], 2, stride=2)\n",
    "            )\n",
    "        else:\n",
    "            self.bottom_conv = nn.Sequential(\n",
    "                ConvBlock(down_filters[-1], down_filters[-1]*2, num_layers=num_layers), \n",
    "                nn.ConvTranspose2d(down_filters[-1]*2, down_filters[-1], 2, stride=2)\n",
    "            )\n",
    "        \n",
    "        self.up_filters = [down_filters[-1]*2]\n",
    "        self.up_filters.extend(reversed(down_filters))\n",
    "        self.up_block = UpBlock(self.up_filters, num_layers)\n",
    "\n",
    "        self.group_norm = nn.GroupNorm(num_groups=num_groups, num_channels=in_channels)\n",
    "        self.conv_out = nn.Conv2d(down_filters[-1], in_channels, 3, padding=1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        if self.T is not None:\n",
    "            t_emb = self.timestep_embedding(curr_t=t, T=self.T)\n",
    "            t_emb = t_emb.view(-1, self.time_embed_dim, 1, 1)\n",
    "\n",
    "        x = self.group_norm(x)\n",
    "        residual_outputs, down_output = self.down_block(x)\n",
    "        bottom_output = self.bottom_conv(down_output)\n",
    "        return self.up_block(bottom_output, residual_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = torch.randn(2, 3, 128, 128)\n",
    "conv_block = ConvBlock(3, 32, 2)\n",
    "h_example_image = conv_block(example_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16384, 32])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_block = Attention(d_k=64, dropout=0.1, num_heads=3, num_channels=32, num_groups=8)\n",
    "attn_output = attention_block(h_example_image)\n",
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(15, 3, 128, 128)\n",
    "T = 1000\n",
    "batch_size = x.shape[0]\n",
    "t = torch.randint(0, T, (batch_size,))\n",
    "e = torch.randn_like(x)\n",
    "alpha_bar_t = alpha_bar(t)\n",
    "print(alpha_bar_t.shape)\n",
    "alpha_bar_t = alpha_bar_t.view(-1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_bar_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, valid_loader, loss_fn, all_valid_loss):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            x, y = batch\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            valid_loss.append(loss.item())\n",
    "    \n",
    "    all_valid_loss.append(sum(valid_loss) / len(valid_loss))\n",
    "\n",
    "def plot_loss(all_train_loss, all_valid_loss):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(all_train_loss, label='Training Loss')\n",
    "    plt.plot(all_valid_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module,\n",
    "                optim: torch.optim.Optimizer,\n",
    "                loss_fn,\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                scheduler,\n",
    "                epochs=10,\n",
    "                valid_every=1\n",
    "                ):\n",
    "\n",
    "    all_train_loss = []\n",
    "    all_valid_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x, _ = batch\n",
    "            (noisy_images, t), e = prepare_batch(x, T, alpha_bar_t)\n",
    "            optim.zero_grad()\n",
    "            y_pred = model(noisy_images)\n",
    "            loss = loss_fn(y_pred, e)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        all_train_loss.append(sum(train_loss) / len(train_loss))\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % valid_every == 0:\n",
    "            validate_model(model, valid_loader, loss_fn, all_valid_loss)\n",
    "            print(\n",
    "                f\"Epoch {epoch}, Train Loss: {sum(train_loss) / len(train_loss)}, \"\n",
    "                f\"Valid Loss: {all_valid_loss[-1]}\"\n",
    "            )\n",
    "    \n",
    "    plot_loss(all_train_loss, all_valid_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
