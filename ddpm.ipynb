{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Models\n",
    "\n",
    "In this notebook, we'll implement the Denoising Diffusion Probabilistic Model (DDPM) proposed in the paper [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) from scratch using PyTorch. We'll implement the U-Net model, the forward and reverse diffusion processes, the training loop and the sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_bar(beta):\n",
    "    alpha = 1. - beta\n",
    "    return alpha.cumprod(dim=0)\n",
    "\n",
    "def prepare_batch(x: torch.Tensor, T: int, alpha_bar: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Prepare a batch for training by generating the noise and the noisy image.\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    t = torch.randint(0, T, (batch_size,))\n",
    "    e = torch.randn_like(x)\n",
    "    alpha_bar_t = alpha_bar(t)\n",
    "    alpha_bar_t = alpha_bar_t.view(-1, 1, 1, 1)\n",
    "    noisy_images = alpha_bar_t.sqrt() * x + (1 - alpha_bar_t).sqrt() * e\n",
    "    \n",
    "    return (noisy_images, t), e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet\n",
    "\n",
    "We split the U-net into 3 blocks, Left, Middle and Right.\n",
    "\n",
    "At the root of it, we have the conv block. Each layer in the left and the right blocks consists of a series of ConvBlocks followed by a maxpool or an upsample. \n",
    "\n",
    "The convblocks are resnets with group normalization and are imported from src.resnet.py. We changed the original resnets to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes that need to be made to a regular U-Net for DDPM:\n",
    "1. Swap batch norm with group norm \n",
    "2. Introduce an attention mechanism at each conv block in the down and up blocks\n",
    "3. Create an embedding for the timestep\n",
    "4. Develop a 2D attention mechanism based on the one used in the text transformer model in [aryamanpandya99/transformers](https://github.com/aryamanpandya99/transformers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.resnet import ResBlock\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels: int, \n",
    "            out_channels: int, \n",
    "            num_layers: int, \n",
    "            num_groups: int = 1, \n",
    "            dropout: float = 0.2, \n",
    "            activation: nn.Module = nn.ReLU,\n",
    "            timestep_emb_dim: int = None\n",
    "            ):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        convs = []\n",
    "        convs.append(\n",
    "            ResBlock(\n",
    "                in_channels, \n",
    "                out_channels, \n",
    "                num_groups=num_groups, \n",
    "                dropout=dropout, \n",
    "                activation=activation,\n",
    "                timestep_emb_dim=timestep_emb_dim\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for _ in range(num_layers-1):\n",
    "            convs.append(\n",
    "                ResBlock(\n",
    "                    out_channels,\n",
    "                    out_channels, \n",
    "                    num_groups=num_groups, \n",
    "                    dropout=dropout, \n",
    "                    activation=activation,\n",
    "                    timestep_emb_dim=timestep_emb_dim\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "\n",
    "    def forward(self, x, timestep_emb=None):\n",
    "        for res_block in self.convs:\n",
    "            x = res_block(x, timestep_emb)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, d_k, mask):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is True:\n",
    "        mask = torch.tril(torch.ones(scores.shape)).to(q.device)\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    return nn.Softmax(-1)(scores)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead attention.\n",
    "\n",
    "    Differences from multihead attention for text:\n",
    "    \n",
    "    - we no longer need a d_model, the internal hidden size \n",
    "    is determined by the number of channels which is determined \n",
    "    by the convolutional layers leading up to the attention layer.\n",
    "\n",
    "    - swap batch norm with group norm\n",
    "\n",
    "    - we resize the image to one of shape: (batch_size, num_channels, height * width)\n",
    "    so that we can perform multihead attention across the image. This closely \n",
    "    mirrors (batch_size, embed_dim, seq_len\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_k: int, \n",
    "                 dropout: float, \n",
    "                 num_heads: int, \n",
    "                 num_channels: int,\n",
    "                 num_groups: int = 8,\n",
    "                 mask: bool = False\n",
    "                 ):\n",
    "        super(Attention, self).__init__()\n",
    "        self.d_k, self.num_heads = d_k, num_heads\n",
    "        self.query_projection, self.key_projection, self.value_projection = (\n",
    "            nn.Linear(num_channels, num_heads* d_k),\n",
    "            nn.Linear(num_channels, num_heads* d_k), \n",
    "            nn.Linear(num_channels, num_heads*d_k)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(num_channels)\n",
    "        self.output_layer = nn.Linear(num_heads*d_k, num_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.mask = mask\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        \n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        x = x.view(batch_size, n_channels, height * width).permute(0, 2, 1)\n",
    "        residual = x\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        if y is not None:\n",
    "            k, q, v = y, x, y\n",
    "        else:\n",
    "            k, q, v = x, x, x\n",
    "        \n",
    "        k_len, q_len, v_len, batch_size = k.size(1), q.size(1), v.size(1),  q.size(0)\n",
    "        \n",
    "        k = self.key_projection(k).view(batch_size, k_len,  self.num_heads, self.d_k)\n",
    "        q = self.query_projection(q).view(batch_size, q_len,  self.num_heads, self.d_k)\n",
    "        v = self.value_projection(v).view(batch_size, v_len,  self.num_heads, self.d_k)\n",
    "        \n",
    "        attention = scaled_dot_product_attention(\n",
    "            q.transpose(1, 2), \n",
    "            k.transpose(1, 2), \n",
    "            self.d_k, \n",
    "            self.mask\n",
    "        )\n",
    "        output = torch.matmul(attention, v.transpose(1, 2))\n",
    "        output = self.output_layer(output.transpose(1, 2).contiguous().view(batch_size, q_len, -1))\n",
    "\n",
    "        h = self.dropout(output) + residual\n",
    "\n",
    "        h = h.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n",
    "        \n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = torch.randn(2, 3, 128, 128)\n",
    "conv_block = ConvBlock(3, 32, 2)\n",
    "h_example_image = conv_block(example_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 128, 128])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_block = Attention(d_k=64, dropout=0.1, num_heads=3, num_channels=32, num_groups=8)\n",
    "attn_output = attention_block(h_example_image)\n",
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_encoding(curr_t: torch.Tensor, T: torch.Tensor, embedding_dim: int, n=10000, device: torch.device = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Naive sin/cosin positional embedding adapted for timestep embedding in DDPM\n",
    "    \"\"\"\n",
    "    curr_t = curr_t / T # normalize the timestep to be between 0 and 1\n",
    "    p = torch.zeros((curr_t.shape[-1], embedding_dim)).to(device) # initialize the positional embedding tensor\n",
    "\n",
    "    m = torch.arange(int(embedding_dim/2)).to(device) # this is divided by two because we alternate between sin and cos\n",
    "    denominators = torch.pow(n, (2*m/embedding_dim))  # compute the denominators for the sin and cos functions\n",
    "    \n",
    "    p[:, 0::2] = torch.sin(curr_t.unsqueeze(1) / denominators.unsqueeze(0))\n",
    "    p[:, 1::2] = torch.cos(curr_t.unsqueeze(1) / denominators.unsqueeze(0))\n",
    "    return p\n",
    "\n",
    "\n",
    "\n",
    "class TimestepEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeds the timestep into a higher dimensional space using a 2 layer MLP.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                in_channels: int, \n",
    "                embedding_dim: int, \n",
    "                activation: nn.Module = nn.ReLU\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: number of input channels\n",
    "            embedding_dim: dimension of the embedding space\n",
    "            activation: activation function\n",
    "        \"\"\"\n",
    "        super(TimestepEmbedding, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_channels, embedding_dim)\n",
    "        self.linear2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.activation = activation()\n",
    "\n",
    "    def forward(self, curr_t: torch.Tensor, T: torch.Tensor):\n",
    "        print(curr_t.shape)\n",
    "        x = self.linear1(curr_t)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeftBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Downampling (left) side of the UNet.\n",
    "    Excludes the bottom-most conv block.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels: int, \n",
    "            filters: list[int], \n",
    "            num_layers: int, \n",
    "            has_attention: bool = False, \n",
    "            num_heads: int = 8, \n",
    "            dropout: float = 0.2, \n",
    "            timestep_emb_dim: int = None\n",
    "            ):\n",
    "        super(LeftBlock, self).__init__()\n",
    "        \n",
    "        self.has_attention = has_attention\n",
    "        conv_blocks = [ConvBlock(in_channels, filters[0], num_layers, timestep_emb_dim=timestep_emb_dim)]\n",
    "        attention_blocks = [\n",
    "            Attention(\n",
    "                d_k=64, \n",
    "                dropout=0.1, \n",
    "                num_heads=num_heads, \n",
    "                num_channels=filters[0], \n",
    "            )\n",
    "        ] if has_attention else []\n",
    "        \n",
    "        for i in range(1, len(filters)):\n",
    "            conv_blocks.append(ConvBlock(filters[i-1], filters[i], num_layers, timestep_emb_dim=timestep_emb_dim))\n",
    "            if has_attention:\n",
    "                attention_blocks.append(\n",
    "                    Attention(\n",
    "                        d_k=64, \n",
    "                        dropout=0.1, \n",
    "                        num_heads=num_heads, \n",
    "                        num_channels=filters[i], \n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        self.conv_blocks = nn.ModuleList(conv_blocks)\n",
    "        self.attention_blocks = nn.ModuleList(attention_blocks)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x, timestep_emb=None):\n",
    "        residual_outputs = []\n",
    "        for i, conv_block in enumerate(self.conv_blocks):\n",
    "            x = conv_block(x, timestep_emb)\n",
    "            if self.has_attention:\n",
    "                x = self.attention_blocks[i](x)\n",
    "            \n",
    "            residual_outputs.append(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        return residual_outputs, x\n",
    "\n",
    "\n",
    "class RightBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling (right) side of the UNet.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            filters: list[int], \n",
    "            num_layers: int, \n",
    "            has_attention: bool = False, \n",
    "            num_heads: int = 8, \n",
    "            dropout: float = 0.2,\n",
    "            timestep_emb_dim: int = None,\n",
    "            ):\n",
    "        super(RightBlock, self).__init__()\n",
    "        self.has_attention = has_attention\n",
    "\n",
    "        conv_layers = []\n",
    "        upsample_layers = []\n",
    "        attention_layers = []\n",
    "        \n",
    "        for i in range(len(filters) - 2):\n",
    "            conv_layers.append(\n",
    "                ConvBlock(filters[i], filters[i+1], num_layers, timestep_emb_dim=timestep_emb_dim)\n",
    "            )\n",
    "            upsample_layers.append(\n",
    "                nn.ConvTranspose2d(filters[i+1], filters[i+1]//2, 2, stride=2)\n",
    "            )\n",
    "\n",
    "            if has_attention:\n",
    "                attention_layers.append(\n",
    "                    Attention(d_k=64, dropout=0.1, num_heads=num_heads, num_channels=filters[i+1]//2)\n",
    "                )\n",
    "        conv_layers.append(\n",
    "            ConvBlock(filters[-2], filters[-1], num_layers, timestep_emb_dim=timestep_emb_dim)\n",
    "        )\n",
    "\n",
    "        self.conv_layers = nn.ModuleList(conv_layers)\n",
    "        self.attention_layers = nn.ModuleList(attention_layers)\n",
    "        self.upsample_layers = nn.ModuleList(upsample_layers)\n",
    "    \n",
    "    def forward(self, x, residual_outputs, timestep_emb=None):\n",
    "        for i in range(len(self.conv_layers)):\n",
    "            residual = residual_outputs[-(i+1)]\n",
    "            _, _, h, w = x.shape\n",
    "            residual = residual[:, :, :h, :w]\n",
    "\n",
    "            x = torch.cat([x, residual], dim=1)\n",
    "            x = self.conv_layers[i](x, timestep_emb)\n",
    "\n",
    "            if i < len(self.upsample_layers):\n",
    "                x = self.upsample_layers[i](x)\n",
    "\n",
    "                if self.has_attention:\n",
    "                    x = self.attention_layers[i](x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = LeftBlock(in_channels=3, filters=[32, 64, 128], num_layers=2, has_attention=True, num_heads=3, timestep_emb_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32])\n"
     ]
    }
   ],
   "source": [
    "timesteps = torch.randint(0, 1000, (2,))\n",
    "t_encoded = timestep_encoding(timesteps, 1000, 32)\n",
    "t_encoded.shape # N x 32\n",
    "\n",
    "time_embedding_layer = TimestepEmbedding(in_channels=32, embedding_dim=128, activation=nn.ReLU) # N x 32 -> N x 128 (embedding_dim)\n",
    "t_embedded = time_embedding_layer(curr_t=t_encoded, T=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 16, 16])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = db(torch.randn(2, 3, 128, 128), timestep_emb=t_embedded)\n",
    "a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 128, 128])\n",
      "----------\n",
      "torch.Size([2, 64, 64, 64])\n",
      "----------\n",
      "torch.Size([2, 128, 32, 32])\n",
      "----------\n",
      "torch.Size([2, 128, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "for residual_output in a[0]:\n",
    "    print(residual_output.shape)\n",
    "    print(\"-\"*10)\n",
    "\n",
    "print(a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 32, 32])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_conv = nn.Sequential(ResBlock(128, 256), nn.ConvTranspose2d(256, 128, 2, stride=2))\n",
    "bottom_conv(a[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "up = RightBlock(filters=[256, 128, 64, 32], num_layers=2, has_attention=True, num_heads=3, timestep_emb_dim=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 32, 32])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_out = bottom_conv(a[1])\n",
    "bottom_out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 128, 128])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "up_out = up(bottom_out, a[0], t_embedded)\n",
    "up_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet model for DDPM.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 down_filters: list[int], \n",
    "                 in_channels: int, \n",
    "                 num_layers: int, \n",
    "                 has_attention: bool = False, \n",
    "                 num_heads: int = 8,\n",
    "                 diffusion_steps: int = None,\n",
    "                 num_groups: int = 8,\n",
    "                 activation: nn.Module = nn.ReLU,\n",
    "                 timestep_emb_dim: int = None\n",
    "                ):\n",
    "        super(UNet, self).__init__()\n",
    "        self.T = diffusion_steps\n",
    "        self.down_filters = down_filters\n",
    "\n",
    "        self.time_embed_dim = down_filters[0] * 4 \n",
    "        \n",
    "        if self.T is not None:\n",
    "            self.timestep_embedding = TimestepEmbedding(\n",
    "                in_channels=self.down_filters[0], \n",
    "                embedding_dim=self.time_embed_dim, \n",
    "                activation=activation, \n",
    "            )\n",
    "        \n",
    "        \n",
    "        self.left_block = LeftBlock(\n",
    "            filters=down_filters, \n",
    "            num_layers=num_layers, \n",
    "            in_channels=in_channels, \n",
    "            has_attention=has_attention, \n",
    "            num_heads=num_heads\n",
    "        )\n",
    "        \n",
    "        # the bottom-most (middle) conv block \n",
    "        if has_attention:\n",
    "            self.middle_conv = ConvBlock(\n",
    "                down_filters[-1], \n",
    "                down_filters[-1]*2, \n",
    "                num_layers, \n",
    "                timestep_emb_dim=timestep_emb_dim\n",
    "            )\n",
    "            self.middle_attention = Attention(d_k=64, dropout=0.1, num_heads=num_heads, num_channels=down_filters[-1]*2)\n",
    "            self.middle_upsample = nn.ConvTranspose2d(down_filters[-1]*2, down_filters[-1], 2, stride=2)\n",
    "        else:\n",
    "            self.middle_conv = ConvBlock(\n",
    "                down_filters[-1], \n",
    "                down_filters[-1]*2, \n",
    "                num_layers, \n",
    "                timestep_emb_dim=timestep_emb_dim\n",
    "            )\n",
    "            self.middle_attention = None\n",
    "            self.middle_upsample = nn.ConvTranspose2d(down_filters[-1]*2, down_filters[-1], 2, stride=2)\n",
    "        \n",
    "        self.up_filters = [down_filters[-1]*2]\n",
    "        self.up_filters.extend(reversed(down_filters))\n",
    "        self.right_block = RightBlock(\n",
    "            filters=self.up_filters, \n",
    "            num_layers=num_layers, \n",
    "            has_attention=has_attention, \n",
    "            num_heads=num_heads,\n",
    "            timestep_emb_dim=timestep_emb_dim\n",
    "        )\n",
    "\n",
    "        self.group_norm = nn.GroupNorm(num_groups=in_channels, num_channels=in_channels)\n",
    "        self.conv_out = nn.Conv2d(down_filters[0], in_channels, 3, padding=1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        if self.T is not None:\n",
    "            print(t.shape)\n",
    "            t_encoded = timestep_encoding(t, self.T, self.down_filters[0], n = 4000, device=x.device)\n",
    "            print(t_encoded.shape)\n",
    "            t_emb = self.timestep_embedding(curr_t=t_encoded, T=self.T)\n",
    "            t_emb = t_emb.view(-1, self.time_embed_dim, 1, 1)\n",
    "        else: \n",
    "            t_emb = None\n",
    "\n",
    "        x = self.group_norm(x)\n",
    "        print(\"group norm complete\")\n",
    "        residual_outputs, down_output = self.left_block(x, t_emb)\n",
    "        print(\"left block complete\")\n",
    "        bottom_output = self.middle_conv(down_output, t_emb)\n",
    "        if self.middle_attention is not None:\n",
    "            bottom_output = self.middle_attention(bottom_output)\n",
    "        print(\"middle attention complete\")\n",
    "        bottom_output = self.middle_upsample(bottom_output)\n",
    "\n",
    "        right_out = self.right_block(bottom_output, residual_outputs, t_emb)\n",
    "        print(\"right block complete\")\n",
    "        output = self.conv_out(right_out)\n",
    "        print(\"conv out complete\")\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_net = UNet(down_filters=[32, 64, 128], in_channels=3, num_layers=2, has_attention=True, num_heads=3, diffusion_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The UNet model has 4,442,319 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_params = count_parameters(u_net)\n",
    "print(f\"The UNet model has {num_params:,} trainable parameters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15])\n",
      "torch.Size([15, 32])\n",
      "torch.Size([15, 32])\n",
      "group norm complete\n",
      "left block complete\n",
      "middle attention complete\n",
      "right block complete\n",
      "conv out complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 32, 32])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "sample_inputs = torch.randn(16, 3, 32, 32).to(device)\n",
    "u_net.to(device)\n",
    "sample_outputs = u_net(sample_inputs, torch.randint(0, 1000, (15,)).to(device))\n",
    "sample_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(15, 3, 128, 128)\n",
    "T = 1000\n",
    "batch_size = x.shape[0]\n",
    "t = torch.randint(0, T, (batch_size,))\n",
    "e = torch.randn_like(x)\n",
    "alpha_bar_t = alpha_bar(t)\n",
    "print(alpha_bar_t.shape)\n",
    "alpha_bar_t = alpha_bar_t.view(-1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 1, 1, 1])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_bar_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, valid_loader, loss_fn, all_valid_loss):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            x, y = batch\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            valid_loss.append(loss.item())\n",
    "    \n",
    "    all_valid_loss.append(sum(valid_loss) / len(valid_loss))\n",
    "\n",
    "def plot_loss(all_train_loss, all_valid_loss):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(all_train_loss, label='Training Loss')\n",
    "    plt.plot(all_valid_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module,\n",
    "                optim: torch.optim.Optimizer,\n",
    "                loss_fn: nn.Module,\n",
    "                train_loader: torch.utils.data.DataLoader,\n",
    "                valid_loader: torch.utils.data.DataLoader,\n",
    "                scheduler: torch.optim.lr_scheduler._LRScheduler,\n",
    "                epochs: int = 10,\n",
    "                valid_every: int = 1\n",
    "                ):\n",
    "\n",
    "    all_train_loss = []\n",
    "    all_valid_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x, _ = batch\n",
    "            (noisy_images, t), e = prepare_batch(x, T, alpha_bar_t)\n",
    "            optim.zero_grad()\n",
    "            y_pred = model(noisy_images)\n",
    "            loss = loss_fn(y_pred, e)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        all_train_loss.append(sum(train_loss) / len(train_loss))\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % valid_every == 0:\n",
    "            validate_model(model, valid_loader, loss_fn, all_valid_loss)\n",
    "            print(\n",
    "                f\"Epoch {epoch}, Train Loss: {sum(train_loss) / len(train_loss)}, \"\n",
    "                f\"Valid Loss: {all_valid_loss[-1]}\"\n",
    "            )\n",
    "    \n",
    "    plot_loss(all_train_loss, all_valid_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
