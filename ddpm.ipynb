{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Models\n",
    "\n",
    "In this notebook, we'll implement the Denoising Diffusion Probabilistic Model (DDPM) proposed in the paper [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) from scratch using PyTorch. We'll implement the U-Net model, the forward and reverse diffusion processes, the training loop and the sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet\n",
    "\n",
    "The U-net is implemented as in the original paper. It's been divided into blocks to make it more modular and easier to work with. The \"downblock\" consists of a series of convolutional layers, each followed by a maxpooling layer. The \"upblock\" consists of a convolutional layer followed by a transpose convolutional layer. The bottom-most conv block is implemented separately and is different from the rest of the blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.resnet import ResBlock\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels: int, \n",
    "            out_channels: int, \n",
    "            num_layers: int, \n",
    "            num_groups: int = 1, \n",
    "            dropout: float = 0.2, \n",
    "            activation: nn.Module = nn.ReLU\n",
    "            ):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        convs = []\n",
    "        convs.append(\n",
    "            ResBlock(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            num_groups=num_groups, \n",
    "            dropout=dropout, \n",
    "            activation=activation\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for _ in range(num_layers-1):\n",
    "            convs.append(\n",
    "                ResBlock(\n",
    "                out_channels,\n",
    "                out_channels, \n",
    "                num_groups=num_groups, \n",
    "                dropout=dropout, \n",
    "                activation=activation\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for conv in self.convs:\n",
    "            x = conv(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Downampling (left) side of the UNet.\n",
    "    Excludes the bottom-most conv block.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, filters: list[int], num_layers: int):\n",
    "        super(DownBlock, self).__init__()\n",
    "        conv_blocks = [ConvBlock(in_channels, filters[0], num_layers)]\n",
    "        for i in range(1, len(filters)):\n",
    "            conv_blocks.append(ConvBlock(filters[i-1], filters[i], num_layers))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(*conv_blocks)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual_outputs = []\n",
    "        for conv_block in self.conv_blocks:\n",
    "            x = conv_block(x)\n",
    "            residual_outputs.append(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        return residual_outputs, x\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling (right) side of the UNet.\n",
    "    \"\"\"\n",
    "    def __init__(self, filters: list[int], num_layers: int):\n",
    "        super(UpBlock, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(filters) - 2):\n",
    "            layers.append(\n",
    "                nn.Sequential(\n",
    "                    ConvBlock(filters[i], filters[i+1], num_layers), \n",
    "                    nn.ConvTranspose2d(filters[i+1], filters[i+1]//2, 2, stride=2)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        layers.append(ConvBlock(filters[-2], filters[-1], num_layers))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x, residual_outputs):\n",
    "        for i in range(len(self.layers)):\n",
    "            residual = residual_outputs[-(i+1)]\n",
    "            _, _, h, w = x.shape\n",
    "            residual = residual[:, :, :h, :w]\n",
    "            x = torch.cat([x, residual], dim=1)\n",
    "            x = self.layers[i](x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DownBlock(in_channels=3, filters=[32, 64, 128], num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DownBlock(\n",
       "  (conv_blocks): Sequential(\n",
       "    (0): ConvBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): GroupNorm(1, 3, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (3): ReLU()\n",
       "            (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (5): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (idconv): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (avgpool): Identity()\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (3): ReLU()\n",
       "            (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (5): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (idconv): Identity()\n",
       "          (avgpool): Identity()\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ConvBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (3): ReLU()\n",
       "            (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (5): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (idconv): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (avgpool): Identity()\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (3): ReLU()\n",
       "            (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (5): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (idconv): Identity()\n",
       "          (avgpool): Identity()\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ConvBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (3): ReLU()\n",
       "            (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (5): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (idconv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (avgpool): Identity()\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (conv): Sequential(\n",
       "            (0): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "            (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (3): ReLU()\n",
       "            (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (5): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (idconv): Identity()\n",
       "          (avgpool): Identity()\n",
       "          (activation): ReLU()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 16, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = db(torch.randn(1, 3, 128, 128))\n",
    "a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 128, 128])\n",
      "----------\n",
      "torch.Size([1, 64, 64, 64])\n",
      "----------\n",
      "torch.Size([1, 128, 32, 32])\n",
      "----------\n",
      "torch.Size([1, 128, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "for residual_output in a[0]:\n",
    "    print(residual_output.shape)\n",
    "    print(\"-\"*10)\n",
    "\n",
    "print(a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 32, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_conv = nn.Sequential(ResBlock(128, 256), nn.ConvTranspose2d(256, 128, 2, stride=2))\n",
    "bottom_conv(a[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "up = UpBlock([256, 128, 64, 32], 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): ConvBlock(\n",
      "    (convs): ModuleList(\n",
      "      (0): ResBlock(\n",
      "        (conv): Sequential(\n",
      "          (0): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): ReLU()\n",
      "          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (5): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (idconv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (avgpool): Identity()\n",
      "        (activation): ReLU()\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): ResBlock(\n",
      "        (conv): Sequential(\n",
      "          (0): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): ReLU()\n",
      "          (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (5): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (idconv): Identity()\n",
      "        (avgpool): Identity()\n",
      "        (activation): ReLU()\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      ")\n",
      "----------\n",
      "Sequential(\n",
      "  (0): ConvBlock(\n",
      "    (convs): ModuleList(\n",
      "      (0): ResBlock(\n",
      "        (conv): Sequential(\n",
      "          (0): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): ReLU()\n",
      "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (5): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (idconv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (avgpool): Identity()\n",
      "        (activation): ReLU()\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (1): ResBlock(\n",
      "        (conv): Sequential(\n",
      "          (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "          (1): ReLU()\n",
      "          (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): ReLU()\n",
      "          (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (5): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "        )\n",
      "        (idconv): Identity()\n",
      "        (avgpool): Identity()\n",
      "        (activation): ReLU()\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      ")\n",
      "----------\n",
      "ConvBlock(\n",
      "  (convs): ModuleList(\n",
      "    (0): ResBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "        (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (5): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (idconv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (avgpool): Identity()\n",
      "      (activation): ReLU()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv): Sequential(\n",
      "        (0): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "        (4): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (5): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (idconv): Identity()\n",
      "      (avgpool): Identity()\n",
      "      (activation): ReLU()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for layer in up.layers:\n",
    "    print(layer)\n",
    "    print(\"-\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 32, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_out = bottom_conv(a[1])\n",
    "bottom_out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_out = up(bottom_out, a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, down_filters, in_channels, num_layers):\n",
    "        super(UNet, self).__init__()\n",
    "        self.down_filters = down_filters\n",
    "        self.down_block = DownBlock(filters=down_filters, num_layers=num_layers, in_channels=in_channels)\n",
    "        \n",
    "        # the bottom-most conv block is different in that it doesn't have a maxpool or a residual connection\n",
    "        self.bottom_conv = nn.Sequential(\n",
    "            ConvBlock(down_filters[-1], down_filters[-1]*2, num_layers=num_layers), \n",
    "            nn.ConvTranspose2d(down_filters[-1]*2, down_filters[-1], 2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.up_filters = [down_filters[-1]*2]\n",
    "        self.up_filters.extend(reversed(down_filters))\n",
    "        self.up_block = UpBlock(self.up_filters, num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual_outputs, down_output = self.down_block(x)\n",
    "        bottom_output = self.bottom_conv(down_output)\n",
    "        return self.up_block(bottom_output, residual_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 128, 128])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_net = UNet([32, 64, 128], 3, 2)\n",
    "a = u_net(torch.randn(1, 3, 128, 128))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_bar(beta):\n",
    "    alpha = 1. - beta\n",
    "    return alpha.cumprod(dim=0)\n",
    "\n",
    "def prepare_batch(x: torch.Tensor, T: int, alpha_bar: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Prepare a batch for training by generating the noise and the noisy image.\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    t = torch.randint(0, T, (batch_size,))\n",
    "    e = torch.randn_like(x)\n",
    "    alpha_bar_t = alpha_bar(t)\n",
    "    alpha_bar_t = alpha_bar_t.view(-1, 1, 1, 1)\n",
    "    noisy_images = alpha_bar_t.sqrt() * x + (1 - alpha_bar_t).sqrt() * e\n",
    "    \n",
    "    return (noisy_images, t), e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes that need to be made to the U-Net for DDPM:\n",
    "1. Swap batch norm with group norm \n",
    "2. Introduce an attention mechanism at each conv block in the down and up blocks\n",
    "3. Create an embedding for the timestep\n",
    "\n",
    "Next, we'll implement these missing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scaled_dot_product_attention(q, k, d_k, mask):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is True:\n",
    "        mask = torch.tril(torch.ones(scores.shape)).to(q.device)\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    return nn.Softmax(-1)(scores)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead attention.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_k, d_model, d_v, dropout, num_heads, mask) -> None:\n",
    "        super(Attention, self).__init__()\n",
    "        self.d_k, self.d_v, self.d_model, self.num_heads = d_k, d_v, d_model, num_heads\n",
    "        self.query_layer, self.key_layer, self.value_layer = (\n",
    "            nn.Linear(d_model, num_heads* d_k),\n",
    "            nn.Linear(d_model, num_heads* d_k), \n",
    "            nn.Linear(d_model, num_heads*d_v)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.concat_projection = nn.Linear(num_heads*d_v, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.mask = mask\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        residual = x\n",
    "        x = self.layer_norm(x)\n",
    "        if y is not None:\n",
    "            k, q, v = y, x, y\n",
    "        else:\n",
    "            k, q, v = x, x, x\n",
    "        \n",
    "        k_len, q_len, v_len, batch_size = k.size(1), q.size(1), v.size(1),  q.size(0)\n",
    "        k = self.key_layer(k).view(batch_size, k_len,  self.num_heads, self.d_k)\n",
    "        q = self.query_layer(q).view(batch_size, q_len,  self.num_heads, self.d_k)\n",
    "        v = self.value_layer(v).view(batch_size, v_len,  self.num_heads, self.d_v)\n",
    "        attention = scaled_dot_product_attention(q.transpose(1, 2), k.transpose(1, 2), self.d_k, self.mask)\n",
    "        output = torch.matmul(attention, v.transpose(1, 2))\n",
    "        output = self.concat_projection(output.transpose(1, 2).contiguous().view(batch_size, q_len, -1))\n",
    "        \n",
    "        return self.dropout(output) + residual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(15, 3, 128, 128)\n",
    "T = 1000\n",
    "batch_size = x.shape[0]\n",
    "t = torch.randint(0, T, (batch_size,))\n",
    "e = torch.randn_like(x)\n",
    "alpha_bar_t = alpha_bar(t)\n",
    "print(alpha_bar_t.shape)\n",
    "alpha_bar_t = alpha_bar_t.view(-1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_bar_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, valid_loader, loss_fn, all_valid_loss):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            x, y = batch\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            valid_loss.append(loss.item())\n",
    "    \n",
    "    all_valid_loss.append(sum(valid_loss) / len(valid_loss))\n",
    "\n",
    "def plot_loss(all_train_loss, all_valid_loss):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(all_train_loss, label='Training Loss')\n",
    "    plt.plot(all_valid_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module,\n",
    "                optim: torch.optim.Optimizer,\n",
    "                loss_fn,\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                scheduler,\n",
    "                epochs=10,\n",
    "                valid_every=1\n",
    "                ):\n",
    "\n",
    "    all_train_loss = []\n",
    "    all_valid_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x, _ = batch\n",
    "            (noisy_images, t), e = prepare_batch(x, T, alpha_bar_t)\n",
    "            optim.zero_grad()\n",
    "            y_pred = model(noisy_images)\n",
    "            loss = loss_fn(y_pred, e)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        all_train_loss.append(sum(train_loss) / len(train_loss))\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % valid_every == 0:\n",
    "            validate_model(model, valid_loader, loss_fn, all_valid_loss)\n",
    "            print(\n",
    "                f\"Epoch {epoch}, Train Loss: {sum(train_loss) / len(train_loss)}, \"\n",
    "                f\"Valid Loss: {all_valid_loss[-1]}\"\n",
    "            )\n",
    "    \n",
    "    plot_loss(all_train_loss, all_valid_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
