{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probabilistic Models\n",
    "\n",
    "In this notebook, we'll implement the Denoising Diffusion Probabilistic Model (DDPM) proposed in the paper [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) from scratch using PyTorch. We'll implement the U-Net model, the forward and reverse diffusion processes, the training loop and the sampling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha_bar(beta):\n",
    "    alpha = 1. - beta\n",
    "    return alpha.cumprod(dim=0)\n",
    "\n",
    "def prepare_batch(x: torch.Tensor, T: int, alpha_bar: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Prepare a batch for training by generating the noise and the noisy image.\n",
    "    \"\"\"\n",
    "    batch_size = x.shape[0]\n",
    "    t = torch.randint(0, T, (batch_size,))\n",
    "    e = torch.randn_like(x)\n",
    "    alpha_bar_t = alpha_bar(t)\n",
    "    alpha_bar_t = alpha_bar_t.view(-1, 1, 1, 1)\n",
    "    noisy_images = alpha_bar_t.sqrt() * x + (1 - alpha_bar_t).sqrt() * e\n",
    "    \n",
    "    return (noisy_images, t), e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes that need to be made to a regular U-Net for DDPM:\n",
    "1. Swap batch norm with group norm \n",
    "2. Introduce an attention mechanism at each conv block in the down and up blocks\n",
    "3. Create an embedding for the timestep\n",
    "4. Develop a 2D attention mechanism based on the one used in the text transformer model in [aryamanpandya99/transformers](https://github.com/aryamanpandya99/transformers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scaled_dot_product_attention(q, k, d_k, mask):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is True:\n",
    "        mask = torch.tril(torch.ones(scores.shape)).to(q.device)\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    return nn.Softmax(-1)(scores)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multihead attention.\n",
    "\n",
    "    Differences from multihead attention for text:\n",
    "    \n",
    "    - we no longer need a d_model, the internal hidden size \n",
    "    is determined by the number of channels which is determined \n",
    "    by the convolutional layers leading up to the attention layer.\n",
    "\n",
    "    - swap batch norm with group norm\n",
    "\n",
    "    - we resize the image to one of shape: (batch_size, num_channels, height * width)\n",
    "    so that we can perform multihead attention across the image. This closely \n",
    "    mirrors (batch_size, embed_dim, seq_len\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 d_k: int, \n",
    "                 dropout: float, \n",
    "                 num_heads: int, \n",
    "                 num_channels: int,\n",
    "                 num_groups: int = 8,\n",
    "                 mask: bool = False\n",
    "                 ):\n",
    "        super(Attention, self).__init__()\n",
    "        self.d_k, self.num_heads = d_k, num_heads\n",
    "        self.query_projection, self.key_projection, self.value_projection = (\n",
    "            nn.Linear(num_channels, num_heads* d_k),\n",
    "            nn.Linear(num_channels, num_heads* d_k), \n",
    "            nn.Linear(num_channels, num_heads*d_k)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(num_channels)\n",
    "        self.output_layer = nn.Linear(num_heads*d_k, num_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.mask = mask\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, x, y = None):\n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        x = x.view(batch_size, n_channels, height * width).permute(0, 2, 1)\n",
    "        residual = x\n",
    "        x = self.layer_norm(x)\n",
    "        \n",
    "        if y is not None:\n",
    "            k, q, v = y, x, y\n",
    "        else:\n",
    "            k, q, v = x, x, x\n",
    "        \n",
    "        k_len, q_len, v_len, batch_size = k.size(1), q.size(1), v.size(1),  q.size(0)\n",
    "        \n",
    "        k = self.key_projection(k).view(batch_size, k_len,  self.num_heads, self.d_k)\n",
    "        q = self.query_projection(q).view(batch_size, q_len,  self.num_heads, self.d_k)\n",
    "        v = self.value_projection(v).view(batch_size, v_len,  self.num_heads, self.d_k)\n",
    "        \n",
    "        attention = scaled_dot_product_attention(\n",
    "            q.transpose(1, 2), \n",
    "            k.transpose(1, 2), \n",
    "            self.d_k, \n",
    "            self.mask\n",
    "        )\n",
    "        output = torch.matmul(attention, v.transpose(1, 2))\n",
    "        output = self.output_layer(output.transpose(1, 2).contiguous().view(batch_size, q_len, -1))\n",
    "\n",
    "        h = self.dropout(output) + residual\n",
    "\n",
    "        h = h.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n",
    "        \n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet\n",
    "\n",
    "We split the U-net into 3 blocks, Left, Middle and Right.\n",
    "\n",
    "At the root of it, we have the conv block. Each layer in the left and the right blocks consists of a series of ConvBlocks followed by a maxpool or an upsample. \n",
    "\n",
    "The convblocks are resnets with group normalization and are imported from src.resnet.py. We changed the original resnets to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.resnet import ResBlock\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels: int, \n",
    "            out_channels: int, \n",
    "            num_layers: int, \n",
    "            num_groups: int = 1, \n",
    "            dropout: float = 0.2, \n",
    "            activation: nn.Module = nn.ReLU,\n",
    "            timestep_emb_dim: int = None\n",
    "            ):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        convs = []\n",
    "        convs.append(\n",
    "            ResBlock(\n",
    "                in_channels, \n",
    "                out_channels, \n",
    "                num_groups=num_groups, \n",
    "                dropout=dropout, \n",
    "                activation=activation,\n",
    "                timestep_emb_dim=timestep_emb_dim\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        for _ in range(num_layers-1):\n",
    "            convs.append(\n",
    "                ResBlock(\n",
    "                    out_channels,\n",
    "                    out_channels, \n",
    "                    num_groups=num_groups, \n",
    "                    dropout=dropout, \n",
    "                    activation=activation,\n",
    "                    timestep_emb_dim=timestep_emb_dim\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "\n",
    "    def forward(self, x, timestep_emb=None):\n",
    "        for res_block in self.convs:\n",
    "            x = res_block(x, timestep_emb)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class LeftBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Downampling (left) side of the UNet.\n",
    "    Excludes the bottom-most conv block.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels: int, \n",
    "            filters: list[int], \n",
    "            num_layers: int, \n",
    "            has_attention: bool = False, \n",
    "            num_heads: int = 8, \n",
    "            dropout: float = 0.2, \n",
    "            timestep_emb_dim: int = None\n",
    "            ):\n",
    "        super(LeftBlock, self).__init__()\n",
    "        \n",
    "        self.has_attention = has_attention\n",
    "        conv_blocks = [ConvBlock(in_channels, filters[0], num_layers, timestep_emb_dim=timestep_emb_dim)]\n",
    "        attention_blocks = [\n",
    "            Attention(\n",
    "                d_k=64, \n",
    "                dropout=0.1, \n",
    "                num_heads=num_heads, \n",
    "                num_channels=filters[0], \n",
    "            )\n",
    "        ] if has_attention else []\n",
    "        \n",
    "        for i in range(1, len(filters)):\n",
    "            conv_blocks.append(ConvBlock(filters[i-1], filters[i], num_layers, timestep_emb_dim=timestep_emb_dim))\n",
    "            if has_attention:\n",
    "                attention_blocks.append(\n",
    "                    Attention(\n",
    "                        d_k=64, \n",
    "                        dropout=0.1, \n",
    "                        num_heads=num_heads, \n",
    "                        num_channels=filters[i], \n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        self.conv_blocks = nn.ModuleList(conv_blocks)\n",
    "        self.attention_blocks = nn.ModuleList(attention_blocks)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x, timestep_emb=None):\n",
    "        residual_outputs = []\n",
    "        for i, conv_block in enumerate(self.conv_blocks):\n",
    "            x = conv_block(x, timestep_emb)\n",
    "            if self.has_attention:\n",
    "                x = self.attention_blocks[i](x)\n",
    "            \n",
    "            residual_outputs.append(x)\n",
    "            x = self.maxpool(x)\n",
    "\n",
    "        return residual_outputs, x\n",
    "\n",
    "\n",
    "class RightBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling (right) side of the UNet.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            filters: list[int], \n",
    "            num_layers: int, \n",
    "            has_attention: bool = False, \n",
    "            num_heads: int = 8, \n",
    "            dropout: float = 0.2,\n",
    "            timestep_emb_dim: int = None,\n",
    "            ):\n",
    "        super(RightBlock, self).__init__()\n",
    "        self.has_attention = has_attention\n",
    "\n",
    "        conv_layers = []\n",
    "        upsample_layers = []\n",
    "        attention_layers = []\n",
    "        \n",
    "        for i in range(len(filters) - 2):\n",
    "            conv_layers.append(\n",
    "                ConvBlock(filters[i], filters[i+1], num_layers, timestep_emb_dim=timestep_emb_dim)\n",
    "            )\n",
    "            upsample_layers.append(\n",
    "                nn.ConvTranspose2d(filters[i+1], filters[i+1]//2, 2, stride=2)\n",
    "            )\n",
    "\n",
    "            if has_attention:\n",
    "                attention_layers.append(\n",
    "                    Attention(d_k=64, dropout=0.1, num_heads=num_heads, num_channels=filters[i+1]//2)\n",
    "                )\n",
    "        conv_layers.append(\n",
    "            ConvBlock(filters[-2], filters[-1], num_layers, timestep_emb_dim=timestep_emb_dim)\n",
    "        )\n",
    "\n",
    "        self.conv_layers = nn.ModuleList(conv_layers)\n",
    "        self.attention_layers = nn.ModuleList(attention_layers)\n",
    "        self.upsample_layers = nn.ModuleList(upsample_layers)\n",
    "    \n",
    "    def forward(self, x, residual_outputs, timestep_emb=None):\n",
    "        for i in range(len(self.conv_layers)):\n",
    "            residual = residual_outputs[-(i+1)]\n",
    "            # Adjust residual dimensions to match x\n",
    "            _, _, h, w = x.shape\n",
    "            residual = residual[:, :, :h, :w]\n",
    "\n",
    "            x = torch.cat([x, residual], dim=1)\n",
    "            x = self.conv_layers[i](x, timestep_emb)\n",
    "\n",
    "            if i < len(self.upsample_layers):\n",
    "                x = self.upsample_layers[i](x)\n",
    "\n",
    "                if self.has_attention:\n",
    "                    x = self.attention_layers[i](x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_encoding(curr_t: torch.Tensor, T: torch.Tensor, embedding_dim: int, n=10000):\n",
    "    \"\"\"\n",
    "    Naive sin/cosin positional embedding adapted for timestep embedding in DDPM\n",
    "    \"\"\"\n",
    "    curr_t = curr_t / T # normalize the timestep to be between 0 and 1\n",
    "    p = torch.zeros((curr_t.shape[-1], embedding_dim)) # initialize the positional embedding tensor\n",
    "\n",
    "    m = torch.arange(int(embedding_dim/2)) # this is divided by two because we alternate between sin and cos\n",
    "    denominators = torch.float_power(n, 2*m/embedding_dim) # compute the denominators for the sin and cos functions\n",
    "    \n",
    "    p[:, 0::2] = torch.sin(curr_t.unsqueeze(1) / denominators.unsqueeze(0))\n",
    "    p[:, 1::2] = torch.cos(curr_t.unsqueeze(1) / denominators.unsqueeze(0))\n",
    "    return p\n",
    "\n",
    "\n",
    "\n",
    "class TimestepEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeds the timestep into a higher dimensional space using a 2 layer MLP.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                in_channels: int, \n",
    "                embedding_dim: int, \n",
    "                activation: nn.Module = nn.ReLU\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: number of input channels\n",
    "            embedding_dim: dimension of the embedding space\n",
    "            activation: activation function\n",
    "        \"\"\"\n",
    "        super(TimestepEmbedding, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_channels, embedding_dim)\n",
    "        self.linear2 = nn.Linear(embedding_dim, embedding_dim)\n",
    "        self.activation = activation()\n",
    "\n",
    "    def forward(self, curr_t: torch.Tensor, T: torch.Tensor):\n",
    "        print(curr_t.shape)\n",
    "        x = self.linear1(curr_t)\n",
    "        print(x.shape)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        print(x.shape)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetAttn(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet model for DDPM.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 down_filters, \n",
    "                 in_channels, \n",
    "                 num_layers, \n",
    "                 has_attention: bool = False, \n",
    "                 num_heads: int = 8,\n",
    "                 diffusion_steps: int = None,\n",
    "                 num_groups: int = 8,\n",
    "                 activation: nn.Module = nn.ReLU,\n",
    "                 timestep_emb_dim: int = None\n",
    "                ):\n",
    "        super(UNetAttn, self).__init__()\n",
    "        self.T = diffusion_steps\n",
    "        self.down_filters = down_filters\n",
    "\n",
    "        self.time_embed_dim = down_filters[0] * 4 \n",
    "        \n",
    "        if self.T is not None:\n",
    "            self.timestep_embedding = TimestepEmbedding(\n",
    "                in_channels=in_channels, \n",
    "                embedding_dim=in_channels, \n",
    "                activation=activation, \n",
    "                post_activation=True\n",
    "            )\n",
    "        \n",
    "        \n",
    "        self.left_block = LeftBlock(\n",
    "            filters=down_filters, \n",
    "            num_layers=num_layers, \n",
    "            in_channels=in_channels, \n",
    "            has_attention=has_attention, \n",
    "            num_heads=num_heads\n",
    "        )\n",
    "        \n",
    "        # the bottom-most (middle) conv block \n",
    "        if has_attention:\n",
    "            self.bottom_conv = nn.Sequential(\n",
    "                ConvBlock(down_filters[-1], down_filters[-1]*2, num_layers, timestep_emb_dim=timestep_emb_dim), \n",
    "                Attention(d_k=64, dropout=0.1, num_heads=num_heads, num_channels=down_filters[-1]*2),\n",
    "                nn.ConvTranspose2d(down_filters[-1]*2, down_filters[-1], 2, stride=2)\n",
    "            )\n",
    "        else:\n",
    "            self.bottom_conv = nn.Sequential(\n",
    "                ConvBlock(\n",
    "                    down_filters[-1], \n",
    "                    down_filters[-1]*2, \n",
    "                    num_layers=num_layers, \n",
    "                    timestep_emb_dim=timestep_emb_dim\n",
    "                ), \n",
    "                nn.ConvTranspose2d(down_filters[-1]*2, down_filters[-1], 2, stride=2)\n",
    "            )\n",
    "        \n",
    "        self.up_filters = [down_filters[-1]*2]\n",
    "        self.up_filters.extend(reversed(down_filters))\n",
    "        self.right_block = RightBlock(\n",
    "            filters=self.up_filters, \n",
    "            num_layers=num_layers, \n",
    "            has_attention=has_attention, \n",
    "            num_heads=num_heads,\n",
    "            timestep_emb_dim=timestep_emb_dim\n",
    "        )\n",
    "\n",
    "        self.group_norm = nn.GroupNorm(num_groups=num_groups, num_channels=in_channels)\n",
    "        self.conv_out = nn.Conv2d(down_filters[-1], in_channels, 3, padding=1)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        if self.T is not None:\n",
    "            t_emb = self.timestep_embedding(curr_t=t, T=self.T)\n",
    "            t_emb = t_emb.view(-1, self.time_embed_dim, 1, 1)\n",
    "\n",
    "        x = self.group_norm(x)\n",
    "        residual_outputs, down_output = self.left_block(x)\n",
    "        bottom_output = self.bottom_conv(down_output)\n",
    "        return self.right_block(bottom_output, residual_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeftBlock(\n",
       "  (conv_blocks): ModuleList(\n",
       "    (0): ConvBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 3, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Conv2d(3, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (timestep_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Identity()\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (timestep_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ConvBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (timestep_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Identity()\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (timestep_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ConvBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (timestep_emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Identity()\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (timestep_emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (attention_blocks): ModuleList(\n",
       "    (0): Attention(\n",
       "      (query_projection): Linear(in_features=32, out_features=192, bias=True)\n",
       "      (key_projection): Linear(in_features=32, out_features=192, bias=True)\n",
       "      (value_projection): Linear(in_features=32, out_features=192, bias=True)\n",
       "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (output_layer): Linear(in_features=192, out_features=32, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): Attention(\n",
       "      (query_projection): Linear(in_features=64, out_features=192, bias=True)\n",
       "      (key_projection): Linear(in_features=64, out_features=192, bias=True)\n",
       "      (value_projection): Linear(in_features=64, out_features=192, bias=True)\n",
       "      (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (output_layer): Linear(in_features=192, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): Attention(\n",
       "      (query_projection): Linear(in_features=128, out_features=192, bias=True)\n",
       "      (key_projection): Linear(in_features=128, out_features=192, bias=True)\n",
       "      (value_projection): Linear(in_features=128, out_features=192, bias=True)\n",
       "      (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (output_layer): Linear(in_features=192, out_features=128, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = LeftBlock(in_channels=3, filters=[32, 64, 128], num_layers=2, has_attention=True, num_heads=3, timestep_emb_dim=128)\n",
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "timesteps = torch.randint(0, 1000, (2,))\n",
    "t_encoded = timestep_encoding(timesteps, 1000, 32)\n",
    "t_encoded.shape # N x 32\n",
    "\n",
    "time_embedding_layer = TimestepEmbedding(in_channels=32, embedding_dim=128, activation=nn.ReLU) # N x 32 -> N x 128 (embedding_dim)\n",
    "t_embedded = time_embedding_layer(curr_t=t_encoded, T=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 16, 16])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = db(torch.randn(2, 3, 128, 128), timestep_emb=t_embedded)\n",
    "a[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 128, 128])\n",
      "----------\n",
      "torch.Size([2, 64, 64, 64])\n",
      "----------\n",
      "torch.Size([2, 128, 32, 32])\n",
      "----------\n",
      "torch.Size([2, 128, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "for residual_output in a[0]:\n",
    "    print(residual_output.shape)\n",
    "    print(\"-\"*10)\n",
    "\n",
    "print(a[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 32, 32])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_conv = nn.Sequential(ResBlock(128, 256), nn.ConvTranspose2d(256, 128, 2, stride=2))\n",
    "bottom_conv(a[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RightBlock(\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): ConvBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 256, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (timestep_emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Identity()\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (timestep_emb_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): ConvBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 128, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (timestep_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Identity()\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (timestep_emb_proj): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): ConvBlock(\n",
       "      (convs): ModuleList(\n",
       "        (0): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (timestep_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "        (1): ResBlock(\n",
       "          (norm1): Sequential(\n",
       "            (0): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Sequential(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): GroupNorm(1, 32, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (activation): ReLU()\n",
       "          (idconv): Identity()\n",
       "          (avgpool): Identity()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (timestep_emb_proj): Linear(in_features=128, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (attention_layers): ModuleList(\n",
       "    (0): Attention(\n",
       "      (query_projection): Linear(in_features=64, out_features=192, bias=True)\n",
       "      (key_projection): Linear(in_features=64, out_features=192, bias=True)\n",
       "      (value_projection): Linear(in_features=64, out_features=192, bias=True)\n",
       "      (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (output_layer): Linear(in_features=192, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): Attention(\n",
       "      (query_projection): Linear(in_features=32, out_features=192, bias=True)\n",
       "      (key_projection): Linear(in_features=32, out_features=192, bias=True)\n",
       "      (value_projection): Linear(in_features=32, out_features=192, bias=True)\n",
       "      (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "      (output_layer): Linear(in_features=192, out_features=32, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (upsample_layers): ModuleList(\n",
       "    (0): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "up = RightBlock(filters=[256, 128, 64, 32], num_layers=2, has_attention=True, num_heads=3, timestep_emb_dim=128)\n",
    "up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 32, 32])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_out = bottom_conv(a[1])\n",
    "bottom_out.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 128, 128])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "up_out = up(bottom_out, a[0], t_embedded)\n",
    "up_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above dimensionality makes no sense. TODO: Check the implementation of the right block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_image = torch.randn(2, 3, 128, 128)\n",
    "conv_block = ConvBlock(3, 32, 2)\n",
    "h_example_image = conv_block(example_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 128, 128])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_block = Attention(d_k=64, dropout=0.1, num_heads=3, num_channels=32, num_groups=8)\n",
    "attn_output = attention_block(h_example_image)\n",
    "attn_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(15, 3, 128, 128)\n",
    "T = 1000\n",
    "batch_size = x.shape[0]\n",
    "t = torch.randint(0, T, (batch_size,))\n",
    "e = torch.randn_like(x)\n",
    "alpha_bar_t = alpha_bar(t)\n",
    "print(alpha_bar_t.shape)\n",
    "alpha_bar_t = alpha_bar_t.view(-1, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 1, 1, 1])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_bar_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, valid_loader, loss_fn, all_valid_loss):\n",
    "    model.eval()\n",
    "    valid_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            x, y = batch\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            valid_loss.append(loss.item())\n",
    "    \n",
    "    all_valid_loss.append(sum(valid_loss) / len(valid_loss))\n",
    "\n",
    "def plot_loss(all_train_loss, all_valid_loss):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(all_train_loss, label='Training Loss')\n",
    "    plt.plot(all_valid_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def train_model(model: nn.Module,\n",
    "                optim: torch.optim.Optimizer,\n",
    "                loss_fn,\n",
    "                train_loader,\n",
    "                valid_loader,\n",
    "                scheduler,\n",
    "                epochs=10,\n",
    "                valid_every=1\n",
    "                ):\n",
    "\n",
    "    all_train_loss = []\n",
    "    all_valid_loss = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            x, _ = batch\n",
    "            (noisy_images, t), e = prepare_batch(x, T, alpha_bar_t)\n",
    "            optim.zero_grad()\n",
    "            y_pred = model(noisy_images)\n",
    "            loss = loss_fn(y_pred, e)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        all_train_loss.append(sum(train_loss) / len(train_loss))\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % valid_every == 0:\n",
    "            validate_model(model, valid_loader, loss_fn, all_valid_loss)\n",
    "            print(\n",
    "                f\"Epoch {epoch}, Train Loss: {sum(train_loss) / len(train_loss)}, \"\n",
    "                f\"Valid Loss: {all_valid_loss[-1]}\"\n",
    "            )\n",
    "    \n",
    "    plot_loss(all_train_loss, all_valid_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
