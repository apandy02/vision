{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04be2ce9",
   "metadata": {},
   "source": [
    "# Video Classification using Space-Time Attention (TimeSFormer)\n",
    "\n",
    "Reference: \"Is Space-Time Attention All You Need for Video Understanding?\" (TimeSFormer), Bertasius et al., NeurIPS 2021. [https://arxiv.org/abs/2102.05095]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accdba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import UCF101\n",
    "\n",
    "from vision.transformers.blocks import MLP\n",
    "from vision.transformers.attention import Attention\n",
    "\n",
    "import lightning as L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1005d17",
   "metadata": {},
   "source": [
    "## Space-Time Encoder \n",
    "\n",
    "The space time encoder is designed to follow the divided space-time attention module defined in the TimeSFormer paper.\n",
    "\n",
    "![Space-Time Attention](images/st_attention.png)\n",
    "\n",
    "*Figure: Divided space-time attention mechanism as described in the TimeSFormer paper*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a9b616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder layer block for ViT\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_heads: int,\n",
    "        num_channels: int,\n",
    "        d_linear: int,\n",
    "        num_linear_layers: int = 2,\n",
    "        num_groups: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        is_masked: bool = False\n",
    "    ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.norm1, self.norm2, self.norm3 = (\n",
    "            nn.LayerNorm(num_channels),  nn.LayerNorm(num_channels), nn.LayerNorm(num_channels)\n",
    "        )\n",
    "        self.mha_space = Attention(dropout, num_heads, num_channels, num_groups)\n",
    "        self.mha_time = Attention(dropout, num_heads, num_channels, num_groups)\n",
    "        self.mlp = MLP(num_channels, d_linear, dropout, num_linear_layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch, time, patches, channels = x.shape\n",
    "        h = x.permute(0, 2, 1, 3).reshape(batch*patches, time, channels)  # convert to [B, patches, time, channels] to compute attention across time dimension\n",
    "        h = self.mha_time(self.norm1(h))  # \n",
    "        h = h.reshape(batch, patches, time, channels).permute(0, 2, 1, 3) + x\n",
    "        h2 = h.reshape(batch*time, patches, channels)\n",
    "        h2 = self.mha_space(self.norm2(h2))\n",
    "        h2  = h2.reshape(batch, time, patches, channels) + h\n",
    "        return self.mlp(self.norm3(h2)) + h2\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_heads: int,\n",
    "        num_channels: int,\n",
    "        num_layers: int,\n",
    "        d_linear: int,\n",
    "        num_linear_layers: int = 2,\n",
    "        num_groups: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        is_masked: bool = False\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                num_heads, num_channels, d_linear, num_linear_layers, num_groups, dropout, is_masked\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "class MLPClassicationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP based classification head\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        num_channels:int,\n",
    "        d_ff: int,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super(MLPClassicationHead, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(num_channels, d_ff, bias=True))\n",
    "        for i in range(1, num_layers - 1):\n",
    "            layers.append(nn.Linear(d_ff, d_ff, bias=True))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        layers.append(nn.Linear(d_ff, num_classes, bias=True))\n",
    "        self.mlp_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mlp_layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a7f99f",
   "metadata": {},
   "source": [
    "## TimeSFormer Model \n",
    "\n",
    "Much like a ViT, the TimeSFormer model patchifies input frames and adds positional embeddings to the input before passing it to the Encoder. In the space-time case, we want to encode positional information about patches through both space and time, so we learn two separate embeddings. Like any other transformer based classifier, we append a cls_token, and run the encoder outputs through a classification MLP at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c2a7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSFormer(nn.Module):\n",
    "    \"\"\"\n",
    "    Cloned from Vision Transformer -- need to adapt to TimeSFormer architecture\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        num_heads: int,\n",
    "        d_model: int,\n",
    "        d_mlp: int,\n",
    "        patch_size: int = 16,\n",
    "        frames: int = 8,\n",
    "        image_size: tuple[int] = 224,\n",
    "        num_encoder_layers: int = 2,\n",
    "        encoder_mlp_depth: int = 2,\n",
    "        classification_mlp_depth: int = 2,\n",
    "        num_groups: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super(TimeSFormer, self).__init__()\n",
    "        self.d_model, self.patch_size, self.image_size = d_model, patch_size, image_size\n",
    "        self.num_frames = frames\n",
    "        self.n_patches = (image_size // patch_size) ** 2\n",
    "        self.linear = nn.Linear((3*patch_size*patch_size), d_model)\n",
    "        self.encoder = Encoder(\n",
    "            num_heads, d_model, num_encoder_layers, d_mlp, encoder_mlp_depth, num_groups, dropout\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, 1, d_model))\n",
    "        self.classification_head = MLPClassicationHead(num_classes, d_model, d_mlp, classification_mlp_depth, dropout) # d_ff and depth are different things\n",
    "\n",
    "        self.pos_embed_space = nn.Parameter(torch.zeros(1, 1, self.n_patches + 1, d_model))\n",
    "        self.pos_embed_time = nn.Parameter(torch.zeros(1, self.num_frames, 1, d_model))\n",
    "        \n",
    "        nn.init.trunc_normal_(self.pos_embed_space, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed_time, std=0.02)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        forward pass for our vit classifier. takes in raw images and outputs a probability distribution over classes\n",
    "        \"\"\"\n",
    "        batch_size, frames = x.shape[0], x.shape[2]\n",
    "        x = self.linear(self._patchify(x))\n",
    "        cls = self.cls_token.expand(batch_size, frames, -1, -1)\n",
    "        x = torch.cat((cls, x), 2)\n",
    "        x = x + self.pos_embed_space + self.pos_embed_time\n",
    "        x = self.encoder(x)\n",
    "        cls_token = x[:, :, 0, :]\n",
    "        return self.classification_head(cls_token.mean(dim=1))\n",
    "    \n",
    "    def _patchify(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Splits a batch of videos into non-overlapping patches.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [B, C, F, H, W]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Patchified tensor of shape [B, F, NUM_P, DIM_P],\n",
    "                          where DIM_P = channels * patch_size * patch_size.\n",
    "        \"\"\"\n",
    "        batch_size, channels, frames, _, _ = x.shape\n",
    "        n_patch_side = self.image_size // self.patch_size\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        x = x.reshape(\n",
    "            batch_size,\n",
    "            frames,\n",
    "            channels,\n",
    "            n_patch_side,\n",
    "            self.patch_size,\n",
    "            n_patch_side,\n",
    "            self.patch_size,\n",
    "        )\n",
    "        x = x.permute(0, 1, 3, 5, 2, 4, 6)\n",
    "        return x.reshape(batch_size, frames, -1, channels * self.patch_size * self.patch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dfcb89",
   "metadata": {},
   "source": [
    "## Lightning module\n",
    "\n",
    "We will use pytorch lightning to reduce boiler plate (there's a lot in previous notebooks, despite the centralized modules in `/src`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "117496fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningTimeSformer(L.LightningModule):\n",
    "    def __init__(self, num_classes, num_heads, d_model, d_mlp):\n",
    "        super().__init__()\n",
    "        self.model = TimeSFormer(\n",
    "            num_classes=num_classes,\n",
    "            num_heads=num_heads,\n",
    "            d_model=d_model,\n",
    "            d_mlp=d_mlp\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, target = batch\n",
    "        output = self(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(output, target)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0423556d",
   "metadata": {},
   "source": [
    "## Load UCF101 Action Recognition dataset \n",
    "\n",
    "We use the UCF101 dataset which contains 13,320 videos from 101 action categories. This dataset is commonly used for benchmarking video action recognition models, such as basketball shooting, biking, diving, golf swinging, horse riding, and playing musical instruments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81cb832f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7c884c1cad40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aryamanpandya/vision/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/aryamanpandya/vision/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1460, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "  File \"/home/aryamanpandya/.local/share/uv/python/cpython-3.10.18-linux-x86_64-gnu/lib/python3.10/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "AssertionError: can only test a child process\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 833/833 [05:56<00:00,  2.33it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 833/833 [05:56<00:00,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered train dataset size: 219119\n",
      "Filtered test dataset size: 85526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def filter_dataset_by_classes(dataset, selected_classes):\n",
    "    \"\"\"\n",
    "    Filter dataset to only include selected classes\n",
    "    \"\"\"\n",
    "    class_to_idx_map = {cls: idx for idx, cls in enumerate(dataset.classes)}\n",
    "    selected_indices = [class_to_idx_map[cls] for cls in selected_classes if cls in class_to_idx_map]\n",
    "    \n",
    "    filtered_samples = [\n",
    "        (video_path, selected_indices.index(class_idx))\n",
    "        for video_path, class_idx in dataset.samples\n",
    "        if class_idx in selected_indices\n",
    "    ]\n",
    "\n",
    "    dataset.samples = filtered_samples\n",
    "    dataset.classes = selected_classes\n",
    "    return dataset\n",
    "\n",
    "train_dataset = UCF101(\n",
    "    root='./data/UCF-101',\n",
    "    annotation_path='./data/ucfTrainTestlist',\n",
    "    frames_per_clip=8,\n",
    "    step_between_clips=8,  # non overlapping clips\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "test_dataset = UCF101(\n",
    "    root='./data/UCF-101',\n",
    "    annotation_path='./data/ucfTrainTestlist',\n",
    "    frames_per_clip=8,\n",
    "    step_between_clips=8,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "# Select only a few classes for faster experimentation\n",
    "\"\"\"\n",
    "selected_classes = [\n",
    "    'ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam',\n",
    "    'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress'\n",
    "]\n",
    "\n",
    "train_dataset = filter_dataset_by_classes(train_dataset, selected_classes)\n",
    "test_dataset = filter_dataset_by_classes(test_dataset, selected_classes)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Filtered train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Filtered test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfd2cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def collate_ucf101(batch):\n",
    "    # batch: list of (video, label, index) where label is detection labels \n",
    "    # and index is the index of the class for recognition\n",
    "    xs, ys = [], []\n",
    "    for v, _, l in batch:\n",
    "        # v: T, H, W, C  (uint8)\n",
    "        v = v.permute(0, 3, 1, 2)            # -> T, C, H, W\n",
    "        v = v.float() / 255.0\n",
    "        v = F.interpolate(v, size=(224, 224), mode='bilinear', align_corners=False)  # resize frames\n",
    "        v = v.permute(1, 0, 2, 3).contiguous()  # -> C, F, H, W\n",
    "        xs.append(v.clone())                  # new storage\n",
    "        ys.append(int(l))\n",
    "    return torch.stack(xs, 0), torch.tensor(ys, dtype=torch.long)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    num_workers=8,\n",
    "    collate_fn=collate_ucf101,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_ucf101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e8871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryamanpandya/vision/.venv/lib/python3.10/site-packages/lightning/fabric/connector.py:571: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/aryamanpandya/vision/.venv/lib/python3.10/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type        | Params | Mode \n",
      "----------------------------------------------\n",
      "0 | model | TimeSFormer | 18.7 M | train\n",
      "----------------------------------------------\n",
      "18.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "18.7 M    Total params\n",
      "74.722    Total estimated model params size (MB)\n",
      "60        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/54780 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryamanpandya/vision/.venv/lib/python3.10/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/home/aryamanpandya/vision/.venv/lib/python3.10/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/home/aryamanpandya/vision/.venv/lib/python3.10/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/home/aryamanpandya/vision/.venv/lib/python3.10/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/home/aryamanpandya/vision/.venv/lib/python3.10/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/home/aryamanpandya/vision/.venv/lib/python3.10/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/home/aryamanpandya/vision/.venv/lib/python3.10/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n",
      "/home/aryamanpandya/vision/.venv/lib/python3.10/site-packages/torchvision/io/video.py:161: UserWarning: The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\n",
      "  warnings.warn(\"The pts_unit 'pts' gives wrong results. Please use pts_unit 'sec'.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/429831 [23:03<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/429831 [23:03<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/429831 [23:03<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/429831 [23:03<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/429831 [23:03<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/429831 [23:03<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/429831 [23:03<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/429831 [23:03<?, ?it/s]\n",
      "Epoch 0:   3%|â–Ž         | 1463/54780 [00:51<31:09, 28.53it/s, v_num=6] "
     ]
    }
   ],
   "source": [
    "lightning_timesformer = LightningTimeSformer(num_classes=101, num_heads=4, d_model=512, d_mlp=512)\n",
    "trainer = L.Trainer(max_epochs=1, precision=16)\n",
    "trainer.fit(model=lightning_timesformer, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e6fa85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
