{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04be2ce9",
   "metadata": {},
   "source": [
    "# Video Classification using Space-Time Attention (TimeSFormer)\n",
    "\n",
    "Reference: \"Is Space-Time Attention All You Need for Video Understanding?\" (TimeSFormer), Bertasius et al., NeurIPS 2021. [https://arxiv.org/abs/2102.05095]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "accdba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "import torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9b616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.transformers.blocks import MLP\n",
    "from vision.transformers.attention import Attention\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder layer block for ViT\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_heads: int,\n",
    "        num_channels: int,\n",
    "        d_linear: int,\n",
    "        num_linear_layers: int = 2,\n",
    "        num_groups: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        is_masked: bool = False\n",
    "    ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.norm1, self.norm2, self.norm3 = (\n",
    "            nn.LayerNorm(num_channels),  nn.LayerNorm(num_channels), nn.LayerNorm(num_channels)\n",
    "        )\n",
    "        self.mha_space = Attention(dropout, num_heads, num_channels, num_groups)\n",
    "        self.mha_time = Attention(dropout, num_heads, num_channels, num_groups)\n",
    "        self.mlp = MLP(num_channels, d_linear, dropout, num_linear_layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        #  assumes x: b, time, num_patches + 1, d_model\n",
    "        #  at each frame, we have num_patches + 1 patches \n",
    "        #  that have d_model dimensionality\n",
    "        batch, time, patches, channels = x.shape\n",
    "        h = x.permute(0, 2, 1, 3).reshape(batch*patches, time, channels)\n",
    "        h = self.mha_time(self.norm1(h))\n",
    "        h = h.reshape(batch, patches, time, channels).permute(0, 2, 1, 3) + x\n",
    "        h2 = h.reshape(batch*time, patches, channels)\n",
    "        h2 = self.mha_space(self.norm2(h2))\n",
    "        h2  = h2.reshape(batch, time, patches, channels) + h\n",
    "        return self.mlp(self.norm3(h2)) + h2\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_heads: int,\n",
    "        num_channels: int,\n",
    "        num_layers: int,\n",
    "        d_linear: int,\n",
    "        num_linear_layers: int = 2,\n",
    "        num_groups: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        is_masked: bool = False\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                num_heads, num_channels, d_linear, num_linear_layers, num_groups, dropout, is_masked\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a7b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassicationHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Class implementation of a position wise MLP\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        num_channels:int,\n",
    "        d_ff: int,\n",
    "        num_layers: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super(MLPClassicationHead, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(num_channels, d_ff, bias=True))\n",
    "        for i in range(1, num_layers - 1):\n",
    "            layers.append(nn.Linear(d_ff, d_ff, bias=True))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        layers.append(nn.Linear(d_ff, num_classes, bias=True))\n",
    "        self.mlp_layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.mlp_layers(x)\n",
    "\n",
    "class TimeSFormer(nn.Module):\n",
    "    \"\"\"\n",
    "    Cloned from Vision Transformer -- need to adapt to TimeSFormer architecture\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes,\n",
    "        num_heads: int,\n",
    "        d_model: int,\n",
    "        d_mlp: int,\n",
    "        patch_size: int = 16,\n",
    "        frames: int = 8,\n",
    "        image_size: tuple[int] = 32,\n",
    "        num_encoder_layers: int = 2,\n",
    "        encoder_mlp_depth: int = 2,\n",
    "        classification_mlp_depth: int = 2,\n",
    "        num_groups: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super(TimeSFormer, self).__init__()\n",
    "        self.d_model, self.patch_size, self.image_size = d_model, patch_size, image_size\n",
    "        self.num_frames = frames\n",
    "        self.n_patches = (image_size // patch_size) ** 2  # assumes square image \n",
    "        self.linear = nn.Linear((3*patch_size*patch_size), d_model)  # assumes rgb image \n",
    "        self.encoder = Encoder(\n",
    "            num_heads, d_model, num_encoder_layers, d_mlp, encoder_mlp_depth, num_groups, dropout\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, 1, d_model))  # assumes rgb image \n",
    "        self.classification_head = MLPClassicationHead(num_classes, d_model, d_mlp, classification_mlp_depth, dropout) # d_ff and depth are different things\n",
    "\n",
    "        self.pos_embed_space = nn.Parameter(torch.zeros(1, 1, self.n_patches + 1, d_model))\n",
    "        self.pos_embed_time = nn.Parameter(torch.zeros(1, self.num_frames, 1, d_model))\n",
    "        nn.init.trunc_normal_(self.pos_embed_space, std=0.02)\n",
    "        nn.init.trunc_normal_(self.pos_embed_time, std=0.02)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        forward pass for our vit classifier. takes in raw images and outputs a probability distribution over classes\n",
    "        \"\"\"\n",
    "        batch_size, frames = x.shape[0], x.shape[2]\n",
    "        x = self.linear(self._patchify(x))\n",
    "        print(self.cls_token.shape)\n",
    "        cls = self.cls_token.expand(batch_size, frames, -1, -1)\n",
    "        x = torch.cat((cls, x), 2)\n",
    "        print(f\"pos_embed_space: {self.pos_embed_space.shape}\")\n",
    "        print(f\"x: {x.shape}\")\n",
    "        x = x + self.pos_embed_space[:, : x.size(1), :].to(x.device)\n",
    "        x = self.encoder(x)\n",
    "        return self.classification_head(x[:,0,:])\n",
    "    \n",
    "    def _patchify(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Splits a batch of videos into non-overlapping patches.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [B, C, F, H, W]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Patchified tensor of shape [B, F, NUM_P, DIM_P],\n",
    "                          where DIM_P = channels * patch_size * patch_size.\n",
    "        \"\"\"\n",
    "        batch_size, channels, frames, _, _ = x.shape\n",
    "        n_patch_side = self.image_size // self.patch_size\n",
    "        x = x.permute(0, 2, 1, 3, 4)\n",
    "        x = x.reshape(\n",
    "            batch_size,\n",
    "            frames,\n",
    "            channels,\n",
    "            n_patch_side,\n",
    "            self.patch_size,\n",
    "            n_patch_side,\n",
    "            self.patch_size,\n",
    "        )\n",
    "        x = x.permute(0, 1, 3, 5, 2, 4, 6)\n",
    "        return x.reshape(batch_size, frames, -1, channels * self.patch_size * self.patch_size)\n",
    "        \n",
    "    # Removed _positional_embedding, as we now use a learnable positional embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05f3d8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 3, 8, 224, 224])\n",
      "Model parameters: 1,349,514\n",
      "torch.Size([1, 1, 1, 128])\n",
      "pos_embed_space: torch.Size([1, 197, 128])\n",
      "x: torch.Size([2, 8, 197, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (197) must match the size of tensor b (8) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Run forward pass\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 29\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mtimesformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdummy_videos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput probabilities sum: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39msoftmax(output,\u001b[38;5;250m \u001b[39mdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/vision/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vision/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[18], line 70\u001b[0m, in \u001b[0;36mTimeSFormer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos_embed_space: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed_space\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 70\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_embed_space\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassification_head(x[:,\u001b[38;5;241m0\u001b[39m,:])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (197) must match the size of tensor b (8) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "channels = 3\n",
    "frames = 8\n",
    "height = 224\n",
    "width = 224\n",
    "\n",
    "dummy_videos = torch.randn(batch_size, channels, frames, height, width)\n",
    "print(f\"Input shape: {dummy_videos.shape}\")\n",
    "\n",
    "# Initialize TimeSFormer with correct parameters based on the class definition\n",
    "timesformer = TimeSFormer(\n",
    "    num_classes=10,\n",
    "    num_heads=4,\n",
    "    d_model=128,\n",
    "    d_mlp=256,\n",
    "    patch_size=16,\n",
    "    image_size=224,\n",
    "    num_encoder_layers=2,\n",
    "    encoder_mlp_depth=2,\n",
    "    classification_mlp_depth=2,\n",
    "    num_groups=8,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in timesformer.parameters()):,}\")\n",
    "\n",
    "# Run forward pass\n",
    "with torch.no_grad():\n",
    "    output = timesformer(dummy_videos)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Output probabilities sum: {torch.softmax(output, dim=-1).sum(dim=-1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe911db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
