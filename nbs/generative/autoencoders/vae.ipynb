{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ConvolutionalVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        n_channels: int,\n",
    "        conv_dim: int,\n",
    "        latent_dim: int,\n",
    "        batch_size: int,s\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.n_channels = n_channels\n",
    "        self.conv_dim = conv_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, conv_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(conv_dim),\n",
    "            nn.Conv2d(conv_dim, 2 * conv_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(2 * conv_dim),\n",
    "            nn.Conv2d(2 * conv_dim, 4 * conv_dim, kernel_size=3, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(4 * conv_dim),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(\n",
    "                conv_dim * 4 * self.input_dim[1] // 8 * self.input_dim[2] // 8, 2 * latent_dim\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 4 * conv_dim * self.input_dim[1] // 8 * self.input_dim[2] // 8),\n",
    "            nn.Unflatten(1, (4 * conv_dim, self.input_dim[1] // 8, self.input_dim[2] // 8)),  # Reshape to (batch, channels, height, width)\n",
    "            nn.ConvTranspose2d(4 * conv_dim, 2 * conv_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(2 * conv_dim),\n",
    "            nn.ConvTranspose2d(2 * conv_dim, conv_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(conv_dim),\n",
    "            nn.ConvTranspose2d(conv_dim, n_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def sample(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        encoded = self.encoder(x)\n",
    "        mu, logvar = encoded.split(self.latent_dim, dim=1)\n",
    "        z = self.sample(mu, logvar)\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_hat shape torch.Size([4, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Checking that dimenionality is correct\n",
    "vae = ConvolutionalVAE(\n",
    "    input_dim=(3, 32, 32),\n",
    "    n_channels=3,\n",
    "    conv_dim=96,\n",
    "    latent_dim=128,\n",
    "    batch_size=4,\n",
    ")\n",
    "\n",
    "random_data = torch.randn(4, 3, 32, 32)\n",
    "x_hat = vae(random_data)\n",
    "\n",
    "print(\"x_hat shape\", x_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
