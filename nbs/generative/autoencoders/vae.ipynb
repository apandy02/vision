{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Architecturally, the difference between a VAE and an AE is that while the encoder in the AE directly outputs the encoded image, in a VAE it outputs mean and stdv values from which we can sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ConvolutionalVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        n_channels: int,\n",
    "        conv_dim: int,\n",
    "        latent_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, conv_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(conv_dim),\n",
    "            nn.Conv2d(conv_dim, 2 * conv_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(2 * conv_dim),\n",
    "            nn.Conv2d(2 * conv_dim, 4 * conv_dim, kernel_size=3, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(4 * conv_dim),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(conv_dim * 4 * input_dim[1] // 8 * input_dim[2] // 8, 2 * latent_dim),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 4 * conv_dim * input_dim[1] // 8 * input_dim[2] // 8),\n",
    "            nn.Unflatten(1, (4 * conv_dim, input_dim[1] // 8, input_dim[2] // 8)),\n",
    "            nn.ConvTranspose2d(4 * conv_dim, 2 * conv_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(2 * conv_dim),\n",
    "            nn.ConvTranspose2d(2 * conv_dim, conv_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(conv_dim),\n",
    "            nn.ConvTranspose2d(conv_dim, n_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def sample(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample from the latent space using the reparameterization trick.\n",
    "\n",
    "        Args:\n",
    "            mu: mean of the latent space\n",
    "            logvar: log variance of the latent space\n",
    "\n",
    "        Returns:\n",
    "            z: sampled latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the VAE.\n",
    "\n",
    "        Args:\n",
    "            x: input data\n",
    "\n",
    "        Returns:\n",
    "            x_hat: reconstructed data\n",
    "        \"\"\"\n",
    "        encoded = self.encoder(x)\n",
    "        mu, logvar = encoded.split(self.latent_dim, dim=1)\n",
    "        z = self.sample(mu, logvar)\n",
    "        return self.decoder(z), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_hat shape torch.Size([4, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "# Checking that dimenionality is correct\n",
    "vae = ConvolutionalVAE(\n",
    "    input_dim=(3, 32, 32),\n",
    "    n_channels=3,\n",
    "    conv_dim=96,\n",
    "    latent_dim=128,\n",
    ")\n",
    "\n",
    "random_data = torch.randn(4, 3, 32, 32)\n",
    "x_hat = vae(random_data)\n",
    "\n",
    "print(\"x_hat shape\", x_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "\n",
    "So we've adjusted the architecture and added in the sampling/reparameterization trick to allow the flow of gradients. What's left?\n",
    "\n",
    "the other difference between the VAE and the AE is the loss function. our loss term consists of two parts now, reconstruction and KL\n",
    "divergence of the latent distribution from a standard normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, reconstruction_loss_func, kl_loss_func, valid_dl):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tot_loss = 0.\n",
    "        count = 0\n",
    "        for xb, _ in valid_dl:\n",
    "            pred = model(xb)\n",
    "            recon_loss = reconstruction_loss_func(pred, xb)\n",
    "            kl_loss = kl_loss_func(pred)\n",
    "            loss = recon_loss + kl_loss\n",
    "            tot_loss += loss.item()\n",
    "            count += len(xb)\n",
    "\n",
    "    return tot_loss / count, recon_loss / count, kl_loss / count\n",
    "\n",
    "def fit(\n",
    "    epochs, \n",
    "    model,\n",
    "    reconstruction_loss_func, \n",
    "    kl_loss_func, \n",
    "    opt, \n",
    "    train_dl, \n",
    "    valid_dl, \n",
    "):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            x_hat, z = model(xb)\n",
    "            recon_loss = reconstruction_loss_func(x_hat, xb)\n",
    "            kl_loss = kl_loss_func(z, torch.randn_like(z))\n",
    "            loss = recon_loss + kl_loss\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        tot_loss_count = validate(model, reconstruction_loss_func, valid_dl)\n",
    "        print(f\"Validation loss: {tot_loss_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data boiler plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "all_batches_data = []\n",
    "all_batches_labels = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    with open(f'data/cifar-10-batches-py/data_batch_{i}', 'rb') as f:\n",
    "        dataset_dict = pickle.load(f, encoding='bytes')\n",
    "        all_batches_data.append(dataset_dict[b'data'])\n",
    "        all_batches_labels.append(dataset_dict[b'labels'])\n",
    "\n",
    "stacked_data = np.vstack(all_batches_data)\n",
    "stacked_labels = np.hstack(all_batches_labels)\n",
    "data = torch.tensor(stacked_data, dtype=torch.float32).view(-1, 3, 32, 32).to(device) / 255.\n",
    "labels = torch.tensor(stacked_labels, dtype=torch.long).to(device)\n",
    "\n",
    "split_idx = int(0.8 * len(data))\n",
    "\n",
    "x_train, x_valid = data[:split_idx], data[split_idx:]\n",
    "y_train, y_valid = labels[:split_idx], labels[split_idx:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CIFARCustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "train_ds = CIFARCustomDataset(x_train, y_train)\n",
    "valid_ds = CIFARCustomDataset(x_valid, y_valid)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "KLDivLoss.forward() missing 1 required positional argument: 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m)\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreconstruction_loss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkl_loss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dl\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[63], line 31\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(epochs, model, reconstruction_loss_func, kl_loss_func, opt, train_dl, valid_dl)\u001b[0m\n\u001b[1;32m     29\u001b[0m pred \u001b[38;5;241m=\u001b[39m model(xb)\n\u001b[1;32m     30\u001b[0m recon_loss \u001b[38;5;241m=\u001b[39m reconstruction_loss_func(pred, xb)\n\u001b[0;32m---> 31\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m \u001b[43mkl_loss_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m recon_loss \u001b[38;5;241m+\u001b[39m kl_loss\n\u001b[1;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/vision/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/vision/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: KLDivLoss.forward() missing 1 required positional argument: 'target'"
     ]
    }
   ],
   "source": [
    "model = ConvolutionalVAE(\n",
    "    input_dim=(3, 32, 32),\n",
    "    n_channels=3,\n",
    "    conv_dim=96,\n",
    "    latent_dim=128,\n",
    ")\n",
    "\n",
    "reconstruction_loss_func = nn.MSELoss()\n",
    "kl_loss_func = nn.KLDivLoss(reduction='batchmean')\n",
    "opt = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "fit(10, model, reconstruction_loss_func, kl_loss_func, opt, train_dl, valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
