{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoders "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Architecturally, the difference between a VAE and an AE is that while the encoder in the AE directly outputs the encoded image, in a VAE it outputs mean and stdv values from which we can sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvolutionalVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        n_channels: int,\n",
    "        latent_dim: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 96, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.Conv2d(96, 192, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(384 * input_dim[1] // 8 * input_dim[2] // 8, 2 * latent_dim),\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 384 * input_dim[1] // 8 * input_dim[2] // 8),\n",
    "            nn.Unflatten(1, (384, input_dim[1] // 8, input_dim[2] // 8)),\n",
    "            nn.ConvTranspose2d(384, 192, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ConvTranspose2d(192, 96, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ConvTranspose2d(96, n_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def sample(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample from the latent space using the reparameterization trick.\n",
    "\n",
    "        Args:\n",
    "            mu: mean of the latent space\n",
    "            logvar: log variance of the latent space\n",
    "\n",
    "        Returns:\n",
    "            z: sampled latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the VAE.\n",
    "\n",
    "        Args:\n",
    "            x: input data\n",
    "\n",
    "        Returns:\n",
    "            x_hat: reconstructed data\n",
    "        \"\"\"\n",
    "        encoded = self.encoder(x)\n",
    "        mu, logvar = encoded.split(self.latent_dim, dim=1)\n",
    "        z = self.sample(mu, logvar)\n",
    "        return self.decoder(z), (mu, logvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_hat shape torch.Size([4, 3, 32, 32])\n",
      "mean shape torch.Size([4, 128])\n",
      "logvar shape torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "# Checking that dimenionality is correct\n",
    "vae = ConvolutionalVAE(\n",
    "    input_dim=(3, 32, 32),\n",
    "    n_channels=3,\n",
    "    latent_dim=128,\n",
    ")\n",
    "\n",
    "random_data = torch.randn(4, 3, 32, 32)\n",
    "x_hat, (mu, logvar) = vae(random_data)\n",
    "\n",
    "print(\"x_hat shape\", x_hat.shape)\n",
    "print(\"mean shape\", mu.shape)\n",
    "print(\"logvar shape\", logvar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function\n",
    "\n",
    "So we've adjusted the architecture and added in the sampling/reparameterization trick to allow the flow of gradients. What's left?\n",
    "\n",
    "the other difference between the VAE and the AE is the loss function. our loss term consists of two parts now, reconstruction and KL\n",
    "divergence of the latent distribution from a standard normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, reconstruction_loss_func, valid_dl, beta=1):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tot_loss = 0.\n",
    "        tot_recon_loss = 0.\n",
    "        tot_kl_loss = 0.\n",
    "        num_batches = 0\n",
    "        for xb, _ in valid_dl:\n",
    "            x_hat, (mu, logvar) = model(xb)\n",
    "            recon_loss = reconstruction_loss_func(x_hat, xb)\n",
    "            kl_loss = kl_loss_func(mu, logvar)\n",
    "            loss = recon_loss + beta * kl_loss  # apply beta weighting to balance regularization and reconstruction loss\n",
    "            \n",
    "            tot_loss += loss.item()\n",
    "            tot_recon_loss += recon_loss.item()\n",
    "            tot_kl_loss += kl_loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    return tot_loss / num_batches, tot_recon_loss / num_batches, tot_kl_loss / num_batches\n",
    "\n",
    "def kl_loss_func(mu, logvar):\n",
    "    # Clamp logvar for numerical stability\n",
    "    logvar = torch.clamp(logvar, -10, 10)\n",
    "    \n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return kl_loss / (mu.size(0) * mu.size(1)) # normalize to prevent explosion\n",
    "\n",
    "def fit(\n",
    "    epochs, \n",
    "    model,\n",
    "    reconstruction_loss_func, \n",
    "    opt, \n",
    "    train_dl, \n",
    "    valid_dl,\n",
    "    beta=1,\n",
    "    grad_clip=1.0\n",
    "):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            x_hat, (mu, logvar) = model(xb)\n",
    "            recon_loss = reconstruction_loss_func(x_hat, xb)\n",
    "            kl_loss = kl_loss_func(mu, logvar)\n",
    "            loss = recon_loss + beta * kl_loss  # Apply Î² weighting\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            opt.step()\n",
    "\n",
    "\n",
    "        total_loss, recon_loss, kl_loss = validate(model, reconstruction_loss_func, valid_dl, beta)\n",
    "        print(f\"Validation loss: {total_loss:.6f}\")\n",
    "        print(f\"Reconstruction loss: {recon_loss:.6f}\")\n",
    "        print(f\"KL loss: {kl_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data boiler plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "all_batches_data = []\n",
    "all_batches_labels = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    with open(f'data/cifar-10-batches-py/data_batch_{i}', 'rb') as f:\n",
    "        dataset_dict = pickle.load(f, encoding='bytes')\n",
    "        all_batches_data.append(dataset_dict[b'data'])\n",
    "        all_batches_labels.append(dataset_dict[b'labels'])\n",
    "\n",
    "stacked_data = np.vstack(all_batches_data)\n",
    "stacked_labels = np.hstack(all_batches_labels)\n",
    "data = torch.tensor(stacked_data, dtype=torch.float32).view(-1, 3, 32, 32).to(device) / 255.\n",
    "labels = torch.tensor(stacked_labels, dtype=torch.long).to(device)\n",
    "\n",
    "split_idx = int(0.8 * len(data))\n",
    "\n",
    "x_train, x_valid = data[:split_idx], data[split_idx:]\n",
    "y_train, y_valid = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CIFARCustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "train_ds = CIFARCustomDataset(x_train, y_train)\n",
    "valid_ds = CIFARCustomDataset(x_valid, y_valid)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Validation loss: 0.010172\n",
      "Reconstruction loss: 0.007969\n",
      "KL loss: 2.202168\n",
      "Epoch 2/100\n",
      "Validation loss: 0.018015\n",
      "Reconstruction loss: 0.006237\n",
      "KL loss: 11.778009\n",
      "Epoch 3/100\n",
      "Validation loss: 0.009682\n",
      "Reconstruction loss: 0.005758\n",
      "KL loss: 3.924230\n",
      "Epoch 4/100\n",
      "Validation loss: 0.012722\n",
      "Reconstruction loss: 0.005563\n",
      "KL loss: 7.158377\n",
      "Epoch 5/100\n",
      "Validation loss: 0.007233\n",
      "Reconstruction loss: 0.005456\n",
      "KL loss: 1.777084\n",
      "Epoch 6/100\n",
      "Validation loss: 0.007953\n",
      "Reconstruction loss: 0.006139\n",
      "KL loss: 1.813407\n",
      "Epoch 7/100\n",
      "Validation loss: 0.007282\n",
      "Reconstruction loss: 0.005565\n",
      "KL loss: 1.716720\n",
      "Epoch 8/100\n",
      "Validation loss: 0.008178\n",
      "Reconstruction loss: 0.005528\n",
      "KL loss: 2.650188\n",
      "Epoch 9/100\n",
      "Validation loss: 0.007699\n",
      "Reconstruction loss: 0.005500\n",
      "KL loss: 2.199640\n",
      "Epoch 10/100\n",
      "Validation loss: 0.007184\n",
      "Reconstruction loss: 0.005547\n",
      "KL loss: 1.637458\n",
      "Epoch 11/100\n",
      "Validation loss: 0.007052\n",
      "Reconstruction loss: 0.005414\n",
      "KL loss: 1.637540\n",
      "Epoch 12/100\n",
      "Validation loss: 0.006897\n",
      "Reconstruction loss: 0.005202\n",
      "KL loss: 1.695074\n",
      "Epoch 13/100\n",
      "Validation loss: 0.006855\n",
      "Reconstruction loss: 0.005177\n",
      "KL loss: 1.678319\n",
      "Epoch 14/100\n",
      "Validation loss: 0.007117\n",
      "Reconstruction loss: 0.005439\n",
      "KL loss: 1.677949\n",
      "Epoch 15/100\n",
      "Validation loss: 0.006758\n",
      "Reconstruction loss: 0.005065\n",
      "KL loss: 1.693342\n",
      "Epoch 16/100\n",
      "Validation loss: 0.006984\n",
      "Reconstruction loss: 0.005193\n",
      "KL loss: 1.791183\n",
      "Epoch 17/100\n",
      "Validation loss: 0.006722\n",
      "Reconstruction loss: 0.005024\n",
      "KL loss: 1.698039\n",
      "Epoch 18/100\n",
      "Validation loss: 0.006813\n",
      "Reconstruction loss: 0.005164\n",
      "KL loss: 1.649385\n",
      "Epoch 19/100\n",
      "Validation loss: 0.006784\n",
      "Reconstruction loss: 0.005124\n",
      "KL loss: 1.660428\n",
      "Epoch 20/100\n",
      "Validation loss: 0.006857\n",
      "Reconstruction loss: 0.005194\n",
      "KL loss: 1.662391\n",
      "Epoch 21/100\n",
      "Validation loss: 0.006746\n",
      "Reconstruction loss: 0.005081\n",
      "KL loss: 1.665250\n",
      "Epoch 22/100\n",
      "Validation loss: 0.008508\n",
      "Reconstruction loss: 0.005415\n",
      "KL loss: 3.092800\n",
      "Epoch 23/100\n",
      "Validation loss: 0.007152\n",
      "Reconstruction loss: 0.005228\n",
      "KL loss: 1.924655\n",
      "Epoch 24/100\n",
      "Validation loss: 0.006847\n",
      "Reconstruction loss: 0.005119\n",
      "KL loss: 1.728140\n",
      "Epoch 25/100\n",
      "Validation loss: 0.006870\n",
      "Reconstruction loss: 0.005161\n",
      "KL loss: 1.709608\n",
      "Epoch 26/100\n",
      "Validation loss: 0.006944\n",
      "Reconstruction loss: 0.005268\n",
      "KL loss: 1.676248\n",
      "Epoch 27/100\n",
      "Validation loss: 0.007255\n",
      "Reconstruction loss: 0.005172\n",
      "KL loss: 2.082951\n",
      "Epoch 28/100\n",
      "Validation loss: 0.006737\n",
      "Reconstruction loss: 0.005031\n",
      "KL loss: 1.705989\n",
      "Epoch 29/100\n",
      "Validation loss: 0.006799\n",
      "Reconstruction loss: 0.005103\n",
      "KL loss: 1.696412\n",
      "Epoch 30/100\n",
      "Validation loss: 0.006744\n",
      "Reconstruction loss: 0.005077\n",
      "KL loss: 1.667196\n",
      "Epoch 31/100\n",
      "Validation loss: 0.006901\n",
      "Reconstruction loss: 0.005183\n",
      "KL loss: 1.717998\n",
      "Epoch 32/100\n",
      "Validation loss: 0.006780\n",
      "Reconstruction loss: 0.005108\n",
      "KL loss: 1.671929\n",
      "Epoch 33/100\n",
      "Validation loss: 0.006836\n",
      "Reconstruction loss: 0.005172\n",
      "KL loss: 1.663927\n",
      "Epoch 34/100\n",
      "Validation loss: 0.006771\n",
      "Reconstruction loss: 0.005135\n",
      "KL loss: 1.636601\n",
      "Epoch 35/100\n",
      "Validation loss: 0.007019\n",
      "Reconstruction loss: 0.005150\n",
      "KL loss: 1.868952\n",
      "Epoch 36/100\n",
      "Validation loss: 0.007065\n",
      "Reconstruction loss: 0.005195\n",
      "KL loss: 1.869299\n",
      "Epoch 37/100\n",
      "Validation loss: 0.006748\n",
      "Reconstruction loss: 0.005104\n",
      "KL loss: 1.644879\n",
      "Epoch 38/100\n",
      "Validation loss: 0.007001\n",
      "Reconstruction loss: 0.005118\n",
      "KL loss: 1.883319\n",
      "Epoch 39/100\n",
      "Validation loss: 0.007241\n",
      "Reconstruction loss: 0.005169\n",
      "KL loss: 2.071610\n",
      "Epoch 40/100\n",
      "Validation loss: 0.006789\n",
      "Reconstruction loss: 0.005088\n",
      "KL loss: 1.701497\n",
      "Epoch 41/100\n",
      "Validation loss: 0.006851\n",
      "Reconstruction loss: 0.005191\n",
      "KL loss: 1.660417\n",
      "Epoch 42/100\n",
      "Validation loss: 0.006871\n",
      "Reconstruction loss: 0.005087\n",
      "KL loss: 1.784452\n",
      "Epoch 43/100\n",
      "Validation loss: 0.007013\n",
      "Reconstruction loss: 0.005059\n",
      "KL loss: 1.953811\n",
      "Epoch 44/100\n",
      "Validation loss: 0.006860\n",
      "Reconstruction loss: 0.005137\n",
      "KL loss: 1.723381\n",
      "Epoch 45/100\n",
      "Validation loss: 0.006954\n",
      "Reconstruction loss: 0.005054\n",
      "KL loss: 1.900255\n",
      "Epoch 46/100\n",
      "Validation loss: 0.007460\n",
      "Reconstruction loss: 0.005350\n",
      "KL loss: 2.110018\n",
      "Epoch 47/100\n",
      "Validation loss: 0.006855\n",
      "Reconstruction loss: 0.005118\n",
      "KL loss: 1.736860\n",
      "Epoch 48/100\n",
      "Validation loss: 0.006838\n",
      "Reconstruction loss: 0.005154\n",
      "KL loss: 1.684028\n",
      "Epoch 49/100\n",
      "Validation loss: 0.007124\n",
      "Reconstruction loss: 0.005198\n",
      "KL loss: 1.926534\n",
      "Epoch 50/100\n",
      "Validation loss: 0.006906\n",
      "Reconstruction loss: 0.004997\n",
      "KL loss: 1.908795\n",
      "Epoch 51/100\n",
      "Validation loss: 0.007000\n",
      "Reconstruction loss: 0.005090\n",
      "KL loss: 1.909852\n",
      "Epoch 52/100\n",
      "Validation loss: 0.006784\n",
      "Reconstruction loss: 0.005106\n",
      "KL loss: 1.677612\n",
      "Epoch 53/100\n",
      "Validation loss: 0.007092\n",
      "Reconstruction loss: 0.005218\n",
      "KL loss: 1.874497\n",
      "Epoch 54/100\n",
      "Validation loss: 0.007012\n",
      "Reconstruction loss: 0.005190\n",
      "KL loss: 1.822045\n",
      "Epoch 55/100\n",
      "Validation loss: 0.008019\n",
      "Reconstruction loss: 0.005309\n",
      "KL loss: 2.710631\n",
      "Epoch 56/100\n",
      "Validation loss: 0.007199\n",
      "Reconstruction loss: 0.005140\n",
      "KL loss: 2.058263\n",
      "Epoch 57/100\n",
      "Validation loss: 0.007299\n",
      "Reconstruction loss: 0.005194\n",
      "KL loss: 2.105278\n",
      "Epoch 58/100\n",
      "Validation loss: 0.006759\n",
      "Reconstruction loss: 0.005068\n",
      "KL loss: 1.691464\n",
      "Epoch 59/100\n",
      "Validation loss: 0.006991\n",
      "Reconstruction loss: 0.005260\n",
      "KL loss: 1.731137\n",
      "Epoch 60/100\n",
      "Validation loss: 0.007540\n",
      "Reconstruction loss: 0.005802\n",
      "KL loss: 1.737862\n",
      "Epoch 61/100\n",
      "Validation loss: 0.006953\n",
      "Reconstruction loss: 0.005216\n",
      "KL loss: 1.737840\n",
      "Epoch 62/100\n",
      "Validation loss: 0.007022\n",
      "Reconstruction loss: 0.005156\n",
      "KL loss: 1.866528\n",
      "Epoch 63/100\n",
      "Validation loss: 0.007157\n",
      "Reconstruction loss: 0.005141\n",
      "KL loss: 2.016660\n",
      "Epoch 64/100\n",
      "Validation loss: 0.006892\n",
      "Reconstruction loss: 0.005133\n",
      "KL loss: 1.759040\n",
      "Epoch 65/100\n",
      "Validation loss: 0.006836\n",
      "Reconstruction loss: 0.005151\n",
      "KL loss: 1.685004\n",
      "Epoch 66/100\n",
      "Validation loss: 0.006749\n",
      "Reconstruction loss: 0.005044\n",
      "KL loss: 1.705138\n",
      "Epoch 67/100\n",
      "Validation loss: 0.007149\n",
      "Reconstruction loss: 0.005173\n",
      "KL loss: 1.975720\n",
      "Epoch 68/100\n",
      "Validation loss: 0.006772\n",
      "Reconstruction loss: 0.005057\n",
      "KL loss: 1.715524\n",
      "Epoch 69/100\n",
      "Validation loss: 0.007983\n",
      "Reconstruction loss: 0.005223\n",
      "KL loss: 2.759260\n",
      "Epoch 70/100\n",
      "Validation loss: 0.006779\n",
      "Reconstruction loss: 0.005090\n",
      "KL loss: 1.688881\n",
      "Epoch 71/100\n",
      "Validation loss: 0.006809\n",
      "Reconstruction loss: 0.005143\n",
      "KL loss: 1.666112\n",
      "Epoch 72/100\n",
      "Validation loss: 0.006727\n",
      "Reconstruction loss: 0.005043\n",
      "KL loss: 1.683729\n",
      "Epoch 73/100\n",
      "Validation loss: 0.006759\n",
      "Reconstruction loss: 0.005068\n",
      "KL loss: 1.690850\n",
      "Epoch 74/100\n",
      "Validation loss: 0.007313\n",
      "Reconstruction loss: 0.005601\n",
      "KL loss: 1.712206\n",
      "Epoch 75/100\n",
      "Validation loss: 0.006777\n",
      "Reconstruction loss: 0.005070\n",
      "KL loss: 1.707046\n",
      "Epoch 76/100\n",
      "Validation loss: 0.007234\n",
      "Reconstruction loss: 0.005296\n",
      "KL loss: 1.937964\n",
      "Epoch 77/100\n",
      "Validation loss: 0.006730\n",
      "Reconstruction loss: 0.005041\n",
      "KL loss: 1.689415\n",
      "Epoch 78/100\n",
      "Validation loss: 0.006769\n",
      "Reconstruction loss: 0.005073\n",
      "KL loss: 1.695623\n",
      "Epoch 79/100\n",
      "Validation loss: 0.006752\n",
      "Reconstruction loss: 0.005076\n",
      "KL loss: 1.675633\n",
      "Epoch 80/100\n",
      "Validation loss: 0.007666\n",
      "Reconstruction loss: 0.005376\n",
      "KL loss: 2.290373\n",
      "Epoch 81/100\n",
      "Validation loss: 0.008454\n",
      "Reconstruction loss: 0.005162\n",
      "KL loss: 3.291836\n",
      "Epoch 82/100\n",
      "Validation loss: 0.006794\n",
      "Reconstruction loss: 0.005101\n",
      "KL loss: 1.692550\n",
      "Epoch 83/100\n",
      "Validation loss: 0.006866\n",
      "Reconstruction loss: 0.005195\n",
      "KL loss: 1.671000\n",
      "Epoch 84/100\n",
      "Validation loss: 0.006737\n",
      "Reconstruction loss: 0.005069\n",
      "KL loss: 1.667839\n",
      "Epoch 85/100\n",
      "Validation loss: 0.007638\n",
      "Reconstruction loss: 0.005485\n",
      "KL loss: 2.152908\n",
      "Epoch 86/100\n",
      "Validation loss: 0.006794\n",
      "Reconstruction loss: 0.005109\n",
      "KL loss: 1.684438\n",
      "Epoch 87/100\n",
      "Validation loss: 0.006875\n",
      "Reconstruction loss: 0.005161\n",
      "KL loss: 1.713766\n",
      "Epoch 88/100\n",
      "Validation loss: 0.006820\n",
      "Reconstruction loss: 0.005120\n",
      "KL loss: 1.699897\n",
      "Epoch 89/100\n",
      "Validation loss: 0.006799\n",
      "Reconstruction loss: 0.005118\n",
      "KL loss: 1.681295\n",
      "Epoch 90/100\n",
      "Validation loss: 0.050312\n",
      "Reconstruction loss: 0.005092\n",
      "KL loss: 45.220704\n",
      "Epoch 91/100\n",
      "Validation loss: 0.192887\n",
      "Reconstruction loss: 0.005174\n",
      "KL loss: 187.713286\n",
      "Epoch 92/100\n",
      "Validation loss: 0.282539\n",
      "Reconstruction loss: 0.005211\n",
      "KL loss: 277.328047\n",
      "Epoch 93/100\n",
      "Validation loss: 0.787704\n",
      "Reconstruction loss: 0.005122\n",
      "KL loss: 782.581521\n",
      "Epoch 94/100\n",
      "Validation loss: 0.962190\n",
      "Reconstruction loss: 0.005255\n",
      "KL loss: 956.934522\n",
      "Epoch 95/100\n",
      "Validation loss: 0.169063\n",
      "Reconstruction loss: 0.005246\n",
      "KL loss: 163.817434\n",
      "Epoch 96/100\n",
      "Validation loss: 0.495691\n",
      "Reconstruction loss: 0.005192\n",
      "KL loss: 490.498078\n",
      "Epoch 97/100\n",
      "Validation loss: 47.523221\n",
      "Reconstruction loss: 0.005309\n",
      "KL loss: 47517.910852\n",
      "Epoch 98/100\n",
      "Validation loss: 0.636885\n",
      "Reconstruction loss: 0.005249\n",
      "KL loss: 631.635929\n",
      "Epoch 99/100\n",
      "Validation loss: 0.008192\n",
      "Reconstruction loss: 0.005317\n",
      "KL loss: 2.874359\n",
      "Epoch 100/100\n",
      "Validation loss: 0.007593\n",
      "Reconstruction loss: 0.005323\n",
      "KL loss: 2.269764\n"
     ]
    }
   ],
   "source": [
    "model = ConvolutionalVAE(\n",
    "    input_dim=(3, 32, 32),\n",
    "    n_channels=3,\n",
    "    latent_dim=128,\n",
    ")\n",
    "\n",
    "reconstruction_loss_func = nn.MSELoss()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "fit(100, model, reconstruction_loss_func, opt, train_dl, valid_dl, beta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_torch_image(img):\n",
    "    img = img.permute(1, 2, 0)\n",
    "    img = img.numpy().astype(np.uint8)\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAWmUlEQVR4nO3cy64k6XUd4B2Rl5PnUlVd3bzAlGxJsClItia2J5rozfwMfh8/iwHLsEDSzW6q63ZumZERHhD+OfReQBM2je8b79oVJy65Mgexpm3btgKAqpr/bx8AAP/vEAoADEIBgEEoADAIBQAGoQDAIBQAGIQCAMO+O/jvf/nTaPHz+dqevVb2/tzb22N79j/88mfR7r/8i/7fef/mJtr9/PTSnv308Uu0e5qyc3g8Tu3Zedtlx7L2v2vMh/B7STA+z+3bu6qqlsuaHcvUP5j9MTuWOdi9Ltlxb9Wf37Zs927Xv1d2c3jts9uwatd/JuZdeK9c+8/PGr4ffL3+8d4n/k//+b/8H2f8UgBgEAoADEIBgEEoADAIBQAGoQDAIBQAGIQCAINQAGAQCgAMQgGAoV348Rp0GVVVnS/9+eWa9as8B50pjy/naPflcmnPPn3pz1ZVvQTdR9elP1tVNc/9LpaqquUczF+z7w7T2i+pmV6z3VFvzxSeky27x3e7Q3v2eE27j4JzGG3OunjW4HxXVS2XpT07hffs4Y/ZH3XNnuVz0Df1GnwWVlUlVVbJ39je+aNvBOBPllAAYBAKAAxCAYBBKAAwCAUABqEAwCAUABiEAgCDUABgyN4bDyQNAzf7LJu+uu/XC5xu+nUBVVXL5bU9+/LUn62qej33Kzf2U1YvsAbVH1VVl7V/gbYl/O4QvKe/S0sagoqGJaxoWPdZHUEF89e1f89WVV2D+oLr2j8nVVXzLrie6dfG5MHfwmsfXp75tb8/qf6oqno592sxnl/DmovkUII6lC6/FAAYhAIAg1AAYBAKAAxCAYBBKAAwCAUABqEAwCAUABiEAgCDUABgaHcfnY5Zx8Z+1+8dub/JKph+9u62PfsQHvf62u8zWpes+2i+9rt45n3WCzOH3S11CTqEojKWqjn4rrGuYT/RdWnPLhWek2s2vwb9Ues+273b9Z+JXdghNCf9ROk5TDq4wuPehx1P16XfOfT80u8yqqp6CuYfz1n30TW4Pvud7iMA/oiEAgCDUABgEAoADEIBgEEoADAIBQAGoQDAIBQAGIQCAEP7Xfqf/+QhWvz8dG7P3t9kr2r/9OHQ353WRQQ1Crste319Dg5ln+Z1dig1JY0BYR3BtvYP5vXcP99VVeclqSPIjvsQVLNUVSUNHZdrdoHWub98Su+VfX/3bp/tnoLx3SE73zdJhUZVnYNzvr5m9+HlsX8fXpesyuUa/J1T/6OwzS8FAAahAMAgFAAYhAIAg1AAYBAKAAxCAYBBKAAwCAUABqEAwCAUABja3Uc/e38fLb7c90s53t5l3Ufvbvu7D4ds97QFpUBTtnu79ndH3URVddxnJSjrpf8fPIa9ME+vQffRJesEWoLCoX1SNlVVD6fsO1JQT1Tbkl3Qy9bv1tmyap2soCg8h9Ou/0w8vL2Ndt/O2fNWU//ELMGzWVX1svSfiedL2H0UzB7W9kd4m18KAAxCAYBBKAAwCAUABqEAwCAUABiEAgCDUABgEAoADEIBgKH9jvTXX2U1F+u1//r13SF7xXya+q/ez+Fr+tvan5+2LFOTdoFaw3OyZX/n+aX/Mv3nz+do9w/P/fkl/DvnoEbh7phdn2vWuFFJs0jYWlKXpV+NcA6rQoKmkForfH6C523JVtfp9hTNT1P/M2i3z+oi1uAzKDyFtQVVO2nDSYdfCgAMQgGAQSgAMAgFAAahAMAgFAAYhAIAg1AAYBAKAAxCAYBBKAAwtAs/vvrqJlq8vPb7O6ZtiXYnSXacs9xbl363zhKW5WxBA84uKkqqul6yFpQvQT/RD4+v0e4Pz5f2bNp9dHfT76h5d8q6ct4/ZPf43e2xPXtesnv8w6V/zq/X7Nqfg16lLSzumXf9+3a5ZOdkC//O022/nOrhPrv2z5f+Pb4/Z3/nEnQf7cLPtw6/FAAYhAIAg1AAYBAKAAxCAYBBKAAwCAUABqEAwCAUABiEAgBDuzPgcMjqCI77fh3BtGbZNAXVCGv4innyUv8ueKU/3p0ddj2HlRtPwWv6T+f+bFVWoxC2XESVDjf77Pp88yarxXgT1GI8v2TnMKl0eA3rIqJHYsouUHLKd2tWW/H89BTNJ8e+m7M6jzdBxcnp0P8srKq6bv3zMoWfQR1+KQAwCAUABqEAwCAUABiEAgCDUABgEAoADEIBgEEoADAIBQAGoQDA0C7lON1k/R3ztmvPbkvW25PMb4dodV3W4FiWrBfmGnS9TGEXy9w/3VVVdRtcz+M+W3649s/LIdz97q5/Qe9P2e6b8Bweg2u0u+l35VRVne+C7qOwQOpw6N/ja3YbVk3975m78NqHHxN1Dkqe5vB52+/681P4cC5BF9wcHEd754++EYA/WUIBgEEoADAIBQAGoQDAIBQAGIQCAINQAGAQCgAMQgGAQSgAMLQLcOZdlh9zv7qlpjnbve3783HvyPm1PXs+Z50zS9AJlHSrVFXtwuvz1cNtezas1qkPj/1zmB732/t+h9Dbu7CvK+imqqraouuZlXC9ub9rz+5ON9Hux9d+J9BT0B9UVXW59s/hNbyxsqtTdQ66yfa78CavfhHTHH6+JZ9YW9jZ1OGXAgCDUABgEAoADEIBgEEoADAIBQAGoQDAIBQAGIQCAINQAGBo9wAcD/16gaqqLXg9fpqzV8zn5NXuLdt9uby0Z3/3sT9bVXUNahRub7K8/uY+q1F4/9CvgPjmzZto93Xtz6c1F7c3/eO+CepQqqr2ld0rd7f9eonjPrs+1+B5Ww7Z7qeXS3v245fsHv/nz8/t2R+C2aqqa1ArUlU1Tf3PoLWyuohjcm+lNT7BfTj9+C0XfikA8AdCAYBBKAAwCAUABqEAwCAUABiEAgCDUABgEAoADEIBgEEoADC0i2Tm3S5avAZ9Rvuwv+MQ9OWcX7Luli/P/V6Y33zIdk/rtT37/j4739+csnx//3Bqz757n3Uf3d7dtWcPYW9PUnuVDVdV9a9PVdVh1+8n2qXfv24f2qPXU/98V1VdLv1n8+kp6yf6n99/aM+e/+nbaPflfI7m56l/PffhvXII5uew3+vQr/eq6x+h/MgvBQAGoQDAIBQAGIQCAINQAGAQCgAMQgGAQSgAMAgFAAahAMDQr7nY+q/GV1VNwevXWaFDVXIoy7pGu3/43K+u+PyUvXa/Bsfy5i54172qbu+zqoM3777q7364j3af7m/bsw9v30W77969b8/uj1mFRlpzMU/9mot5zo5lnfvXf4k2V51f+lUu92HNxemhfz2nY//8VVV99+130fzr62t7dq7s8203BfO7H7+K4n9b1+y4O/xSAGAQCgAMQgGAQSgAMAgFAAahAMAgFAAYhAIAg1AAYBAKAAxCAYChXbCyLVmH0Lr0e2TOl2x3EmWPz/3+k6qqT0/9+ef0uKs/fwh7Yd5+/XU0f/PQ7xC6LP2unKqq84fH9uztm/5xVFXdfv1n7dmb+4do9/7Q72yqqpp2/T6jLfz+db30O7henz5Hu/eH/j1+OGadWqe3wfU8ZPf4eslanr799vv27Jx0GVXVPPWPZZqz7qOk2y2omGvzSwGAQSgAMAgFAAahAMAgFAAYhAIAg1AAYBAKAAxCAYBBKAAwtGsuzq9Z1cHyEtRLBJUYVVVb8G73S3Iclb2Sfjxmmbqbd+3Zn//0XbT7X/7VX0bz7+5O7dnvf/2raPcSVKLMu/45qao6nPpVFMegyqOqan/KznlSR7AuWUVDzf0KjV32+NS69u/x001WFXIKns3rnD0/3//m19H873b9/UtY5bJV/+Kv16xC4xx8Hk7hOezwSwGAQSgAMAgFAAahAMAgFAAYhAIAg1AAYBAKAAxCAYBBKAAwCAUAhnb30XIOu4+CrpdpDbuPgtKZ18eXaPfp0O9uef/2GO1+e3vTnv27f/evo92/+De/jOavz4/t2em330a757l/fXY3/S6jqqp53z+H064/+/uDya5nVH5U/fuqqmq79p+fa79q6vfm9mNfh9v7aPUUdFndXrPPlOOh3wdVlfWePT9lnxM3N/3v02H1UV3X4PkJPq+6/FIAYBAKAAxCAYBBKAAwCAUABqEAwCAUABiEAgCDUABgEAoADEIBgKFdgrIGfRxVVdeg8GNdst1b0g2yz/pS9nO/c+Yn77Junb/563/Vn/2Pfx/tfvP+J9H8l9/89/bsITyHp6CjJukyqqparv2erO3xS7R7XrISoSXoj3r9/CHa/fIU7L7079mqquOx3/E0HbI+qN3U/545He6i3emxvL7275XPj/2epKqqber3R123rJ9o3vfP4f7Hrz7ySwGAPxAKAAxCAYBBKAAwCAUABqEAwCAUABiEAgCDUABgEAoADP13tcP4WIN/cM3aBWofvAb+5phVNDwHb7v//OffRLv//h/+oT37kz//62j3vPVf6a+qeg0qA27vszqC/XHXnj2fL9Hub//pH9uzj2F1wcsluxEvz0/t2fPnrHLjvPSv5+72Ntr9iz//RXv29O59tLum4HkLP1MOp+w+vAZ1OMuWVe1cgg+tsOWi9klVyB+h58IvBQAGoQDAIBQAGIQCAINQAGAQCgAMQgGAQSgAMAgFAAahAMAgFAAY+t1HlXVsXM797pbrJevtqep36xxOwZ9YVf/i51+3Z//t3/1ttPtnf/YX7dndlHWxLC9Zt871urRnp7l/vquy3quPP3yOdv/ud/+jPftf/9uvot0fn16i+dO+f17e3d5Euw+nU3v2zddfRbs/f+x3CL17/BTtPu36nVrL0r8Hq6qen7J7fKr+/mPYITQFXUm7Xfbdexd8ZB2yR7PFLwUABqEAwCAUABiEAgCDUABgEAoADEIBgEEoADAIBQAGoQDA0H6h+rqs0eLn53N7dg13L0tSi5HVRfzk64f27HGfVWhcn/qVDuelf/6qql4fP0bzT5/788uaXZ8tqC05X7J6gd/98Nie/fDpOdr9Ep7z05t+XcT+mN0r877/fe3lJavn+PjDh/bsw3e/jXa/CZ7NT1+yCo3vfvPraH7aLu3Z2+zy1Lr2/87dlFZo9J+35Di6/FIAYBAKAAxCAYBBKAAwCAUABqEAwCAUABiEAgCDUABgEAoADEIBgKHd+LFGfUNVy7K0Z+cwmragS+TlnPXZvLy+tmc/ffwQ7f7uV//Ynp2v/d6WqqqnT/1epaqqx4/9+eWaXfvTw6m/e9tFu68V9DBNWe/V2zf9466q+uabt+3Zu9Mx2v382n9+Pn4Ie68e+/1R1yXrj3r/zdft2Y+PX6Ld3/32+2j+HHwGbdmtUrenQ3v2eNOf/f18v4gp6chq7/zRNwLwJ0soADAIBQAGoQDAIBQAGIQCAINQAGAQCgAMQgGAQSgAMLTfp96HXRTXrV+NsFW/tqKq6njoVyOs1+z99XNQL/H9P2ev3V/XfoXGdunPVlUtr1kVRXB56vY+q3+Yd/17Zbfrv9JfVXX75qY9+/C2P1tVdTqGx/LQ339dg3qOqnp8fWnPfvic1UVcL/17/POXT9Hur777bX/3Y1ah8TH8O/eH5HMl+wzaBZ9BSSVGVdVtUItxnbPj7vBLAYBBKAAwCAUABqEAwCAUABiEAgCDUABgEAoADEIBgEEoADAIBQCGdtnLN1/dRosfPwf9HZX1wuz7tSO1Tlk3yFT9UqDlnHW3nF/6Bz6H52ROTkpVHe/6vT23d3fR7tNd/165ZNVUdXfXv6/ePGSdTVPYI7Ot/XtlDTq1qqq267k9O29LtPu89Hd/+pR1cF2XfmdThdd+FzybVVXHoIPrcMh6r+6T5+eUdXAdjv1nefvxq4/8UgDgD4QCAINQAGAQCgAMQgGAQSgAMAgFAAahAMAgFAAYhAIAQ/vd7ndvs1e1k/lPT8Gr8VW1rv0KiEPwqntV1S6ol9i2fl1AVdV17VcGTHNWWzGFNRd17J+X+ZRVAMxBZcB8zSoajsf+e/0PD8do99NzVunwGtWcZBUNU/XPyy6subgJLue8y7ookuM+hdUSD0G1RFXVrt+IUjfHbPdtMH9zCg6kqnbBs7yEVSEdfikAMAgFAAahAMAgFAAYhAIAg1AAYBAKAAxCAYBBKAAwCAUABqEAwNAuH7m9yfo73t73u0HOl6y75fXS75G5ht0gl6W/ezlnfTbPz/2upN0h6zLagt1VVYdgfjlfot3PN4/93UGPVVXVl+en9uzTc9JNVHUJ78Np6t9cyezv5/uzp6TMqKq2qf9dcH8IDqSqjnN/fp/8kVU177L5/W3/vBz32Tk8BMcyh1+9d/v+7mnNzkmHXwoADEIBgEEoADAIBQAGoQDAIBQAGIQCAINQAGAQCgAMQgGAQSgAMLQLP8Kakrq97XcfHR5fo92Xtd8j8xT2E63B7l2Yqee138WzBh0yVVXXJesQ2m39/Q8vL9Hu4+nYnr1W1gn0/Nq/V86XrA9qu2bnMOriCe6rqqpdUH11e8p6ybY5OJasgqv2U/8c7sPzXfvsedvt+we/C3dPwTmc5uwzqILxdQ4vUINfCgAMQgGAQSgAMAgFAAahAMAgFAAYhAIAg1AAYBAKAAxCAYChXXOxhfkxB+/pn27ah1FVVeel/x74zS6sFwiaC6bkffSqWoMqiiWooaiqWtesMmALeks+f8l278/9eok5qVyoquu1f87j2ootu8f31T+HaRtBdDnDSpRtWvqza3aPX4MDz658VdoWUZdLe3QN7quqqt3UrxZJnrWqqnXqH3eFnxMdfikAMAgFAAahAMAgFAAYhAIAg1AAYBAKAAxCAYBBKAAwCAUABqEAwNAuHZqPN9Hi032/A+VNhR01u36Wvb4EPSKV1cjMYaROc7/jKawyqiXsqLkE80mfTVXV5dLvPtonZVNVtQ+6kg77rHBou2YXNDr0KbygQa/SsmYtQtuWXPvs+VmDY9nCrqld2u917Xc8VXgs29I/L0t43Oelfw4vyd/Y5JcCAINQAGAQCgAMQgGAQSgAMAgFAAahAMAgFAAYhAIAg1AAYGj3LhyP/YqGqqptO7Znr+tdtHu/9SsAXvfP0e4t2D3vshqFNTnuc1YvsF2zqoMKqkJ2aY1C9ed3Sa9IZdUic3C+q6qmbLx2wTlMO1EuQfXLsvZrRaqqlqW/O7yrovtwS6tZsvFagqO/XLK/9HwO/s6gsqSqat2CKpfkHmzySwGAQSgAMAgFAAahAMAgFAAYhAIAg1AAYBAKAAxCAYBBKAAwCAUAhnah0f6Y9fwk3Ue1Zbvv9v357U1wHFV1vfZzcu2fvqqquiz9PqPD5TXa/XrOimGWoBdo27Ld29Tv1qmwF2Zb+/PTdYl276fs75yC3qZ1yr5/JVVJUQdTVZ2DezztG5qDCqE5bFa6hsdyDnqYXs7JPVv1cunP73fZPb7fJ/1rYWFXZ+ePvhGAP1lCAYBBKAAwCAUABqEAwCAUABiEAgCDUABgEAoADEIBgKHd0zAfbrLF06E/HIxWVe3qtj0777LX19el/2r8NaiKqKragvf0L2t23MuSVToEf2atYV3Ea1DnkXYXbFv/wPdr2IsQ1CJUVa1rf/71ml3P+dK/t8LVNQW1GNmVr9pX/2DmKXt+liX7Q/ufEtl9VVWVtMpsW3bcW1D/Mc1ZjU+HXwoADEIBgEEoADAIBQAGoQDAIBQAGIQCAINQAGAQCgAMQgGAQSgAMExbWvoBwP+3/FIAYBAKAAxCAYBBKAAwCAUABqEAwCAUABiEAgCDUABg+F+LjpTk1814egAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_output, (mu, logvar) = model(x_train[0].unsqueeze(0))\n",
    "print(sample_output.shape)\n",
    "show_torch_image(255*sample_output.squeeze(0).detach().cpu())\n",
    "show_torch_image(255 * x_train[0].cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some learnings so far\n",
    "\n",
    "- VAEs are a lot more sensitive to hyperparameters due to the existence of multiple loss terms. Ideally, we might want to put our hparam beta on a schedule. \n",
    "- VAEs are regularized and therefore generalize in distribution better than AEs that inherently overfit to their training dataset \n",
    "- We can see the smoothness problem that led to the introduction of VQ-VAEs above\n",
    "- VAEs take a lot more time to train because: \n",
    "    1. There are more layers/parameters \n",
    "    2. The reparameterization trick adds some complexity to how gradients flow \n",
    "    3. There is a dance going on between loss functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some things we can do to improve performance \n",
    "\n",
    "- Instead of using a linear layer to output a flattened latent, we could use a conv layer to better represent spatial information \n",
    "- Train for longer \n",
    "- Use a beta scheduler for the b-vae implementation \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
