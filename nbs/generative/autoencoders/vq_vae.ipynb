{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Quantized Variational Autoencoders (VQ-VAEs)\n",
    "\n",
    "Rather than computing a continuous latent representation of the input, in VQ-VAEs we compute a discrete latent representation. Here, our encoder output is discretized with respect to a learned discrete set of embeddings we refer to as a codebook.\n",
    "\n",
    "The goal here is to tackle the smoothing problem noticed in general VAEs, as well as to reduce computational complexity. We can then use the (encoder + codebook) as a tokenizer for different types of transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        n_channels: int,\n",
    "        latent_dim: int,\n",
    "        codebook_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, 96, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.Conv2d(96, 192, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.Conv2d(384, latent_dim, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(latent_dim)\n",
    "        )\n",
    "\n",
    "        self.codebook = nn.Parameter(torch.zeros(codebook_size, latent_dim)*0.1, requires_grad=True)\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 384, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ConvTranspose2d(384, 192, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ConvTranspose2d(192, 96, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ConvTranspose2d(96, n_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the VAE.\n",
    "\n",
    "        Args:\n",
    "            x: input data\n",
    "\n",
    "        Returns:\n",
    "            x_hat: reconstructed data\n",
    "        \"\"\"\n",
    "        z_encoder = self.encoder(x)\n",
    "        z_quantized = self._quantize_encoder_output(z_encoder)\n",
    "        z_q_st = z_encoder + (z_quantized - z_encoder).detach()\n",
    "        return self.decoder(z_q_st), self.codebook, z_encoder, z_quantized\n",
    "    \n",
    "    def _quantize_encoder_output(self, z_e: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Quantizing the encoded tensor by snapping its elements to the closest codebook \n",
    "        entry.\n",
    "\n",
    "        Args:\n",
    "            z_e: encoded representation\n",
    "\n",
    "        Returns:\n",
    "            z_q: quantized representation\n",
    "        \"\"\"\n",
    "        batch_size, latent_dim, h, w = z_e.shape\n",
    "        encoded = z_e.permute(0, 2, 3, 1).reshape(batch_size*h*w, latent_dim)\n",
    "        quantized = self.codebook[torch.argmin(torch.cdist(encoded, self.codebook), dim=1)]\n",
    "        z_q = quantized.reshape(batch_size, h, w, latent_dim).permute(0, 3, 1, 2)\n",
    "        return z_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 3, 64, 64])\n",
      "Running forward pass...\n",
      "Output shape: torch.Size([2, 3, 64, 64]), codebook.shape: torch.Size([512, 128])\n",
      "z_e.shape: torch.Size([2, 128, 6, 6]), z_quantized.shape: torch.Size([2, 128, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "# Create dummy tensors and model for testing\n",
    "batch_size = 2\n",
    "n_channels = 3\n",
    "height = 64\n",
    "width = 64\n",
    "latent_dim = 128\n",
    "codebook_size = 512\n",
    "\n",
    "dummy_input = torch.randn(batch_size, n_channels, height, width)\n",
    "\n",
    "dummy_model = VQVAE(\n",
    "    input_dim=(n_channels, height, width),\n",
    "    latent_dim=latent_dim,\n",
    "    n_channels=n_channels,\n",
    "    codebook_size=codebook_size\n",
    ")\n",
    "\n",
    "print(\"Input shape:\", dummy_input.shape)\n",
    "print(\"Running forward pass...\")\n",
    "x_hat, codebook, z_e, z_quantized = dummy_model(dummy_input)\n",
    "print(f\"Output shape: {x_hat.shape}, codebook.shape: {codebook.shape}\")\n",
    "print(f\"z_e.shape: {z_e.shape}, z_quantized.shape: {z_quantized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "\n",
    "Our objective function consists of 3 terms:\n",
    "\n",
    "![VQ-VAE Loss Function](images/vq_vae_loss.png)\n",
    "\n",
    "1. Reconstruction Loss: Ensures the decoded output matches the input\n",
    "2. Codebook Loss: Keeps the codebook entries close to the encoded representations  \n",
    "3. Commitment Loss: Prevents the encoder from growing too large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    model: nn.Module,\n",
    "    recon_loss_fn: nn.Module,\n",
    "    cb_loss_fn: nn.Module,\n",
    "    commit_loss_fn: nn.Module,\n",
    "    valid_dl: DataLoader,\n",
    "    beta: float = 1.\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tot_loss = 0.\n",
    "        tot_recon_loss = 0.\n",
    "        tot_codebook_loss = 0.\n",
    "        tot_commit_loss = 0.\n",
    "        num_batches = 0\n",
    "        for xb, _ in valid_dl:\n",
    "            x_hat, codebook, z_e, z_quantized = model(xb)\n",
    "            recon_loss = recon_loss_fn(x_hat, xb)\n",
    "            codebook_loss = cb_loss_fn(z_quantized, z_e.detach())\n",
    "            commit_loss = commit_loss_fn(z_e, z_quantized.detach())\n",
    "            loss = recon_loss + codebook_loss + (beta * commit_loss)\n",
    "            \n",
    "            tot_loss += loss.item()\n",
    "            tot_recon_loss += recon_loss.item()\n",
    "            tot_codebook_loss += codebook_loss.item()\n",
    "            tot_commit_loss += commit_loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    tot_loss /= num_batches\n",
    "    tot_recon_loss /= num_batches\n",
    "    tot_codebook_loss /= num_batches\n",
    "    tot_commit_loss /= num_batches\n",
    "\n",
    "    return tot_loss, tot_recon_loss, tot_codebook_loss, tot_commit_loss\n",
    "\n",
    "def kl_loss_func(mu, logvar):\n",
    "    # Clamp logvar for numerical stability\n",
    "    logvar = torch.clamp(logvar, -10, 10)\n",
    "    \n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return kl_loss / (mu.size(0) * mu.size(1)) # normalize to prevent explosion\n",
    "\n",
    "def fit(\n",
    "    epochs: int, \n",
    "    model: nn.Module,\n",
    "    recon_loss_fn: nn.Module,\n",
    "    cb_loss_fn: nn.Module,\n",
    "    commit_loss_fn: nn.Module,\n",
    "    opt: torch.optim.Optimizer, \n",
    "    train_dl: DataLoader, \n",
    "    valid_dl: DataLoader,\n",
    "    beta: float = 1,\n",
    "    grad_clip: float = 1.0\n",
    "):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        model.train()\n",
    "        for xb, _ in train_dl:\n",
    "            x_hat, codebook, z_e, z_quantized = model(xb)\n",
    "            recon_loss = recon_loss_fn(x_hat, xb)\n",
    "            codebook_loss = cb_loss_fn(z_quantized, z_e.detach())\n",
    "            commit_loss = commit_loss_fn(z_e, z_quantized.detach())\n",
    "            loss = recon_loss + codebook_loss + (beta * commit_loss)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            opt.step()\n",
    "\n",
    "\n",
    "        total_loss, recon_loss, codebook_loss, commit_loss = validate(\n",
    "            model, recon_loss_fn, cb_loss_fn, commit_loss_fn, valid_dl, beta\n",
    "        )\n",
    "        print(f\"Validation loss: {total_loss:.6f}\")\n",
    "        print(f\"Reconstruction loss: {recon_loss:.6f}\")\n",
    "        print(f\"Codebook loss: {codebook_loss:.6f}\")\n",
    "        print(f\"Commit loss: {commit_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "all_batches_data = []\n",
    "all_batches_labels = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    with open(f'data/cifar-10-batches-py/data_batch_{i}', 'rb') as f:\n",
    "        dataset_dict = pickle.load(f, encoding='bytes')\n",
    "        all_batches_data.append(dataset_dict[b'data'])\n",
    "        all_batches_labels.append(dataset_dict[b'labels'])\n",
    "\n",
    "stacked_data = np.vstack(all_batches_data)\n",
    "stacked_labels = np.hstack(all_batches_labels)\n",
    "data = torch.tensor(stacked_data, dtype=torch.float32).view(-1, 3, 32, 32).to(device) / 255.\n",
    "labels = torch.tensor(stacked_labels, dtype=torch.long).to(device)\n",
    "\n",
    "split_idx = int(0.8 * len(data))\n",
    "\n",
    "x_train, x_valid = data[:split_idx], data[split_idx:]\n",
    "y_train, y_valid = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "class CIFARCustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "train_ds = CIFARCustomDataset(x_train, y_train)\n",
    "valid_ds = CIFARCustomDataset(x_valid, y_valid)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Validation loss: 0.625217\n",
      "Reconstruction loss: 0.035543\n",
      "Codebook loss: 0.536067\n",
      "Commit loss: 0.536067\n",
      "Epoch 2/100\n",
      "Validation loss: 0.249973\n",
      "Reconstruction loss: 0.031905\n",
      "Codebook loss: 0.198243\n",
      "Commit loss: 0.198243\n",
      "Epoch 3/100\n",
      "Validation loss: 0.151239\n",
      "Reconstruction loss: 0.028940\n",
      "Codebook loss: 0.111181\n",
      "Commit loss: 0.111181\n",
      "Epoch 4/100\n",
      "Validation loss: 0.107513\n",
      "Reconstruction loss: 0.027322\n",
      "Codebook loss: 0.072901\n",
      "Commit loss: 0.072901\n",
      "Epoch 5/100\n",
      "Validation loss: 0.103869\n",
      "Reconstruction loss: 0.026527\n",
      "Codebook loss: 0.070312\n",
      "Commit loss: 0.070312\n",
      "Epoch 6/100\n",
      "Validation loss: 0.098844\n",
      "Reconstruction loss: 0.026102\n",
      "Codebook loss: 0.066130\n",
      "Commit loss: 0.066130\n",
      "Epoch 7/100\n",
      "Validation loss: 0.096712\n",
      "Reconstruction loss: 0.025716\n",
      "Codebook loss: 0.064542\n",
      "Commit loss: 0.064542\n",
      "Epoch 8/100\n",
      "Validation loss: 0.096117\n",
      "Reconstruction loss: 0.025477\n",
      "Codebook loss: 0.064218\n",
      "Commit loss: 0.064218\n",
      "Epoch 9/100\n",
      "Validation loss: 0.095459\n",
      "Reconstruction loss: 0.025388\n",
      "Codebook loss: 0.063701\n",
      "Commit loss: 0.063701\n",
      "Epoch 10/100\n",
      "Validation loss: 0.092480\n",
      "Reconstruction loss: 0.025093\n",
      "Codebook loss: 0.061260\n",
      "Commit loss: 0.061260\n",
      "Epoch 11/100\n",
      "Validation loss: 0.093834\n",
      "Reconstruction loss: 0.025001\n",
      "Codebook loss: 0.062575\n",
      "Commit loss: 0.062575\n",
      "Epoch 12/100\n",
      "Validation loss: 0.092022\n",
      "Reconstruction loss: 0.024918\n",
      "Codebook loss: 0.061004\n",
      "Commit loss: 0.061004\n",
      "Epoch 13/100\n",
      "Validation loss: 0.090260\n",
      "Reconstruction loss: 0.024870\n",
      "Codebook loss: 0.059445\n",
      "Commit loss: 0.059445\n",
      "Epoch 14/100\n",
      "Validation loss: 0.091904\n",
      "Reconstruction loss: 0.024708\n",
      "Codebook loss: 0.061088\n",
      "Commit loss: 0.061088\n",
      "Epoch 15/100\n",
      "Validation loss: 0.088735\n",
      "Reconstruction loss: 0.024582\n",
      "Codebook loss: 0.058321\n",
      "Commit loss: 0.058321\n",
      "Epoch 16/100\n",
      "Validation loss: 0.088940\n",
      "Reconstruction loss: 0.024557\n",
      "Codebook loss: 0.058531\n",
      "Commit loss: 0.058531\n",
      "Epoch 17/100\n",
      "Validation loss: 0.089143\n",
      "Reconstruction loss: 0.024539\n",
      "Codebook loss: 0.058731\n",
      "Commit loss: 0.058731\n",
      "Epoch 18/100\n",
      "Validation loss: 0.089052\n",
      "Reconstruction loss: 0.024545\n",
      "Codebook loss: 0.058642\n",
      "Commit loss: 0.058642\n",
      "Epoch 19/100\n",
      "Validation loss: 0.086682\n",
      "Reconstruction loss: 0.024371\n",
      "Codebook loss: 0.056646\n",
      "Commit loss: 0.056646\n",
      "Epoch 20/100\n",
      "Validation loss: 0.084745\n",
      "Reconstruction loss: 0.024360\n",
      "Codebook loss: 0.054896\n",
      "Commit loss: 0.054896\n",
      "Epoch 21/100\n",
      "Validation loss: 0.089061\n",
      "Reconstruction loss: 0.024298\n",
      "Codebook loss: 0.058875\n",
      "Commit loss: 0.058875\n",
      "Epoch 22/100\n",
      "Validation loss: 0.088871\n",
      "Reconstruction loss: 0.024206\n",
      "Codebook loss: 0.058786\n",
      "Commit loss: 0.058786\n",
      "Epoch 23/100\n",
      "Validation loss: 0.089293\n",
      "Reconstruction loss: 0.024246\n",
      "Codebook loss: 0.059134\n",
      "Commit loss: 0.059134\n",
      "Epoch 24/100\n",
      "Validation loss: 0.089532\n",
      "Reconstruction loss: 0.024143\n",
      "Codebook loss: 0.059444\n",
      "Commit loss: 0.059444\n",
      "Epoch 25/100\n",
      "Validation loss: 0.085419\n",
      "Reconstruction loss: 0.024139\n",
      "Codebook loss: 0.055709\n",
      "Commit loss: 0.055709\n",
      "Epoch 26/100\n",
      "Validation loss: 0.082514\n",
      "Reconstruction loss: 0.024121\n",
      "Codebook loss: 0.053085\n",
      "Commit loss: 0.053085\n",
      "Epoch 27/100\n",
      "Validation loss: 0.087666\n",
      "Reconstruction loss: 0.024189\n",
      "Codebook loss: 0.057707\n",
      "Commit loss: 0.057707\n",
      "Epoch 28/100\n",
      "Validation loss: 0.086519\n",
      "Reconstruction loss: 0.024152\n",
      "Codebook loss: 0.056698\n",
      "Commit loss: 0.056698\n",
      "Epoch 29/100\n",
      "Validation loss: 0.084783\n",
      "Reconstruction loss: 0.024294\n",
      "Codebook loss: 0.054990\n",
      "Commit loss: 0.054990\n",
      "Epoch 30/100\n",
      "Validation loss: 0.082751\n",
      "Reconstruction loss: 0.024029\n",
      "Codebook loss: 0.053384\n",
      "Commit loss: 0.053384\n",
      "Epoch 31/100\n",
      "Validation loss: 0.082434\n",
      "Reconstruction loss: 0.023986\n",
      "Codebook loss: 0.053134\n",
      "Commit loss: 0.053134\n",
      "Epoch 32/100\n",
      "Validation loss: 0.084704\n",
      "Reconstruction loss: 0.023945\n",
      "Codebook loss: 0.055235\n",
      "Commit loss: 0.055235\n",
      "Epoch 33/100\n",
      "Validation loss: 0.085022\n",
      "Reconstruction loss: 0.024038\n",
      "Codebook loss: 0.055441\n",
      "Commit loss: 0.055441\n",
      "Epoch 34/100\n",
      "Validation loss: 0.085197\n",
      "Reconstruction loss: 0.023940\n",
      "Codebook loss: 0.055688\n",
      "Commit loss: 0.055688\n",
      "Epoch 35/100\n",
      "Validation loss: 0.083333\n",
      "Reconstruction loss: 0.023971\n",
      "Codebook loss: 0.053966\n",
      "Commit loss: 0.053966\n",
      "Epoch 36/100\n",
      "Validation loss: 0.081745\n",
      "Reconstruction loss: 0.023895\n",
      "Codebook loss: 0.052591\n",
      "Commit loss: 0.052591\n",
      "Epoch 37/100\n",
      "Validation loss: 0.086058\n",
      "Reconstruction loss: 0.023950\n",
      "Codebook loss: 0.056462\n",
      "Commit loss: 0.056462\n",
      "Epoch 38/100\n",
      "Validation loss: 0.083848\n",
      "Reconstruction loss: 0.023938\n",
      "Codebook loss: 0.054464\n",
      "Commit loss: 0.054464\n",
      "Epoch 39/100\n",
      "Validation loss: 0.083808\n",
      "Reconstruction loss: 0.023887\n",
      "Codebook loss: 0.054474\n",
      "Commit loss: 0.054474\n",
      "Epoch 40/100\n",
      "Validation loss: 0.086382\n",
      "Reconstruction loss: 0.023905\n",
      "Codebook loss: 0.056797\n",
      "Commit loss: 0.056797\n",
      "Epoch 41/100\n",
      "Validation loss: 0.085325\n",
      "Reconstruction loss: 0.023893\n",
      "Codebook loss: 0.055848\n",
      "Commit loss: 0.055848\n",
      "Epoch 42/100\n",
      "Validation loss: 0.107404\n",
      "Reconstruction loss: 0.023950\n",
      "Codebook loss: 0.075868\n",
      "Commit loss: 0.075868\n",
      "Epoch 43/100\n",
      "Validation loss: 0.084712\n",
      "Reconstruction loss: 0.023861\n",
      "Codebook loss: 0.055319\n",
      "Commit loss: 0.055319\n",
      "Epoch 44/100\n",
      "Validation loss: 0.085022\n",
      "Reconstruction loss: 0.023881\n",
      "Codebook loss: 0.055583\n",
      "Commit loss: 0.055583\n",
      "Epoch 45/100\n",
      "Validation loss: 0.087861\n",
      "Reconstruction loss: 0.023884\n",
      "Codebook loss: 0.058161\n",
      "Commit loss: 0.058161\n",
      "Epoch 46/100\n",
      "Validation loss: 0.084441\n",
      "Reconstruction loss: 0.023850\n",
      "Codebook loss: 0.055084\n",
      "Commit loss: 0.055084\n",
      "Epoch 47/100\n",
      "Validation loss: 0.084204\n",
      "Reconstruction loss: 0.023846\n",
      "Codebook loss: 0.054871\n",
      "Commit loss: 0.054871\n",
      "Epoch 48/100\n",
      "Validation loss: 0.082844\n",
      "Reconstruction loss: 0.023836\n",
      "Codebook loss: 0.053644\n",
      "Commit loss: 0.053644\n",
      "Epoch 49/100\n",
      "Validation loss: 0.084340\n",
      "Reconstruction loss: 0.023804\n",
      "Codebook loss: 0.055033\n",
      "Commit loss: 0.055033\n",
      "Epoch 50/100\n",
      "Validation loss: 0.087732\n",
      "Reconstruction loss: 0.023906\n",
      "Codebook loss: 0.058024\n",
      "Commit loss: 0.058024\n",
      "Epoch 51/100\n",
      "Validation loss: 0.083674\n",
      "Reconstruction loss: 0.023866\n",
      "Codebook loss: 0.054371\n",
      "Commit loss: 0.054371\n",
      "Epoch 52/100\n",
      "Validation loss: 0.084005\n",
      "Reconstruction loss: 0.023786\n",
      "Codebook loss: 0.054744\n",
      "Commit loss: 0.054744\n",
      "Epoch 53/100\n",
      "Validation loss: 0.085322\n",
      "Reconstruction loss: 0.023808\n",
      "Codebook loss: 0.055921\n",
      "Commit loss: 0.055921\n",
      "Epoch 54/100\n",
      "Validation loss: 0.082697\n",
      "Reconstruction loss: 0.023809\n",
      "Codebook loss: 0.053535\n",
      "Commit loss: 0.053535\n",
      "Epoch 55/100\n",
      "Validation loss: 0.083448\n",
      "Reconstruction loss: 0.023834\n",
      "Codebook loss: 0.054195\n",
      "Commit loss: 0.054195\n",
      "Epoch 56/100\n",
      "Validation loss: 0.085294\n",
      "Reconstruction loss: 0.023812\n",
      "Codebook loss: 0.055892\n",
      "Commit loss: 0.055892\n",
      "Epoch 57/100\n",
      "Validation loss: 0.086274\n",
      "Reconstruction loss: 0.023854\n",
      "Codebook loss: 0.056745\n",
      "Commit loss: 0.056745\n",
      "Epoch 58/100\n",
      "Validation loss: 0.080322\n",
      "Reconstruction loss: 0.023794\n",
      "Codebook loss: 0.051390\n",
      "Commit loss: 0.051390\n",
      "Epoch 59/100\n",
      "Validation loss: 0.080974\n",
      "Reconstruction loss: 0.023824\n",
      "Codebook loss: 0.051954\n",
      "Commit loss: 0.051954\n",
      "Epoch 60/100\n",
      "Validation loss: 0.081806\n",
      "Reconstruction loss: 0.023790\n",
      "Codebook loss: 0.052742\n",
      "Commit loss: 0.052742\n",
      "Epoch 61/100\n",
      "Validation loss: 0.083124\n",
      "Reconstruction loss: 0.023847\n",
      "Codebook loss: 0.053888\n",
      "Commit loss: 0.053888\n",
      "Epoch 62/100\n",
      "Validation loss: 0.087826\n",
      "Reconstruction loss: 0.023831\n",
      "Codebook loss: 0.058178\n",
      "Commit loss: 0.058178\n",
      "Epoch 63/100\n",
      "Validation loss: 0.082312\n",
      "Reconstruction loss: 0.023836\n",
      "Codebook loss: 0.053160\n",
      "Commit loss: 0.053160\n",
      "Epoch 64/100\n",
      "Validation loss: 0.083815\n",
      "Reconstruction loss: 0.023832\n",
      "Codebook loss: 0.054529\n",
      "Commit loss: 0.054529\n",
      "Epoch 65/100\n",
      "Validation loss: 0.081006\n",
      "Reconstruction loss: 0.023820\n",
      "Codebook loss: 0.051987\n",
      "Commit loss: 0.051987\n",
      "Epoch 66/100\n",
      "Validation loss: 0.083642\n",
      "Reconstruction loss: 0.023823\n",
      "Codebook loss: 0.054381\n",
      "Commit loss: 0.054381\n",
      "Epoch 67/100\n",
      "Validation loss: 0.084358\n",
      "Reconstruction loss: 0.023831\n",
      "Codebook loss: 0.055024\n",
      "Commit loss: 0.055024\n",
      "Epoch 68/100\n",
      "Validation loss: 0.083653\n",
      "Reconstruction loss: 0.023805\n",
      "Codebook loss: 0.054407\n",
      "Commit loss: 0.054407\n",
      "Epoch 69/100\n",
      "Validation loss: 0.081351\n",
      "Reconstruction loss: 0.023820\n",
      "Codebook loss: 0.052301\n",
      "Commit loss: 0.052301\n",
      "Epoch 70/100\n",
      "Validation loss: 0.083033\n",
      "Reconstruction loss: 0.023824\n",
      "Codebook loss: 0.053827\n",
      "Commit loss: 0.053827\n",
      "Epoch 71/100\n",
      "Validation loss: 0.081733\n",
      "Reconstruction loss: 0.023802\n",
      "Codebook loss: 0.052664\n",
      "Commit loss: 0.052664\n",
      "Epoch 72/100\n",
      "Validation loss: 0.080858\n",
      "Reconstruction loss: 0.023810\n",
      "Codebook loss: 0.051861\n",
      "Commit loss: 0.051861\n",
      "Epoch 73/100\n",
      "Validation loss: 0.081975\n",
      "Reconstruction loss: 0.023791\n",
      "Codebook loss: 0.052894\n",
      "Commit loss: 0.052894\n",
      "Epoch 74/100\n",
      "Validation loss: 0.087155\n",
      "Reconstruction loss: 0.023857\n",
      "Codebook loss: 0.057543\n",
      "Commit loss: 0.057543\n",
      "Epoch 75/100\n",
      "Validation loss: 0.085491\n",
      "Reconstruction loss: 0.023807\n",
      "Codebook loss: 0.056076\n",
      "Commit loss: 0.056076\n",
      "Epoch 76/100\n",
      "Validation loss: 0.089751\n",
      "Reconstruction loss: 0.023794\n",
      "Codebook loss: 0.059961\n",
      "Commit loss: 0.059961\n",
      "Epoch 77/100\n",
      "Validation loss: 0.082336\n",
      "Reconstruction loss: 0.023799\n",
      "Codebook loss: 0.053216\n",
      "Commit loss: 0.053216\n",
      "Epoch 78/100\n",
      "Validation loss: 0.082436\n",
      "Reconstruction loss: 0.023798\n",
      "Codebook loss: 0.053308\n",
      "Commit loss: 0.053308\n",
      "Epoch 79/100\n",
      "Validation loss: 0.081035\n",
      "Reconstruction loss: 0.023773\n",
      "Codebook loss: 0.052056\n",
      "Commit loss: 0.052056\n",
      "Epoch 80/100\n",
      "Validation loss: 0.080111\n",
      "Reconstruction loss: 0.023821\n",
      "Codebook loss: 0.051172\n",
      "Commit loss: 0.051172\n",
      "Epoch 81/100\n",
      "Validation loss: 0.085445\n",
      "Reconstruction loss: 0.023863\n",
      "Codebook loss: 0.055983\n",
      "Commit loss: 0.055983\n",
      "Epoch 82/100\n",
      "Validation loss: 0.084160\n",
      "Reconstruction loss: 0.023785\n",
      "Codebook loss: 0.054887\n",
      "Commit loss: 0.054887\n",
      "Epoch 83/100\n",
      "Validation loss: 0.087891\n",
      "Reconstruction loss: 0.023853\n",
      "Codebook loss: 0.058217\n",
      "Commit loss: 0.058217\n",
      "Epoch 84/100\n",
      "Validation loss: 0.081749\n",
      "Reconstruction loss: 0.023814\n",
      "Codebook loss: 0.052668\n",
      "Commit loss: 0.052668\n",
      "Epoch 85/100\n",
      "Validation loss: 0.191426\n",
      "Reconstruction loss: 0.023905\n",
      "Codebook loss: 0.152292\n",
      "Commit loss: 0.152292\n",
      "Epoch 86/100\n",
      "Validation loss: 0.081874\n",
      "Reconstruction loss: 0.023820\n",
      "Codebook loss: 0.052776\n",
      "Commit loss: 0.052776\n",
      "Epoch 87/100\n",
      "Validation loss: 0.083400\n",
      "Reconstruction loss: 0.023842\n",
      "Codebook loss: 0.054143\n",
      "Commit loss: 0.054143\n",
      "Epoch 88/100\n",
      "Validation loss: 0.081373\n",
      "Reconstruction loss: 0.023825\n",
      "Codebook loss: 0.052316\n",
      "Commit loss: 0.052316\n",
      "Epoch 89/100\n",
      "Validation loss: 0.094995\n",
      "Reconstruction loss: 0.023952\n",
      "Codebook loss: 0.064584\n",
      "Commit loss: 0.064584\n",
      "Epoch 90/100\n",
      "Validation loss: 0.084900\n",
      "Reconstruction loss: 0.024000\n",
      "Codebook loss: 0.055364\n",
      "Commit loss: 0.055364\n",
      "Epoch 91/100\n",
      "Validation loss: 0.083170\n",
      "Reconstruction loss: 0.023842\n",
      "Codebook loss: 0.053935\n",
      "Commit loss: 0.053935\n",
      "Epoch 92/100\n",
      "Validation loss: 0.080634\n",
      "Reconstruction loss: 0.023882\n",
      "Codebook loss: 0.051593\n",
      "Commit loss: 0.051593\n",
      "Epoch 93/100\n",
      "Validation loss: 0.082247\n",
      "Reconstruction loss: 0.023886\n",
      "Codebook loss: 0.053055\n",
      "Commit loss: 0.053055\n",
      "Epoch 94/100\n",
      "Validation loss: 0.080555\n",
      "Reconstruction loss: 0.023956\n",
      "Codebook loss: 0.051453\n",
      "Commit loss: 0.051453\n",
      "Epoch 95/100\n",
      "Validation loss: 0.081465\n",
      "Reconstruction loss: 0.023877\n",
      "Codebook loss: 0.052352\n",
      "Commit loss: 0.052352\n",
      "Epoch 96/100\n",
      "Validation loss: 0.079213\n",
      "Reconstruction loss: 0.023884\n",
      "Codebook loss: 0.050299\n",
      "Commit loss: 0.050299\n",
      "Epoch 97/100\n",
      "Validation loss: 0.078750\n",
      "Reconstruction loss: 0.023867\n",
      "Codebook loss: 0.049893\n",
      "Commit loss: 0.049893\n",
      "Epoch 98/100\n",
      "Validation loss: 0.080832\n",
      "Reconstruction loss: 0.023868\n",
      "Codebook loss: 0.051785\n",
      "Commit loss: 0.051785\n",
      "Epoch 99/100\n",
      "Validation loss: 0.080793\n",
      "Reconstruction loss: 0.023846\n",
      "Codebook loss: 0.051770\n",
      "Commit loss: 0.051770\n",
      "Epoch 100/100\n",
      "Validation loss: 0.079912\n",
      "Reconstruction loss: 0.023904\n",
      "Codebook loss: 0.050916\n",
      "Commit loss: 0.050916\n"
     ]
    }
   ],
   "source": [
    "model = VQVAE(\n",
    "    input_dim=(3, 32, 32),\n",
    "    n_channels=3,\n",
    "    latent_dim=128,\n",
    "    codebook_size=256,\n",
    ")\n",
    "\n",
    "recon_loss_fn = nn.MSELoss()\n",
    "cb_loss_fn = nn.MSELoss()\n",
    "commit_loss_fn = nn.MSELoss()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-5)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "fit(100, model, recon_loss_fn, cb_loss_fn, commit_loss_fn, opt, train_dl, valid_dl, beta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_torch_image(img):\n",
    "    img = img.permute(1, 2, 0)\n",
    "    img = img.numpy().astype(np.uint8)\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "sample_output, codebook, z_e, z_quantized = model(x_train[0].unsqueeze(0))\n",
    "print(sample_output.shape)\n",
    "show_torch_image(255*sample_output.squeeze(0).detach().cpu())\n",
    "show_torch_image(255 * x_train[0].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
