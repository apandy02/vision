{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Quantized Variational Autoencoders (VQ-VAEs)\n",
    "\n",
    "Rather than computing a continuous latent representation of the input, in VQ-VAEs we compute a discrete latent representation. Here, our encoder output is discretized with respect to a learned discrete set of embeddings we refer to as a codebook.\n",
    "\n",
    "The goal here is to tackle the smoothing problem noticed in general VAEs, as well as to reduce computational complexity. We can then use the (encoder + codebook) as a tokenizer for different types of transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from vision.resnet import ResBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_channels: int,\n",
    "        latent_dim: int,\n",
    "        codebook_size: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(n_channels, latent_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(latent_dim),\n",
    "            nn.Conv2d(latent_dim, latent_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(latent_dim),\n",
    "            nn.Conv2d(latent_dim, latent_dim, kernel_size=3, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(latent_dim),\n",
    "            nn.Conv2d(latent_dim, latent_dim, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(latent_dim)\n",
    "        )\n",
    "\n",
    "        self.codebook = nn.Parameter(\n",
    "            torch.randn(codebook_size, latent_dim) * 0.02, \n",
    "            requires_grad=True\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, latent_dim, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(latent_dim),\n",
    "            nn.ConvTranspose2d(latent_dim, latent_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(latent_dim),\n",
    "            nn.ConvTranspose2d(latent_dim, latent_dim, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(latent_dim),\n",
    "            nn.ConvTranspose2d(latent_dim, n_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the VAE.\n",
    "\n",
    "        Args:\n",
    "            x: input data\n",
    "\n",
    "        Returns:\n",
    "            x_hat: reconstructed data\n",
    "        \"\"\"\n",
    "        z_encoder = self.encoder(x)\n",
    "        z_quantized = self._quantize_encoder_output(z_encoder)\n",
    "        z_q_st = z_encoder + (z_quantized - z_encoder).detach()\n",
    "        return self.decoder(z_q_st), z_encoder, z_quantized\n",
    "    \n",
    "    def _quantize_encoder_output(self, z_e: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Quantizing the encoded tensor by snapping its elements to the closest codebook \n",
    "        entry.\n",
    "\n",
    "        Args:\n",
    "            z_e: encoded representation\n",
    "\n",
    "        Returns:\n",
    "            z_q: quantized representation\n",
    "        \"\"\"\n",
    "        batch_size, latent_dim, h, w = z_e.shape\n",
    "        encoded = z_e.permute(0, 2, 3, 1).reshape(batch_size*h*w, latent_dim)\n",
    "        quantized = self.codebook[torch.argmin(torch.cdist(encoded, self.codebook), dim=1)]\n",
    "        z_q = quantized.reshape(batch_size, h, w, latent_dim).permute(0, 3, 1, 2)\n",
    "        return z_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 3, 64, 64])\n",
      "Running forward pass...\n",
      "Output shape: torch.Size([2, 3, 64, 64])\n",
      "z_e.shape: torch.Size([2, 128, 8, 8]), z_quantized.shape: torch.Size([2, 128, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# Create dummy tensors and model for testing\n",
    "batch_size = 2\n",
    "n_channels = 3\n",
    "height = 64\n",
    "width = 64\n",
    "latent_dim = 128\n",
    "codebook_size = 256\n",
    "\n",
    "dummy_input = torch.randn(batch_size, n_channels, height, width)\n",
    "\n",
    "dummy_model = VQVAE(\n",
    "    latent_dim=latent_dim,\n",
    "    n_channels=n_channels,\n",
    "    codebook_size=codebook_size\n",
    ")\n",
    "\n",
    "print(\"Input shape:\", dummy_input.shape)\n",
    "print(\"Running forward pass...\")\n",
    "x_hat, z_e, z_quantized = dummy_model(dummy_input)\n",
    "print(f\"Output shape: {x_hat.shape}\")\n",
    "print(f\"z_e.shape: {z_e.shape}, z_quantized.shape: {z_quantized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "\n",
    "Our objective function consists of 3 terms:\n",
    "\n",
    "![VQ-VAE Loss Function](images/vq_vae_loss.png)\n",
    "\n",
    "1. Reconstruction Loss: Ensures the decoded output matches the input\n",
    "2. Codebook Loss: Keeps the codebook entries close to the encoded representations  \n",
    "3. Commitment Loss: Prevents the encoder from growing too large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    model: nn.Module,\n",
    "    recon_loss_fn: nn.Module,\n",
    "    cb_loss_fn: nn.Module,\n",
    "    commit_loss_fn: nn.Module,\n",
    "    valid_dl: DataLoader,\n",
    "    beta: float = 1.\n",
    "):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tot_loss = 0.\n",
    "        tot_recon_loss = 0.\n",
    "        tot_codebook_loss = 0.\n",
    "        tot_commit_loss = 0.\n",
    "        num_batches = 0\n",
    "        for xb, _ in valid_dl:\n",
    "            x_hat, z_e, z_quantized = model(xb)\n",
    "            recon_loss = recon_loss_fn(x_hat, xb)\n",
    "            codebook_loss = cb_loss_fn(z_quantized, z_e.detach())\n",
    "            commit_loss = commit_loss_fn(z_e, z_quantized.detach())\n",
    "            loss = recon_loss + codebook_loss + (beta * commit_loss)\n",
    "            \n",
    "            tot_loss += loss.item()\n",
    "            tot_recon_loss += recon_loss.item()\n",
    "            tot_codebook_loss += codebook_loss.item()\n",
    "            tot_commit_loss += commit_loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    tot_loss /= num_batches\n",
    "    tot_recon_loss /= num_batches\n",
    "    tot_codebook_loss /= num_batches\n",
    "    tot_commit_loss /= num_batches\n",
    "\n",
    "    return tot_loss, tot_recon_loss, tot_codebook_loss, tot_commit_loss\n",
    "\n",
    "def kl_loss_func(mu, logvar):\n",
    "    # Clamp logvar for numerical stability\n",
    "    logvar = torch.clamp(logvar, -10, 10)\n",
    "    \n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return kl_loss / (mu.size(0) * mu.size(1)) # normalize to prevent explosion\n",
    "\n",
    "def fit(\n",
    "    epochs: int, \n",
    "    model: nn.Module,\n",
    "    recon_loss_fn: nn.Module,\n",
    "    cb_loss_fn: nn.Module,\n",
    "    commit_loss_fn: nn.Module,\n",
    "    opt: torch.optim.Optimizer, \n",
    "    scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "    train_dl: DataLoader, \n",
    "    valid_dl: DataLoader,\n",
    "    beta: float = 1,\n",
    "    grad_clip: float = 1.0\n",
    "):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        model.train()\n",
    "        for xb, _ in train_dl:\n",
    "            xb = xb.to(device)\n",
    "            x_hat, z_e, z_quantized = model(xb)\n",
    "            recon_loss = recon_loss_fn(x_hat, xb)\n",
    "            codebook_loss = cb_loss_fn(z_quantized, z_e.detach())\n",
    "            commit_loss = commit_loss_fn(z_e, z_quantized.detach())\n",
    "            loss = recon_loss + codebook_loss + (beta * commit_loss)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss, recon_loss, codebook_loss, commit_loss = validate(\n",
    "            model, recon_loss_fn, cb_loss_fn, commit_loss_fn, valid_dl, beta\n",
    "        )\n",
    "        print(f\"Validation loss: {total_loss:.6f}\")\n",
    "        print(f\"Reconstruction loss: {recon_loss:.6f}\")\n",
    "        print(f\"Codebook loss: {codebook_loss:.6f}\")\n",
    "        print(f\"Commit loss: {commit_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "all_batches_data = []\n",
    "all_batches_labels = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    with open(f'data/cifar-10-batches-py/data_batch_{i}', 'rb') as f:\n",
    "        dataset_dict = pickle.load(f, encoding='bytes')\n",
    "        all_batches_data.append(dataset_dict[b'data'])\n",
    "        all_batches_labels.append(dataset_dict[b'labels'])\n",
    "\n",
    "stacked_data = np.vstack(all_batches_data)\n",
    "stacked_labels = np.hstack(all_batches_labels)\n",
    "data = torch.tensor(stacked_data, dtype=torch.float32).view(-1, 3, 32, 32).to(device) / 255.\n",
    "labels = torch.tensor(stacked_labels, dtype=torch.long).to(device)\n",
    "\n",
    "split_idx = int(0.8 * len(data))\n",
    "\n",
    "x_train, x_valid = data[:split_idx], data[split_idx:]\n",
    "y_train, y_valid = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "class CIFARCustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "train_ds = CIFARCustomDataset(x_train, y_train)\n",
    "valid_ds = CIFARCustomDataset(x_valid, y_valid)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Validation loss: 1.622861\n",
      "Reconstruction loss: 0.018989\n",
      "Codebook loss: 1.069248\n",
      "Commit loss: 1.069248\n",
      "Epoch 2/50\n",
      "Validation loss: 1.496447\n",
      "Reconstruction loss: 0.018061\n",
      "Codebook loss: 0.985591\n",
      "Commit loss: 0.985591\n",
      "Epoch 3/50\n",
      "Validation loss: 1.521920\n",
      "Reconstruction loss: 0.017719\n",
      "Codebook loss: 1.002801\n",
      "Commit loss: 1.002801\n",
      "Epoch 4/50\n",
      "Validation loss: 1.338807\n",
      "Reconstruction loss: 0.017804\n",
      "Codebook loss: 0.880669\n",
      "Commit loss: 0.880669\n",
      "Epoch 5/50\n",
      "Validation loss: 1.166935\n",
      "Reconstruction loss: 0.017617\n",
      "Codebook loss: 0.766212\n",
      "Commit loss: 0.766212\n",
      "Epoch 6/50\n",
      "Validation loss: 1.069815\n",
      "Reconstruction loss: 0.017272\n",
      "Codebook loss: 0.701695\n",
      "Commit loss: 0.701695\n",
      "Epoch 7/50\n",
      "Validation loss: 0.965243\n",
      "Reconstruction loss: 0.017338\n",
      "Codebook loss: 0.631936\n",
      "Commit loss: 0.631936\n",
      "Epoch 8/50\n",
      "Validation loss: 0.788884\n",
      "Reconstruction loss: 0.017377\n",
      "Codebook loss: 0.514338\n",
      "Commit loss: 0.514338\n",
      "Epoch 9/50\n",
      "Validation loss: 0.663502\n",
      "Reconstruction loss: 0.017306\n",
      "Codebook loss: 0.430798\n",
      "Commit loss: 0.430798\n",
      "Epoch 10/50\n",
      "Validation loss: 0.597180\n",
      "Reconstruction loss: 0.017016\n",
      "Codebook loss: 0.386776\n",
      "Commit loss: 0.386776\n",
      "Epoch 11/50\n",
      "Validation loss: 0.482171\n",
      "Reconstruction loss: 0.017011\n",
      "Codebook loss: 0.310106\n",
      "Commit loss: 0.310106\n",
      "Epoch 12/50\n",
      "Validation loss: 0.400306\n",
      "Reconstruction loss: 0.017123\n",
      "Codebook loss: 0.255455\n",
      "Commit loss: 0.255455\n",
      "Epoch 13/50\n",
      "Validation loss: 0.329351\n",
      "Reconstruction loss: 0.017018\n",
      "Codebook loss: 0.208222\n",
      "Commit loss: 0.208222\n",
      "Epoch 14/50\n",
      "Validation loss: 0.256639\n",
      "Reconstruction loss: 0.016824\n",
      "Codebook loss: 0.159877\n",
      "Commit loss: 0.159877\n",
      "Epoch 15/50\n",
      "Validation loss: 0.225825\n",
      "Reconstruction loss: 0.016799\n",
      "Codebook loss: 0.139350\n",
      "Commit loss: 0.139350\n",
      "Epoch 16/50\n",
      "Validation loss: 0.191175\n",
      "Reconstruction loss: 0.016804\n",
      "Codebook loss: 0.116248\n",
      "Commit loss: 0.116248\n",
      "Epoch 17/50\n",
      "Validation loss: 0.153641\n",
      "Reconstruction loss: 0.016716\n",
      "Codebook loss: 0.091283\n",
      "Commit loss: 0.091283\n",
      "Epoch 18/50\n",
      "Validation loss: 0.146654\n",
      "Reconstruction loss: 0.016531\n",
      "Codebook loss: 0.086749\n",
      "Commit loss: 0.086749\n",
      "Epoch 19/50\n",
      "Validation loss: 0.126768\n",
      "Reconstruction loss: 0.016555\n",
      "Codebook loss: 0.073475\n",
      "Commit loss: 0.073475\n",
      "Epoch 20/50\n",
      "Validation loss: 0.130055\n",
      "Reconstruction loss: 0.016639\n",
      "Codebook loss: 0.075611\n",
      "Commit loss: 0.075611\n",
      "Epoch 21/50\n",
      "Validation loss: 0.100408\n",
      "Reconstruction loss: 0.016614\n",
      "Codebook loss: 0.055863\n",
      "Commit loss: 0.055863\n",
      "Epoch 22/50\n",
      "Validation loss: 0.102699\n",
      "Reconstruction loss: 0.016591\n",
      "Codebook loss: 0.057405\n",
      "Commit loss: 0.057405\n",
      "Epoch 23/50\n",
      "Validation loss: 0.097436\n",
      "Reconstruction loss: 0.016531\n",
      "Codebook loss: 0.053937\n",
      "Commit loss: 0.053937\n",
      "Epoch 24/50\n",
      "Validation loss: 0.091439\n",
      "Reconstruction loss: 0.016739\n",
      "Codebook loss: 0.049800\n",
      "Commit loss: 0.049800\n",
      "Epoch 25/50\n",
      "Validation loss: 0.083602\n",
      "Reconstruction loss: 0.016596\n",
      "Codebook loss: 0.044671\n",
      "Commit loss: 0.044671\n",
      "Epoch 26/50\n",
      "Validation loss: 0.081029\n",
      "Reconstruction loss: 0.016442\n",
      "Codebook loss: 0.043058\n",
      "Commit loss: 0.043058\n",
      "Epoch 27/50\n",
      "Validation loss: 0.072573\n",
      "Reconstruction loss: 0.016430\n",
      "Codebook loss: 0.037428\n",
      "Commit loss: 0.037428\n",
      "Epoch 28/50\n",
      "Validation loss: 0.079251\n",
      "Reconstruction loss: 0.016417\n",
      "Codebook loss: 0.041889\n",
      "Commit loss: 0.041889\n",
      "Epoch 29/50\n",
      "Validation loss: 0.067751\n",
      "Reconstruction loss: 0.016426\n",
      "Codebook loss: 0.034217\n",
      "Commit loss: 0.034217\n",
      "Epoch 30/50\n",
      "Validation loss: 0.068497\n",
      "Reconstruction loss: 0.016415\n",
      "Codebook loss: 0.034721\n",
      "Commit loss: 0.034721\n",
      "Epoch 31/50\n",
      "Validation loss: 0.066210\n",
      "Reconstruction loss: 0.016410\n",
      "Codebook loss: 0.033200\n",
      "Commit loss: 0.033200\n",
      "Epoch 32/50\n",
      "Validation loss: 0.066673\n",
      "Reconstruction loss: 0.016475\n",
      "Codebook loss: 0.033465\n",
      "Commit loss: 0.033465\n",
      "Epoch 33/50\n",
      "Validation loss: 0.065092\n",
      "Reconstruction loss: 0.016504\n",
      "Codebook loss: 0.032392\n",
      "Commit loss: 0.032392\n",
      "Epoch 34/50\n",
      "Validation loss: 0.063616\n",
      "Reconstruction loss: 0.016510\n",
      "Codebook loss: 0.031404\n",
      "Commit loss: 0.031404\n",
      "Epoch 35/50\n",
      "Validation loss: 0.062582\n",
      "Reconstruction loss: 0.016443\n",
      "Codebook loss: 0.030759\n",
      "Commit loss: 0.030759\n",
      "Epoch 36/50\n",
      "Validation loss: 0.059661\n",
      "Reconstruction loss: 0.016674\n",
      "Codebook loss: 0.028658\n",
      "Commit loss: 0.028658\n",
      "Epoch 37/50\n",
      "Validation loss: 0.057782\n",
      "Reconstruction loss: 0.016633\n",
      "Codebook loss: 0.027433\n",
      "Commit loss: 0.027433\n",
      "Epoch 38/50\n",
      "Validation loss: 0.059641\n",
      "Reconstruction loss: 0.016639\n",
      "Codebook loss: 0.028668\n",
      "Commit loss: 0.028668\n",
      "Epoch 39/50\n",
      "Validation loss: 0.059635\n",
      "Reconstruction loss: 0.016692\n",
      "Codebook loss: 0.028628\n",
      "Commit loss: 0.028628\n",
      "Epoch 40/50\n",
      "Validation loss: 0.061379\n",
      "Reconstruction loss: 0.016789\n",
      "Codebook loss: 0.029727\n",
      "Commit loss: 0.029727\n",
      "Epoch 41/50\n",
      "Validation loss: 0.057029\n",
      "Reconstruction loss: 0.016536\n",
      "Codebook loss: 0.026996\n",
      "Commit loss: 0.026996\n",
      "Epoch 42/50\n",
      "Validation loss: 0.055193\n",
      "Reconstruction loss: 0.016476\n",
      "Codebook loss: 0.025811\n",
      "Commit loss: 0.025811\n",
      "Epoch 43/50\n",
      "Validation loss: 0.057500\n",
      "Reconstruction loss: 0.016316\n",
      "Codebook loss: 0.027456\n",
      "Commit loss: 0.027456\n",
      "Epoch 44/50\n",
      "Validation loss: 0.054178\n",
      "Reconstruction loss: 0.016357\n",
      "Codebook loss: 0.025214\n",
      "Commit loss: 0.025214\n",
      "Epoch 45/50\n",
      "Validation loss: 0.053650\n",
      "Reconstruction loss: 0.016423\n",
      "Codebook loss: 0.024818\n",
      "Commit loss: 0.024818\n",
      "Epoch 46/50\n",
      "Validation loss: 0.055231\n",
      "Reconstruction loss: 0.016357\n",
      "Codebook loss: 0.025916\n",
      "Commit loss: 0.025916\n",
      "Epoch 47/50\n",
      "Validation loss: 0.053963\n",
      "Reconstruction loss: 0.016258\n",
      "Codebook loss: 0.025136\n",
      "Commit loss: 0.025136\n",
      "Epoch 48/50\n",
      "Validation loss: 0.052003\n",
      "Reconstruction loss: 0.016419\n",
      "Codebook loss: 0.023723\n",
      "Commit loss: 0.023723\n",
      "Epoch 49/50\n",
      "Validation loss: 0.050129\n",
      "Reconstruction loss: 0.016327\n",
      "Codebook loss: 0.022535\n",
      "Commit loss: 0.022535\n",
      "Epoch 50/50\n",
      "Validation loss: 0.050873\n",
      "Reconstruction loss: 0.016180\n",
      "Codebook loss: 0.023129\n",
      "Commit loss: 0.023129\n"
     ]
    }
   ],
   "source": [
    "model = VQVAE(\n",
    "    n_channels=3,\n",
    "    latent_dim=256,\n",
    "    codebook_size=128,\n",
    ")\n",
    "\n",
    "recon_loss_fn = nn.MSELoss()\n",
    "cb_loss_fn = nn.MSELoss()\n",
    "commit_loss_fn = nn.MSELoss()\n",
    "# Reduce learning rate for codebook specifically\n",
    "opt = torch.optim.AdamW([\n",
    "    {'params': [p for n, p in model.named_parameters() if 'codebook' not in n], 'lr': 1e-4},\n",
    "    {'params': [model.codebook], 'lr': 1e-5}  # lower LR for codebook\n",
    "])\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=100)\n",
    "\n",
    "fit(50, model, recon_loss_fn, cb_loss_fn, commit_loss_fn, opt, scheduler, train_dl, valid_dl, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAVLklEQVR4nO3cS44kSZYd0KdqH4/IyF9VFkGQQO+FuyD3xg1wF1wOZw12ZUa4m5l+OMjC6wmBfpfIBAninLG4uJioqF3Tgd7lPM+zAKCq1v/bCwDg/x1CAYAmFABoQgGAJhQAaEIBgCYUAGhCAYB2nQ78L//5P0UT37bHeOwlmrlqDaLsngyuqtt1vCV1f7tFc98v80/6159/ieb++pzvd1XVc3uNx348srkfH/O5H68tmvs4jvnYaOaqj3V+7auc8f/t+D/xjD/CM/56JWf8Gc398ZzP/Xzt0dyvY/4+8RJe+//63/77vznGkwIATSgA0IQCAE0oANCEAgBNKADQhAIATSgA0IQCAE0oANCEAgBtXIKyHllHTQXDz3Pe9VFVtSzLfO7rfGxVlpKXS7juY76WZcvacj5V1lGzBHt+hv1EZ9D1sm/Z75JtmzcaLZVd+0qv5x502oRn/LLM92XNLn0ty3zdlyM7h9egi+dyZn1D92DdVVXLOj8r+yVrytqDs7In56SqzjPo9zr/+N/1nhQAaEIBgCYUAGhCAYAmFABoQgGAJhQAaEIBgCYUAGhCAYA2rrmoI3tN/zjmr3anNRfnOn/1fg3nPo7kdfds7mvQGPB2yeoFLku2lgpepV/v82NSle3h9ideny1rFwivZtUR3BOXNT3j87HZSalagk+6hruyXIO5w5+ky5rVliTjL+Hcl6BqJ537OIPx6aEd8KQAQBMKADShAEATCgA0oQBAEwoANKEAQBMKADShAEATCgA0oQBAG5faHGFHzbIHHUJhf8el5nOfe9Y7cgaFLEG9U1VVXT/PO4Qu16xvKJXUsWxb2AvzmF+fy3WL5q5ncH2OVzT1sWe/kfakJys8K+ttPnd4xOsW9PasYWfTNejtWYJ1VFVdwp+wyeVc0+6j4HsivDzR+OVP+FnvSQGAJhQAaEIBgCYUAGhCAYAmFABoQgGAJhQAaEIBgCYUAGjjLoU17KLYj/n4JWgLqKp6HvNqhCV8yXxbgwqNLduTLRh/hOu+zC9lVVUFbQR1PbO6iC24nl+/Zf0Pz9dzPja9PsGZraqq13ztyzU8h0GFxm3Jrv1+XsZjl3BLkps5aIqoqqrjyPYwaa5IqyjO4HOGbR61BDU+y/nH/673pABAEwoANKEAQBMKADShAEATCgA0oQBAEwoANKEAQBMKADShAEAbl6ZcwqKSIykeCftVznPeDbKfYWdT0q2zzjtkqqo+PeYdQh+Pj2jusFon+pxfH79Gc79/m6/98fHI5n7Ne6+2bT62qup5D38j7fP59/D3V9Iddr1HU9eyzdfyCPqdqqput/li1rAUaEnHB3u+LNn1WaPx4RdcMHfa7TbhSQGAJhQAaEIBgCYUAGhCAYAmFABoQgGAJhQAaEIBgCYUAGjjmovPt6zS4XKfv+5+btlr4K9znmXnns39DOo8lrAC4H8e82qJ5fZbNHdd5tUfVVWPpHLj1/m6q6r++dv7eOy3j/k6qqq2c77n257tyVbZ+DNYyy08h8slqKIIr/26BTUxwdiqqu1I7ok/t+YiqtqJu3bm+5JVYlStwdzHH99y4UkBgH8lFABoQgGAJhQAaEIBgCYUAGhCAYAmFABoQgGAJhQAaEIBgDbuPvr06S2a+FK38djXa4vmXp7z8XvQT1OVdb28b49o7sc57xDa5pemqqrOoFepqur5Md/Dvz+yz/l8zPf8sWXr3oOul7T3qi7ZGV+PYP4lO4fLOZ97SzalqrZjfu2P4D6uqtqDDqGg4uf38eHlrOD6H8m1rKoj2PNw6qol+YM/vvzIkwIATSgA0IQCAE0oANCEAgBNKADQhAIATSgA0IQCAE0oANDGXQr/9Ld/F038W9CM8HxmVQffvn6Mx/76bT7297nfx2PfH69o7i2o0Pj68Vs09/u3r9H4fZ/XLmzB2KqqM3iv/wi7C9Zl/lr/sma/eX68h7+RLvfx0DVsIziCte9nVomy75fx2I89m/vymI9/D9ZRVfUI6m2qql6v+dn6yL6C6mObz/18ZRf/dcz3JT3jE54UAGhCAYAmFABoQgGAJhQAaEIBgCYUAGhCAYAmFABoQgGAJhQAaOOiksuZlYPc1qDXJCyGeVzmc98qm/sWDH+FkboHcy/J/lXV9ZJ9zuWcz3+5ZP1EFQy/htf+Eoy/3j5Fc//th3mXUVXVD/egF2hJ+6Pm1+fzPTsr99t83V8+Z91Ht1twrsLvlHu4h8s67xrbL/Oxv4+fH/KkZ6yq6jznazlO3UcA/ImEAgBNKADQhAIATSgA0IQCAE0oANCEAgBNKADQhAIATSgA0MbFJmsYH9egnyit73gLql4e16xbZw06hC5hP9H9Nv+gx+UtmvsW9sgcx7y7Zb19jua+XOf78uWSdet8+TLvM/r8Oesy+vc/Z5/z7fw2Hntu0dR1VHA9z+yMX9f5nr+9Zdcn+aJIv1OWsCcrGZ90alVVXZY/b+4juZ5hLdmEJwUAmlAAoAkFAJpQAKAJBQCaUACgCQUAmlAAoAkFAJpQAKCN32Ffgte6q6re7vO8OY9o6rq+5mtZ5x+xqqouy3zdQZvD72u5zN9JP96ymotweC3B2r/cv4vm/vHHeV3EXz7form//PT9eOzfvsvW/fMPYdXB41/GY7ftFc39es5vimdYcZLcb+cRVksEFQ2/vrIb/xL+hN2D8WtacxF0dGQzZ+ODr6sxTwoANKEAQBMKADShAEATCgA0oQBAEwoANKEAQBMKADShAEATCgC0cTFQUPVRVVVH0Dl0ve3R3NfbfO77Neuc+RqMvazzLqOqqmtQlnT/PuvtuVZWxPTlu0/jsT99/2M09y9/+Xk89i8/zHuSqqp++jLfly+f79HcXz49ovHH+/ym2B/ZOXy83sdj359Zv9eWdA4FXUZVVc/X/F7+umedTUfYw5TUGaX9ROcy38OwNq6WCuY+//jf9Z4UAGhCAYAmFABoQgGAJhQAaEIBgCYUAGhCAYAmFABoQgGANq+5CN/Vvqzz2oW1tmjuRzD3a8/mXmpeXXFZskz9fH0bj7399Jdo7rc1q1H47ofbeOwvP/41mvtvf/lpPPbHe3auvvzw83js2y2rIXm7ZFUhx/IxHvsMz0pdg/Fn9jmTk7IfWQVNspb0O2VJxwe/eZfw+qzR+Oz6VDD3Ehd0/Ns8KQDQhAIATSgA0IQCAE0oANCEAgBNKADQhAIATSgA0IQCAE0oANDG3UcVdmzc1vn49yPrnNmej/HYdcl6R67LfC3LLduT+5dP47E///JLNPf363xPqqq+/3Hew/T9j99nc3+5j8d+d/0czf32FvTCBB1ZVVXL+R6NT07WZQlutao6j3ln13Gb73dV1X7M249ee3bGs6qkP7f76Ai+g+J+ovMYD816kqrWYO7jj68+8qQAwL8SCgA0oQBAEwoANKEAQBMKADShAEATCgA0oQBAEwoAtPG798Gb1/8YP3//en09o7kfQc1F7dnr6/s6H3+9zKsiqqp++PJlPPavv/wczf3jOq8uqKr67rt5NcKXT1mNwv02r5e4hVUhFVRXnEd6aMOqg6QCIqxoSH6v7Wf42y5YyyVc9x5UV8TfKeHlSe7948gmP4JrH05dFVXz/PE9F54UAGhCAYAmFABoQgGAJhQAaEIBgCYUAGhCAYAmFABoQgGAJhQAaOPuo/d93jlTVbVc5v0d73s0dW1BH8t2GX/Eqqq63+ZjP/807zKqqvrhl7+Ox/6H//hP2dzrv0Tjl6BIZg37b+63+W+NI+0EOrb52D0r1/n67Ws0/vXtfTz2CHuYtqAw5xVsSVXV9prv+euZFfe8gns5/U55PLMP+nrN1/6R1a/Vxzaf+xnsd1XV65jvy7L+8b/rPSkA0IQCAE0oANCEAgBNKADQhAIATSgA0IQCAE0oANCEAgBNKADQxsVA33+d97xUVdVrnjfrbx/R1HtSlvTISk22c95T8t2edZr8HAxfk46fqlq3RzS+lnkXz5LV39QyP1Z1OV7R3Mc2X/d5hqVaj2/ZWtZ5V9IZdIFVVV32+bk938L+qMf8bD0u2bk6r/PPeZ7Zd8qyZffy+TE/W8979h30eM7nfk8KoaqqguHXJeuPmvCkAEATCgA0oQBAEwoANKEAQBMKADShAEATCgA0oQBAEwoAtHEfwVtYu/D6Nn89/rJnr4Hf9vkr5p/njQu/2+evjX+5Za+Yr3Ubjz2PeZ1DVdW+zueuqqrnvNLhOLNrf77m9QVr2KGRtEUsS3auji37nJfr/Bo9j6yiISn/WJ9ZRcO313zd25HVXHxs8z0/12y/j2hXqvaaH5Z1zapCbvf5+DP4TqmqOmp+ffY17KAZ8KQAQBMKADShAEATCgA0oQBAEwoANKEAQBMKADShAEATCgA0oQBAGzcD3Y+sY2Pdv47H7mfWO/J90PexvLK5j9t8/OdP92juL2/Buivr7Vn2rP9me807bdZn2Ht1znt+rkmZUVVdj3nH0+WWdeUsZ7bnezL/kfXf7O/z6/n+zPbw8XXeTfX+Cq/9Yz5++5T9Jt2u2R5e7/O1LEv2PXEPrudyZj1mQTVVLafuIwD+REIBgCYUAGhCAYAmFABoQgGAJhQAaEIBgCYUAGhCAYA2rrn4tM2rC6qqXslr/XtWR5DUYlyvWe6dt3mNwqfs7fVa6m089gj35LVldQR7UF9wfnyL5j6P+fisWKLqCK7Pmm1hXa/Zas5jfra2R3b/PB/zmovfwuvz2/u85uIIL9BHBbULa1YTc6nwjAfT3y5ZhUZy7Y8l3MR9/sWyBGOnPCkA0IQCAE0oANCEAgBNKADQhAIATSgA0IQCAE0oANCEAgBNKADQxt1H65b1d6zrvJ9oXYK+lKpag56SY8/m/hR+zsTbY94589yznpft/RGN399/G499BD08VVVnMPflknW3bG/fjcfex6f7d2tl/TfHa95n9BGWCCXdR3//Nbs+2/u8FOojbKda5tVUdSxZ91HwlVJVVWfQw/QW/j7ebvN9OdbsO2h5BXMf2dwTnhQAaEIBgCYUAGhCAYAmFABoQgGAJhQAaEIBgCYUAGhCAYA2r7kIXumvqlou83fSr2uWTftl/lr/fXmL5n59nlcdfLdkFQ37x9fx2O0jqy5YggqNqqqP9/n8WzC2qur1nFduXJfs2t9f8wqN7ZbVKNSajT+f83vi+ZHVRfz6LZj7Pbs3vwXjj/CM1z7fw+sPWZXLEVaiLEtQh3PP6iKu+/z7bV+z+pQlqMXYsnabEU8KADShAEATCgA0oQBAEwoANKEAQBMKADShAEATCgA0oQBAEwoAtHn30ZF1t6zXea/J9Zj3iFRVXc/5+C3oYKqqumzzdS/PeZdRVdUS1Ks8vs07fqqqXl//Ho1/fv02HvsRdDZVVR37Kxibnasvb0Gn1vVzNPcSViUdy/xzvj+zbqpvQffRxzFfR1XVK9jzV1bbU/cK1rJ8iuY+w9+wy2X+Odew4inphLqF3W6vNTjjt3DhA54UAGhCAYAmFABoQgGAJhQAaEIBgCYUAGhCAYAmFABoQgGANq65OMOai+N4jMcuwWvdVVVn8Nr4ZfwJ/2Gdd1EcNa8iqKp6vD7GY//5f0RT1/me1VzsQZ1HMraq6jjmr97vQfVHVdVvQVvE5Trf76qqt/ntUFVVr/uv47HbKzsrH8+g5uKRXZ+t5tdnDe/N/TK/N8+gCqeq6gh/wp7J2QrrPNZg/FnZd2fd5+fwnm3hiCcFAJpQAKAJBQCaUACgCQUAmlAAoAkFAJpQAKAJBQCaUACgCQUA2rz7KO4GCfo+tqwb5BJ0g3wcWXdLLcH4cOpkKUu839lijqB0KO2mWoLOmaWy8qNkLUv4kyfd81vQ8/MMD8saHJb1zD7oGvTlXNJzuM57leLanvR+C9YS1EFVVdV5zhdznOHkwQdN758JTwoANKEAQBMKADShAEATCgA0oQBAEwoANKEAQBMKADShAEATCgC0cYlQWDtS6xJ0oAQdMlVV2x50JV2y3pE96ATa1ls09xLsSbrf8V8kfUZH2E8U9EfF3UfB2DXpsQrnrqp6BWtfgq6c3/8gOLeXtP8mWMstm/u8z8ceyWesrG+oqmoN7uU9vTzB3Gn92nnOW6HOCsupBjwpANCEAgBNKADQhAIATSgA0IQCAE0oANCEAgBNKADQhAIAbVxzcaxhFUXwavd5yd4D39d5zcW5v6K5z+TV+y17Tf9Y5rUYaXHBWdlaKqmASH86JO/1hx/0DNZyhOcq3vPrvI5gObPZ93N8a9YluB+qqpYlqWgIOxpe8/FnWJ+yho0O+z6/J/bw6h/BWo7s8lQF1+c8wvt+wJMCAE0oANCEAgBNKADQhAIATSgA0IQCAE0oANCEAgBNKADQhAIAbVyw8rxmHShb0H9zvWT9HWsw9z7/iL+PDz7mEnbrbEfQfXSGvUrR6KqqoKNmCUtn/sSfGlEVzxl2H4V7vu9JEVPWrXMLhm9LtuF7cFrWNVv3JehIC6vDavs/OOVTR9J5VlWXoITrCDqYqqqW4N6MC7sGPCkA0IQCAE0oANCEAgBNKADQhAIATSgA0IQCAE0oANCEAgBt3AGxX+7RxJf7/NXu57dXNHf2mn6We5fgvfHHllVo3OctF3Umr7pXhf0PWevCGi7lCFox1rCiYQk+57pm9QLpni9BLcZxDWtLghqFdc3O4fWc329HePHPsFokcYQtF8laljOrctmD74n4Xt7nQ8/w+23CkwIATSgA0IQCAE0oANCEAgBNKADQhAIATSgA0IQCAE0oANCEAgBtXJqyXoPinqp6be/jsUfa37HMe0r2sBfmDDqB6pp1mmxBX0raZZT2E1VQ9XIkZUbpWpZkw6uWZHzYqxTv+XW+lmPL1nJ9C3qV9uz6LEEP0z08WNsR9A2Fv0nXpBTo938wtmXHsJZgX5Z08mDh5/7Hd015UgCgCQUAmlAAoAkFAJpQAKAJBQCaUACgCQUAmlAAoAkFANq4A2K73KOJH+f8Vfr1nr0G/grmvqSv0idVB+Hb68+kMiCsaNjDz7kE8y+X+X7/4y/mI6NekbCGJKzQSPf8DPZ8CSs0lnVeXbFWdn3eLvN1v4LaiqqqZdmCsWF9yp/4G/aS1q3s8/FRNUtFLT61hc0fE54UAGhCAYAmFABoQgGAJhQAaEIBgCYUAGhCAYAmFABoQgGAJhQAaMt5huUzAPx/y5MCAE0oANCEAgBNKADQhAIATSgA0IQCAE0oANCEAgDtfwHqpWCQIgaC/AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZcElEQVR4nO3cy44kB3rd8S/ynpWVWbeuS9/IJtnTNDUYciSNhAEtQxpoI28Ee+WH0GP4JbyyXsAwBMEwYMCGBQEeLTQDCpZIjSne+1pd1VVZlffMiAwtDHxe6hyAgD3G/7f++uvIiMg6GYs4RV3XdQAAEBGN/9sHAAD4fwehAABIhAIAIBEKAIBEKAAAEqEAAEiEAgAgEQoAgNRSB3/v9//AWjweX8mz3cbW2n3Y0d+3e+tox9p9fDiQZ+/s71q7O822PNvq9q3d0ZQvZUREXF2P5dl16b3feLC/J882qo21e7VaybPL5dLa3ev3rPkqKnl2vphau/f2R/pwrR9HRMR6tZZnm6HfsxERzWZTnh3uet+fwUD/bkZEtNv69VwY5yQioi6M39MN77vpXJ+yLqzdf/Jv/90/OcOTAgAgEQoAgEQoAAASoQAASIQCACARCgCARCgAABKhAABIhAIAIBEKAIBEKAAAklzK8elnn1qLx5eX8uyhVzkTxZH+D+5UQ293/0SenW31fqeIiGmldwjVRcfaPV963S3zhd4htKm8bqrLpt7H0mt5vUplqR9L0+yc6Xa71vx8OZNny613fYrlkTzb0OuGIiJiY/RH9Vvel3Nq9PZcVaW1e2fH6z4qGnpvU2H0kkVEREP/PT1fev1e5Uafb7a8e1bBkwIAIBEKAIBEKAAAEqEAAEiEAgAgEQoAgEQoAAASoQAASIQCACARCgCAJPcA9Ft6dUFERBhvX79t1FZERDw63ZNnT44Prd1941X6ovDOyWK1lGeXG72KICKiNo+l0+/rw6VXRVFv9WPfO9yxdpcb/Vg6beMzRkRVWePR7Og3+WqtX/uIiE2pX88d4zgiIloD/bz0zN1loVd/NGqvPqUM7x432lZid+Ddh9PZXJ7dlF7NRcM47sntjbVb+v+/940AgF9bhAIAIBEKAIBEKAAAEqEAAEiEAgAgEQoAgEQoAAASoQAASIQCACARCgCAJHcf9YrSWjwcyqvjyf0Da/dRvynPtrde58z0ai3PVlsvUxdz/Rw2OtbqGO3vWvMto9NmfDPxduuXPg6HXufM5Fbv1lkv9dmIiMXS66ipjS6e3YHeqRURsVkv5NlGZZzwiGh39WtfVd45aRmFQ6uVt7vT9r4Uja3+fVtNr63dUekdXF39z1VERJRbvRPqZuZ1pCl4UgAAJEIBAJAIBQBAIhQAAIlQAAAkQgEAkAgFAEAiFAAAiVAAACRCAQCQ5PfjD7req/R941X6vUHf2n08asuz1baydjvTzZb5/npDz+DV1qwXcLolIqJV66/SVyu9ciEiom7qn/P167G1u9roV2gyn1u755VecRIRsdsf6cMr7z5shn59GoVeuRAR0ez25NnFzKuJ2Wnr56RVe8e9XHrXZ7HRay624R3LeKqfl/Hc+y5PjTqc5eb7/13PkwIAIBEKAIBEKAAAEqEAAEiEAgAgEQoAgEQoAAASoQAASIQCACARCgCARCgAAJJcmHO8r/elREQM23ovUK/ndQg1mnpPSb/v9SptSr2jZhuFtbuu9e6Wdel1sVRrr19lW+vztdkJVLc68uxkPbN2V5V+r8wrvT8oIqI05ycz/Rw+v/I+Z7uhH8to6t2Hm1eX8uzixuuPeuvOY3n25OSBtbsY3ljzq+s38ux06l2fm4nefXR543WHffNU/5xV0+s8U/CkAABIhAIAIBEKAIBEKAAAEqEAAEiEAgAgEQoAgEQoAAASoQAASIQCACDJ70jfOx5Yi0edUp7d3dFrESIiCqOiIcKriyhqvV5gtfAqABpGLcbRcM/aPRh4NSS3N3rVwd5oZO2eLPXr8+1z/TgiIqYrveai47VWxP0drzKg1dbrC755M7Z2r2r9c7YL7x7fGw3l2Y9/4yfW7tuXek1MPTeP+07bml/N9es5nXq/j7tt/VgenunnOyLi5ORUnj2/1es2VDwpAAASoQAASIQCACARCgCARCgAABKhAABIhAIAIBEKAIBEKAAAEqEAAEiEAgAgyeUgh8O+t3g9lme7ba9zZqe7I8+uFk5PUsRmq3c27e8fWLvrWu96WVdeXm82XgfKzu6uPPviYmXt/vLbG3n2YqKf74iIuTH+dl/vD4qI+Ff/4sfW/IO7+jn8D7/8ytr9V1+8kmfL7dra3Wro9+FkfGHtnk/1e2U49LqMotK7wyIiej19f6fn3Ss7hb67rLx7/K2H9+TZ4dXE2q3gSQEAkAgFAEAiFAAAiVAAACRCAQCQCAUAQCIUAACJUAAAJEIBAJAIBQBAkvslTg6PrMWLK712oVF4NRfTuV5dsVh7r5i3Cv119/mmsnY7CbzYeNUF+wcja35d6VUHXz17Ye2+utXPS93qWLubTf0sjnre9TlpeZUBvSu90uEHozNr98tD/XOej19bu1dz/d765PPPrd2NcivPbgbePRt7p958Q/+7srenV+dERAy3+vdnufaqdur1rTz76Hhg7VbwpAAASIQCACARCgCARCgAABKhAABIhAIAIBEKAIBEKAAAEqEAAEiEAgAgEQoAgCSXgxzcObYWH+z25dlGo23tHt9ey7Ob2dTa3aj0vpxt6D0vERF1W+9i2d3tWbs34c3//Vd6p81sNbN293pdfbbj9V71B3pHzUHT67365Rfn1ny51o99ted1Hx0f6NezCK9DaFPqvWTz9cLaPZvrnUDr0rs+hdkHFoU+2m4YwxFRN/SOtHbLu8fLld6pVRsdZiqeFAAAiVAAACRCAQCQCAUAQCIUAACJUAAAJEIBAJAIBQBAIhQAAIlQAAAkQgEAkPRSDrOfqGh7845uT9+9EwNrd8vIyUbDy9SN0ZXU7e9Zuy9fTaz5+aXeH/XuodertNKrdaJndBlFRLz/3n15tuEcSESUTe+evTU6uFrNG2v3sKPft0cH71m73/vBW/Ls19/9tbX7V58/l2c7Lb3jJyKirr0es7I0/ry1Otbudke/V7ZbryNta5Q2FcX3/7ueJwUAQCIUAACJUAAAJEIBAJAIBQBAIhQAAIlQAAAkQgEAkAgFAEAiFAAASX4PfLHcWIuLzcKYLq3ds9mtPLveeLlXNvRKh+ncq5a4NebvP9Rf0Y+IqEvvWN6+o79K/949r/5hvtR333/ykbW7U+vVFdc33j3b3z+y5uNNUx59eHbXWj2ezeTZd//ZD6zdowO9WmR08IG1+/pCvw+vb7zqj7ZR/RER0ai78uxmW1m7neaKauP9fWvoX5+o69raLf3/3/tGAMCvLUIBAJAIBQBAIhQAAIlQAAAkQgEAkAgFAEAiFAAAiVAAACRCAQCQCAUAQJILdqrC6wapK73vw+3v6Pf68uzuUO95iYh4caF3Nn397MLa3Wrrn7Nz/sLavTz3juUHJ3qf0R/+gdet8+XzK3l2eP/Y2n3n6EyefX1xbu3e3ze7dbb6Oew09J6kiIjXF8/l2VZvbO2+GL+UZ5+/nFq72239+7Y/MgqEImKx8P5O1C39N2/hFA5FxNboSmoU3u6ioR939f1XH/GkAAD4PwgFAEAiFAAAiVAAACRCAQCQCAUAQCIUAACJUAAAJEIBAJAIBQBAkmsu9vd3rcVlS6+5mE6X1u56o79ifjO5sXZ/+51ejTCdehUA/Z6ewS+/vrV2n/Y61vz9+2/Ls/v33rF2tydGfUFPr4qIiHjw0e/qq1/pVREREf3SqwqpQr9vZzPvHr+7o9d/rCuvLqIY6N/lB4N71u7hvl5DMnnzytr9+vyNNb8p9HtruV5Zu6Oh90sMuj1r9Xqh/11pd7zvj4InBQBAIhQAAIlQAAAkQgEAkAgFAEAiFAAAiVAAACRCAQCQCAUAQCIUAACJUAAAJLn7aDL2ekda64k82y7MbGoax9E0hiNiPtW7kg6GA2v3/kDvQFlce91HJ/eOrPn7H/6+PPt3z9bW7s+/0Oc/vnto7R6P9d2n731k7W7E3Jpfr/SupP3a6ye6fa1/3/rrjbX77qF+zsdV19rd/vBAnl2MX1q7/8d//nNr/tlT/fo07Q6hQp5c6DVJERGxMX6rNzbetZd2fu8bAQC/tggFAEAiFAAAiVAAACRCAQCQCAUAQCIUAACJUAAAJEIBAJAIBQBAkmsumvpb3RERUS2m8mxtvDIeEdGIUj+Owqu5uDbeGr+99d5fr1d6RcPdPa9C43d+9jNr/sH7P5Vn/+Of/ntr99lgV55trhfW7udffakfx7u/Ye3uHT225ge1XuUyv3pt7e5v9bqI9cKr57ic6PP7x+9Yu4/OHsmzi+nI2t3wxqPqLOXZouH9Ddps9O9yUVbW7qLW58tS/hMu40kBAJAIBQBAIhQAAIlQAAAkQgEAkAgFAEAiFAAAiVAAACRCAQCQCAUAQCIUAABJLs4ovJqfqDZ6iVDR8LKpZYzXC6PMKCKKrT57eLRj7T7b0TubfusnT6zdH3ysdxlFRFy/1rupuuWNtfvdBw/k2a1zwiPi7ORYni2X+vmOiJiP9T6biIh1qe/fLLyOmir0/qgvnz+zdv/t3/1Cnv34p945OTo7kmdvJ14fVNv7usWdR3p/2Nb8G1StjX4io/MsIuLmYizPribmSRHwpAAASIQCACARCgCARCgAABKhAABIhAIAIBEKAIBEKAAAEqEAAEiEAgAgEQoAgCQXsmxLvesjImKx0jttOgO95yUiotVqy7PNhtc78vjsQJ7t9b1MffT2Q3n2o9/7mbX77vsfWvN/81d/Ks++9VA/JxERZz/8kTzbOX7P2t3a2ZNn50u93ykiYnE7sebPXzyVZ6/PvX6iajOXZ/vDnrX7zh39+/P0xSfW7tO79+XZcu5dn3qxsuaL2bU8W9UL71iMMrh+Vz/fERGdM33+tltYuxU8KQAAEqEAAEiEAgAgEQoAgEQoAAASoQAASIQCACARCgCARCgAABKhAABIcs1FuymPRkTE9UR/Tb9aeq9q93f68myzob+OHhFxcrQjzz59ObZ2v/dbfyTPPviRPvu/eVUUm8lMnt0b6tUSERHHT34sz85ah9buTz/5a3l2tdA/Y0TE7e3Ymr98/p0826y8upVeT/++3X9Hr5aIiPjwyWN5tmwOrN3t5r4+29lYu1vLpTU///a5POvW+JTGz+lps2nt3jnSz/npvSNrt4InBQBAIhQAAIlQAAAkQgEAkAgFAEAiFAAAiVAAACRCAQCQCAUAQCIUAACJUAAAJLlgZbXwekd2unp3S9HzukHajVKerSt9NiKiv6sfyx//mz+2dn/8L/9Qnh3dObV2n3/199Z80ziH48mNtfvim/8lz76YeJ0zf/FnfybP7vbb1u7lamrNn53qnVCjodch9PWzp/Ls2riWERGH9x7Js09+9NvW7qi68ujV+Jm1em52pF0v9PNS1F6323KxlWentde/Vk/1v7Uf7FurJTwpAAASoQAASIQCACARCgCARCgAABKhAABIhAIAIBEKAIBEKAAAEqEAAEjyu93beu1t3ur1BUWpvzIeEVHWG3134b1i3uuO5Nkf/7ZXAdBt67ULn/3NJ9bu6xdfWvOrlf4q/eT6ytr99IvP5Nlp3bd2tyv9uHdbXn3KqOdVURwf6DUXL89fWbvLjX6PzydePcfTr78zpj+1dk+nE3m21/K+m2X3xJp/U+rf5X6/Z+3eGer3bb+lV39EREzmt/JsufUqThQ8KQAAEqEAAEiEAgAgEQoAgEQoAAASoQAASIQCACARCgCARCgAABKhAABIhAIAIMndRxFeP9G21LuSWu0da3dV6r1K6/C6QU73DuTZ//Ln/8nafXiq98ic3H1o7V7Pb6z5dlvvY9kd6B0yERGtht45NDD6oCIizk6O5NnF5Nra3W96HTVvLi7l2c1av2cjIoY9vVtnPfW6j/7hk1/Isy9/9bm1e1Uu9OG2101VGfdVRMTggdFlNfC63RpdvYOrZ/YTHYR+7T/44TvWbgVPCgCARCgAABKhAABIhAIAIBEKAIBEKAAAEqEAAEiEAgAgEQoAgEQoAACSXHOx3RbW4k5LfyW91/IqNKKhH0vdNF51j4jteiPPXl6+snZPL/T5/ubW2r0NrwLg8ECvi9i/d2ztLquVPPv8hXcO66jl2UbDaHGJiHXp1RE0C72iY9DzqlxK4yvRdIYjIgr9HFZrrz6lYfyduJ17NSTrrlGhERHDe/p9OOuPrd2TrV6LsZx5v72PRu/Ks3eM2hcVTwoAgEQoAAASoQAASIQCACARCgCARCgAABKhAABIhAIAIBEKAIBEKAAAEqEAAEhyOUyj6FqLe92+PFuH1zkz6Os9MoPhHWv3fLOUZ4+GHWt3y/ic65tza/e24R3LvK335ZyevuMdy1rvhXn/wwfW7p//9/8mz67rubW7XXj9Xoupvn80HFm7Oy29t6lZeN1H06V+j3/90usnGo/1e3xVzKzdx0+837D39/W/Qeva+/5cX+rXvrPUO7IiIgb39T6jxbyydit4UgAAJEIBAJAIBQBAIhQAAIlQAAAkQgEAkAgFAEAiFAAAiVAAACRCAQCQ5HfpOy0vP+arlTzb7A2s3dumXrkx3yys3c12Lc92O/pr9BER7bb+OTs7e9buvZF3Dl9d6DUa8/teFcXJw8fy7PPXl9buH/7OP5dnpxcvrN1fff6pNT+bjuXZVtO7D/f29FqMIryai5fP9fPy3bc31u5GV78PR6d6XU1ExPGhVxVSGHUexZX3/Tm41mtI7p8cWrsf7Ovfty8+e2Xt/tm//qdneFIAACRCAQCQCAUAQCIUAACJUAAAJEIBAJAIBQBAIhQAAIlQAAAkQgEAkAgFAECSCzxOj7382Lx5I88uKq+7ZTbTZ+tGZe1utfROk9HoyNrdabfl2cXs1trdb+vHHRERa33+Fz//ubX63ff1XqVnz7zulkajkGd3uvr5johoGp1aERH9vt6XM5t63UeLhT5flmtr925f/5wf/+YTa3dvqPcTlc3S2l1t5tb84qnefdSY9KzdJztDefY3n/zQ271/Ks/+8uXX1m4FTwoAgEQoAAASoQAASIQCACARCgCARCgAABKhAABIhAIAIBEKAIBEKAAAEqEAAEhyAc5bDzvW4r1C7xL54qnXaXJ+Ucuz68rrs9nd1TuBZvMba3e1ncqzTTOvry70rqmIiMlU751ZbrzP2az1+eHugbX7/NWVPPtspnffRERsa71XKSLi9Fjvviq2G2v39fhanu0OvHt8f0/v7ek0vftwtTa6xlpeN9Vs5R3LeqrvH2y93Y8fnsmz9868jrSnz/TusDcX3t9OBU8KAIBEKAAAEqEAAEiEAgAgEQoAgEQoAAASoQAASIQCACARCgCARCgAAJLc6TA68F5JXxivXx+cNK3dMdiRRy/PV9bq5Xotz7Y6I2u3sTq2G6MuICI2lfc5bxZ6jcKg79UoLOd6vcRieWntXhvnpTLPYV179+H0Vr/HR6O+tXs02pNnFwuv6uDyjX7td3cH1u6iof/OLEq9riYiotPyzmFXb9qJTse79o8eP5JnF3Pvc/7lX34mz/7Pz19buxU8KQAAEqEAAEiEAgAgEQoAgEQoAAASoQAASIQCACARCgCARCgAABKhAABIhAIAIMndR62ePBoREb1RR5493PWyqbXQe37a/a21+/ba+JyVd9z93om+uu0dd7UaW/OdHf1ztlv6tYyIaDb1bqpV7X3O9UYvkKrrwtpdeBU1Ua/1jqdKH42IiHbL6BrreN1U42u9+2ix3li79/b1PrCW0ZMUEdEw78N5lPLs+eXE2n091XdPZjfW7v/6F7+SZ8+92isJTwoAgEQoAAASoQAASIQCACARCgCARCgAABKhAABIhAIAIBEKAIBEKAAAktx1MJ0ar91HRDR35dHdgdcB0O7rfQSDbs/avben1y5MbxfW7untuT47r6zdm6U3P+wcybO9tnfty5VeQ9Jqeb9LOsZ4u9u0dheFdyw7u3pVSMNriYmy0msUOn1v+WhfryG5uvLqHyZGbcnoUL8HIyLmpV5xEhHxD9+8kWd/9bdPrd2nh3qdx+kD/XxHRERDP4d39obebuW//943AgB+bREKAIBEKAAAEqEAAEiEAgAgEQoAgEQoAAASoQAASIQCACARCgCARCgAAJJcmvLsW2/xaqx3Dg2P9Z6XiIhefyPP7ukVTBERcXio98hMZ3Nr93isz1+/6Vi7r/Wal4iIaG71XqBtrXdNRURUldHDtPU6m5xfMUWjsHY3W16H0KLSj6b2bvFob/V7vJxfWburhX4fVi2v92o81XevvUsfV2bX2Ddf6F+K8ZuZtXs90w/+bO/M2v3B2/flWfOUSHhSAAAkQgEAkAgFAEAiFAAAiVAAACRCAQCQCAUAQCIUAACJUAAAJEIBAJDk9/qr9h1r8abzE3l2tV1ZuxvlpTzb2/OqDvaP9XqOg4bXXXA438qz46u+tXt8qddWREQsZnqlQ1V6lRtR6781tqV+TiIiloulPNvpeMfdbHnncLLUj30x1Y87IqJdr+XZYWNo7d42buXZzcar/ugO9EqUXrtr7d7v6OckIuLd2Jdnf/TRwNr9/ocfybOPHj+2dv/uT/WqkGcvptZuBU8KAIBEKAAAEqEAAEiEAgAgEQoAgEQoAAASoQAASIQCACARCgCARCgAABKhAABIRV3XelkJAOD/azwpAAASoQAASIQCACARCgCARCgAABKhAABIhAIAIBEKAIBEKAAA0j8Cs+jjz4w54nYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_torch_image(img):\n",
    "    img = img.permute(1, 2, 0)\n",
    "    img = img.numpy().astype(np.uint8)\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "sample_output, z_e, z_quantized = model(x_train[0].unsqueeze(0))\n",
    "print(sample_output.shape)\n",
    "show_torch_image(255*sample_output.squeeze(0).detach().cpu())\n",
    "show_torch_image(255 * x_train[0].cpu())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
